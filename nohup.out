WARNING:tensorflow:From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-11-18 20:06:53.087651: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-11-18 20:06:54.131116: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
round 0	 loss: [0.02, 0.02]	 reward: [-329.2, -19.0]	 value: [-0.15, 0.03]
round 1	 loss: [0.02, 0.02]	 reward: [-326.0, -20.0]	 value: [-0.09, 0.11]
round 2	 loss: [0.02, 0.03]	 reward: [-174.8, -20.0]	 value: [-0.02, 0.22]
round 3	 loss: [0.02, 0.02]	 reward: [-274.2, -20.0]	 value: [0.07, 0.26]
round 4	 loss: [0.01, 0.02]	 reward: [-309.2, -20.0]	 value: [0.12, 0.36]
round 5	 loss: [0.01, 0.02]	 reward: [-317.2, -18.0]	 value: [0.18, 0.46]
round 6	 loss: [0.01, 0.03]	 reward: [-278.2, -20.0]	 value: [0.21, 0.54]
round 7	 loss: [0.01, 0.02]	 reward: [-269.6, -20.0]	 value: [0.26, 0.6]
round 8	 loss: [0.02, 0.03]	 reward: [-122.8, -20.0]	 value: [0.28, 0.67]
round 9	 loss: [0.01, 0.03]	 reward: [-317.2, -20.0]	 value: [0.32, 0.68]
round 10	 loss: [0.02, 0.03]	 reward: [-145.8, -20.0]	 value: [0.37, 0.74]
round 11	 loss: [0.01, 0.02]	 reward: [-197.2, -20.0]	 value: [0.42, 0.77]
round 12	 loss: [0.01, 0.03]	 reward: [-150.8, -20.0]	 value: [0.43, 0.83]
round 13	 loss: [0.01, 0.03]	 reward: [-303.2, -20.0]	 value: [0.49, 0.87]
round 14	 loss: [0.01, 0.03]	 reward: [-312.2, -18.0]	 value: [0.52, 0.86]
round 15	 loss: [0.01, 0.03]	 reward: [-303.6, -20.0]	 value: [0.57, 0.87]
Namespace(eval=False, greedy=False, load_from=None, map_size=15, n_round=500, name='pursuit', render=False, render_every=10, save_every=2, train=True)
view_space (11, 11, 5)
feature_space (18,)
===== sample =====
eps 1.0 number [10, 10]
step   0,  reward: [-1.6  0. ],  total_reward: [-1.6  0. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-56.8 -13. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-120.8  -17. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-188.2  -18. ] 
step 200,  reward: [-1.4  0. ],  total_reward: [-260.  -19.] 
step 250,  reward: [-1.4  0. ],  total_reward: [-329.2  -19. ] 
steps: 251,  total time: 1.17,  step average 0.00
===== train =====
train_time 5.34
round time 6.52  total time 6.52

===== sample =====
eps 0.996 number [10, 10]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-1.4  0. ],  total_reward: [-57. -11.] 
step 100,  reward: [-1.4  0. ],  total_reward: [-124.2  -14. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-190.8  -19. ] 
step 200,  reward: [-1.4  0. ],  total_reward: [-256.6  -20. ] 
step 250,  reward: [-1.6  0. ],  total_reward: [-326.  -20.] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 4.95
round time 5.95  total time 12.47

save model... 
===== sample =====
eps 0.992 number [10, 10]
step   0,  reward: [-1.2  0. ],  total_reward: [-1.2  0. ] 
step  50,  reward: [-1.8  0. ],  total_reward: [-51.2 -14. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-116.4  -17. ] 
steps: 146,  total time: 0.58,  step average 0.00
===== train =====
train_time 2.73
round time 3.32  total time 16.10

===== sample =====
eps 0.988 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.6 -1. ],  total_reward: [-59.2  -9. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-116.  -17.] 
step 150,  reward: [-0.8  0. ],  total_reward: [-181.  -20.] 
step 200,  reward: [-1.  0.],  total_reward: [-249.6  -20. ] 
steps: 219,  total time: 0.87,  step average 0.00
===== train =====
train_time 4.42
round time 5.29  total time 21.39

save model... 
===== sample =====
eps 0.984 number [10, 10]
step   0,  reward: [-1.2  0. ],  total_reward: [-1.2  0. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-50.8 -13. ] 
step 100,  reward: [-0.2 -1. ],  total_reward: [-116.2  -16. ] 
step 150,  reward: [-1.8  0. ],  total_reward: [-178.2  -17. ] 
step 200,  reward: [-1.  0.],  total_reward: [-243.  -20.] 
step 250,  reward: [-1.4  0. ],  total_reward: [-309.2  -20. ] 
steps: 251,  total time: 1.02,  step average 0.00
===== train =====
train_time 4.88
round time 5.90  total time 27.59

===== sample =====
eps 0.98 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-56. -12.] 
step 100,  reward: [-1.4  0. ],  total_reward: [-124.4  -13. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-188.  -15.] 
step 200,  reward: [-1.4  0. ],  total_reward: [-250.  -17.] 
step 250,  reward: [-0.8  0. ],  total_reward: [-317.2  -18. ] 
steps: 251,  total time: 1.01,  step average 0.00
===== train =====
train_time 5.04
round time 6.06  total time 33.65

save model... 
===== sample =====
eps 0.976 number [10, 10]
step   0,  reward: [-0.6 -1. ],  total_reward: [-0.6 -1. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-52.8 -12. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-117.2  -17. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-180.2  -19. ] 
step 200,  reward: [-1.2  0. ],  total_reward: [-249.4  -19. ] 
steps: 223,  total time: 0.89,  step average 0.00
===== train =====
train_time 4.33
round time 5.22  total time 39.25

===== sample =====
eps 0.972 number [10, 10]
step   0,  reward: [-1.6  0. ],  total_reward: [-1.6  0. ] 
step  50,  reward: [0.2 0. ],  total_reward: [-54.8 -13. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-116.  -16.] 
step 150,  reward: [-1.6  0. ],  total_reward: [-185.8  -17. ] 
step 200,  reward: [-1.6  0. ],  total_reward: [-250.8  -20. ] 
steps: 215,  total time: 0.85,  step average 0.00
===== train =====
train_time 4.33
round time 5.18  total time 44.43

save model... 
===== sample =====
eps 0.968 number [10, 10]
step   0,  reward: [-1.6  0. ],  total_reward: [-1.6  0. ] 
step  50,  reward: [-1.8  0. ],  total_reward: [-54.2 -13. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-109.2  -19. ] 
steps: 112,  total time: 0.46,  step average 0.00
===== train =====
train_time 2.12
round time 2.58  total time 47.37

===== sample =====
eps 0.964 number [10, 10]
step   0,  reward: [-0.6 -1. ],  total_reward: [-0.6 -1. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-48.6 -18. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-110.  -19.] 
step 150,  reward: [-1.  0.],  total_reward: [-177.2  -20. ] 
step 200,  reward: [-1.4  0. ],  total_reward: [-245.8  -20. ] 
step 250,  reward: [-1.  0.],  total_reward: [-317.2  -20. ] 
steps: 251,  total time: 5.34,  step average 0.02
===== train =====
train_time 4.65
round time 9.99  total time 57.36

save model... 
===== sample =====
eps 0.96 number [10, 10]
step   0,  reward: [-1.8  0. ],  total_reward: [-1.8  0. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-47.4 -17. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-110.8  -19. ] 
steps: 127,  total time: 0.56,  step average 0.00
===== train =====
train_time 3.26
round time 3.82  total time 61.51

===== sample =====
eps 0.956 number [10, 10]
step   0,  reward: [ 0.8 -2. ],  total_reward: [ 0.8 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-48.2 -14. ] 
step 100,  reward: [-1.  0.],  total_reward: [-105.  -19.] 
step 150,  reward: [-1.4  0. ],  total_reward: [-166.2  -20. ] 
steps: 174,  total time: 0.84,  step average 0.00
===== train =====
train_time 3.60
round time 4.45  total time 65.96

save model... 
===== sample =====
eps 0.952 number [10, 10]
step   0,  reward: [-0.2 -1. ],  total_reward: [-0.2 -1. ] 
step  50,  reward: [-1.8  0. ],  total_reward: [-53. -14.] 
step 100,  reward: [-1.2  0. ],  total_reward: [-111.  -20.] 
steps: 131,  total time: 1.74,  step average 0.01
===== train =====
train_time 3.18
round time 4.92  total time 71.24

===== sample =====
eps 0.948 number [10, 10]
step   0,  reward: [-1.6  0. ],  total_reward: [-1.6  0. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-54.4 -10. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-113.4  -16. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-176.  -18.] 
step 200,  reward: [-1.6  0. ],  total_reward: [-237.8  -20. ] 
step 250,  reward: [-1.6  0. ],  total_reward: [-303.2  -20. ] 
steps: 251,  total time: 1.14,  step average 0.00
===== train =====
train_time 5.33
round time 6.47  total time 77.71

save model... 
===== sample =====
eps 0.944 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-51. -15.] 
step 100,  reward: [-1.  0.],  total_reward: [-110.2  -17. ] 
step 150,  reward: [-1.6  0. ],  total_reward: [-177.8  -17. ] 
step 200,  reward: [-1.2  0. ],  total_reward: [-242.2  -18. ] 
step 250,  reward: [-1.6  0. ],  total_reward: [-312.2  -18. ] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 4.94
round time 5.93  total time 83.99

===== sample =====
eps 0.94 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-45.2 -14. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-103.8  -18. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-169.2  -19. ] 
step 200,  reward: [-1.4  0. ],  total_reward: [-234.4  -20. ] 
step 250,  reward: [-2.  0.],  total_reward: [-303.6  -20. ] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 4.60
round time 5.59  total time 89.58

save model... 
===== sample =====
eps 0.9359999999999999 number [10, 10]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-1.8  0. ],  total_reward: [-51.8 -14. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-113.8  -17. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-178.  -17.] 
step 200,  reward: [-1.2  0. ],  total_reward: [-246.2  -18. ] round 16	 loss: [0.01, 0.03]	 reward: [-308.0, -20.0]	 value: [0.6, 0.94]
round 17	 loss: [0.01, 0.04]	 reward: [-215.4, -20.0]	 value: [0.65, 0.96]
round 18	 loss: [0.01, 0.03]	 reward: [-219.2, -20.0]	 value: [0.68, 0.99]
round 19	 loss: [0.01, 0.04]	 reward: [-303.6, -17.0]	 value: [0.71, 1.01]
round 20	 loss: [0.01, 0.04]	 reward: [-188.8, -20.0]	 value: [0.73, 1.02]
round 21	 loss: [0.01, 0.04]	 reward: [-293.2, -20.0]	 value: [0.74, 1.03]
round 22	 loss: [0.01, 0.03]	 reward: [-232.6, -20.0]	 value: [0.77, 1.06]
round 23	 loss: [0.01, 0.04]	 reward: [-181.0, -20.0]	 value: [0.81, 1.09]
round 24	 loss: [0.01, 0.04]	 reward: [-63.6, -20.0]	 value: [0.84, 1.14]
round 25	 loss: [0.01, 0.03]	 reward: [-73.4, -20.0]	 value: [0.91, 1.15]
round 26	 loss: [0.01, 0.04]	 reward: [-215.4, -20.0]	 value: [0.97, 1.2]
round 27	 loss: [0.01, 0.05]	 reward: [-104.0, -20.0]	 value: [1.0, 1.19]
round 28	 loss: [0.01, 0.05]	 reward: [-223.0, -20.0]	 value: [1.01, 1.22]
round 29	 loss: [0.02, 0.05]	 reward: [-91.4, -20.0]	 value: [1.01, 1.22]
round 30	 loss: [0.02, 0.05]	 reward: [-181.4, -20.0]	 value: [1.09, 1.26]
round 31	 loss: [0.02, 0.05]	 reward: [-117.6, -20.0]	 value: [1.18, 1.27]
round 32	 loss: [0.02, 0.06]	 reward: [-281.4, -20.0]	 value: [1.19, 1.3]
round 33	 loss: [0.02, 0.05]	 reward: [-282.8, -20.0]	 value: [1.27, 1.33]
round 34	 loss: [0.03, 0.04]	 reward: [-83.4, -20.0]	 value: [1.32, 1.35]
round 35	 loss: [0.03, 0.04]	 reward: [-21.0, -20.0]	 value: [1.4, 1.35]

step 250,  reward: [-1.4  0. ],  total_reward: [-308.  -20.] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 5.22
round time 6.22  total time 96.14

===== sample =====
eps 0.9319999999999999 number [10, 10]
step   0,  reward: [ 0. -1.],  total_reward: [ 0. -1.] 
step  50,  reward: [-1.2  0. ],  total_reward: [-44.4 -18. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-107.2  -19. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-172.2  -20. ] 
steps: 186,  total time: 0.74,  step average 0.00
===== train =====
train_time 3.46
round time 4.20  total time 100.34

save model... 
===== sample =====
eps 0.9279999999999999 number [10, 10]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-50.8 -12. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-103.6  -18. ] 
step 150,  reward: [-1.6  0. ],  total_reward: [-167.6  -19. ] 
steps: 192,  total time: 0.77,  step average 0.00
===== train =====
train_time 4.45
round time 5.22  total time 106.13

===== sample =====
eps 0.924 number [10, 10]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
step  50,  reward: [-1.  0.],  total_reward: [-49.2 -12. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-113.  -13.] 
step 150,  reward: [-1.4  0. ],  total_reward: [-179.4  -15. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-240.2  -17. ] 
step 250,  reward: [-1.  0.],  total_reward: [-303.6  -17. ] 
steps: 251,  total time: 6.88,  step average 0.03
===== train =====
train_time 5.56
round time 12.43  total time 118.57

save model... 
===== sample =====
eps 0.92 number [10, 10]
step   0,  reward: [-1.6  0. ],  total_reward: [-1.6  0. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-56.8  -8. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-109.2  -17. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-164.  -20.] 
steps: 173,  total time: 0.71,  step average 0.00
===== train =====
train_time 3.61
round time 4.33  total time 123.47

===== sample =====
eps 0.916 number [10, 10]
step   0,  reward: [ 0.6 -2. ],  total_reward: [ 0.6 -2. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-49.4 -16. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-108.2  -19. ] 
step 150,  reward: [-1.4  0. ],  total_reward: [-169.8  -19. ] 
step 200,  reward: [-1.6  0. ],  total_reward: [-231.2  -19. ] 
step 250,  reward: [-1.  0.],  total_reward: [-293.2  -20. ] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 4.85
round time 5.84  total time 129.31

save model... 
===== sample =====
eps 0.912 number [10, 10]
step   0,  reward: [ 0.2 -2. ],  total_reward: [ 0.2 -2. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-46.2 -15. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-104.8  -19. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-167.4  -19. ] 
step 200,  reward: [-1.4  0. ],  total_reward: [-226.6  -20. ] 
steps: 206,  total time: 0.85,  step average 0.00
===== train =====
train_time 4.30
round time 5.15  total time 134.88

===== sample =====
eps 0.908 number [10, 10]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
step  50,  reward: [-1.  0.],  total_reward: [-51.2 -11. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-102.8  -19. ] 
step 150,  reward: [-1.  0.],  total_reward: [-163.6  -19. ] 
steps: 167,  total time: 0.73,  step average 0.00
===== train =====
train_time 3.48
round time 4.21  total time 139.10

save model... 
===== sample =====
eps 0.904 number [10, 10]
step   0,  reward: [-0.8 -1. ],  total_reward: [-0.8 -1. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-40.4 -17. ] 
steps: 72,  total time: 0.30,  step average 0.00
===== train =====
train_time 1.38
round time 1.68  total time 141.18

===== sample =====
eps 0.9 number [10, 10]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-1.2  0. ],  total_reward: [-40.6 -18. ] 
steps: 79,  total time: 0.33,  step average 0.00
===== train =====
train_time 1.48
round time 1.81  total time 142.99

save model... 
===== sample =====
eps 0.896 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-45.2 -17. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-103.6  -19. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-161.8  -20. ] 
steps: 194,  total time: 0.82,  step average 0.00
===== train =====
train_time 4.41
round time 5.23  total time 148.64

===== sample =====
eps 0.892 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1. -1.],  total_reward: [-44.8 -15. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-95. -20.] 
steps: 109,  total time: 1.43,  step average 0.01
===== train =====
train_time 2.27
round time 3.70  total time 152.34

save model... 
===== sample =====
eps 0.888 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-49.8 -15. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-107.8  -18. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-170.  -20.] 
steps: 197,  total time: 0.85,  step average 0.00
===== train =====
train_time 4.24
round time 5.09  total time 157.93

===== sample =====
eps 0.884 number [10, 10]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.4 -1. ],  total_reward: [-39.4 -18. ] 
steps: 97,  total time: 1.82,  step average 0.02
===== train =====
train_time 2.26
round time 4.08  total time 162.02

save model... 
===== sample =====
eps 0.88 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-42.2 -16. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-100.2  -19. ] 
step 150,  reward: [-1.  0.],  total_reward: [-162.2  -20. ] 
steps: 169,  total time: 1.15,  step average 0.01
===== train =====
train_time 3.80
round time 4.95  total time 167.57

===== sample =====
eps 0.876 number [10, 10]
step   0,  reward: [-0.2 -1. ],  total_reward: [-0.2 -1. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-43.4 -18. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-101.6  -20. ] 
steps: 113,  total time: 0.74,  step average 0.01
===== train =====
train_time 2.44
round time 3.18  total time 170.75

save model... 
===== sample =====
eps 0.872 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-43.6 -17. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-98.4 -20. ] 
step 150,  reward: [-1.8  0. ],  total_reward: [-161.6  -20. ] 
step 200,  reward: [-1.6  0. ],  total_reward: [-219.4  -20. ] 
step 250,  reward: [-1.6  0. ],  total_reward: [-281.4  -20. ] 
steps: 251,  total time: 1.39,  step average 0.01
===== train =====
train_time 5.35
round time 6.73  total time 178.03

===== sample =====
eps 0.868 number [10, 10]
step   0,  reward: [-0.2 -1. ],  total_reward: [-0.2 -1. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-43. -17.] 
step 100,  reward: [-1.  0.],  total_reward: [-100.8  -18. ] 
step 150,  reward: [-1.2  0. ],  total_reward: [-163.  -18.] 
step 200,  reward: [-1.8  0. ],  total_reward: [-225.6  -18. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-282.8  -20. ] 
steps: 251,  total time: 1.46,  step average 0.01
===== train =====
train_time 5.33
round time 6.80  total time 184.82

save model... 
===== sample =====
eps 0.864 number [10, 10]
step   0,  reward: [ 0.6 -2. ],  total_reward: [ 0.6 -2. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-44.6 -17. ] 
steps: 88,  total time: 0.38,  step average 0.00
===== train =====
train_time 1.71
round time 2.09  total time 187.47

===== sample =====
eps 0.86 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
steps: 42,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.77
round time 0.96  total time 188.43

save model... 
===== sample =====
eps 0.856 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [ 0. -1.],  total_reward: [-47.6 -11. ] 
step 100,  reward: [-1.4  0. ],  total_reward: [-97.6 -19. ] 
steps: 114,  total time: 0.46,  step average 0.00round 36	 loss: [0.03, 0.06]	 reward: [-110.8, -20.0]	 value: [1.42, 1.4]
round 37	 loss: [0.04, 0.06]	 reward: [-93.6, -20.0]	 value: [1.47, 1.42]
round 38	 loss: [0.03, 0.06]	 reward: [-102.8, -20.0]	 value: [1.51, 1.42]
round 39	 loss: [0.04, 0.06]	 reward: [-123.8, -20.0]	 value: [1.5, 1.46]
round 40	 loss: [0.04, 0.05]	 reward: [-46.2, -20.0]	 value: [1.55, 1.48]
round 41	 loss: [0.05, 0.06]	 reward: [-46.4, -20.0]	 value: [1.7, 1.48]
round 42	 loss: [0.05, 0.08]	 reward: [-98.8, -20.0]	 value: [1.89, 1.52]
round 43	 loss: [0.05, 0.07]	 reward: [-76.8, -20.0]	 value: [1.93, 1.56]
round 44	 loss: [0.05, 0.06]	 reward: [-48.8, -20.0]	 value: [2.1, 1.57]
round 45	 loss: [0.07, 0.06]	 reward: [-40.6, -20.0]	 value: [2.21, 1.62]
round 46	 loss: [0.07, 0.08]	 reward: [-60.8, -20.0]	 value: [2.42, 1.66]
round 47	 loss: [0.07, 0.06]	 reward: [-34.4, -20.0]	 value: [2.56, 1.69]
round 48	 loss: [0.09, 0.08]	 reward: [-70.6, -20.0]	 value: [2.82, 1.74]
round 49	 loss: [0.1, 0.11]	 reward: [-25.6, -20.0]	 value: [2.75, 1.74]
round 50	 loss: [0.1, 0.08]	 reward: [-47.4, -20.0]	 value: [2.9, 1.78]
round 51	 loss: [0.09, 0.07]	 reward: [-16.8, -20.0]	 value: [2.93, 1.79]
round 52	 loss: [0.09, 0.1]	 reward: [-110.8, -20.0]	 value: [2.86, 1.82]
round 53	 loss: [0.09, 0.09]	 reward: [-137.2, -20.0]	 value: [2.9, 1.85]
round 54	 loss: [0.1, 0.09]	 reward: [-21.2, -20.0]	 value: [3.1, 1.85]
batch number: 19  add: 2510  replay_len: 2510/4194304
batch     0,  loss 0.051508, eval -0.145441
batches: 19,  total time: 5.32,  1k average: 5.32
batch number: 19  add: 2510  replay_len: 5020/4194304
batch     0,  loss 0.018968, eval -0.152136
batches: 19,  total time: 4.93,  1k average: 4.93
batch number: 11  add: 1460  replay_len: 6480/4194304
batch     0,  loss 0.021324, eval -0.076834
batches: 11,  total time: 2.72,  1k average: 2.72
batch number: 17  add: 2190  replay_len: 8670/4194304
batch     0,  loss 0.013252, eval -0.028894
batches: 17,  total time: 4.40,  1k average: 4.40
batch number: 19  add: 2510  replay_len: 11180/4194304
batch     0,  loss 0.013727, eval 0.075673
batches: 19,  total time: 4.86,  1k average: 4.86
batch number: 19  add: 2510  replay_len: 13690/4194304
batch     0,  loss 0.021825, eval 0.140394
batches: 19,  total time: 5.03,  1k average: 5.03
batch number: 17  add: 2230  replay_len: 15920/4194304
batch     0,  loss 0.013503, eval 0.179780
batches: 17,  total time: 4.30,  1k average: 4.30
batch number: 16  add: 2150  replay_len: 18070/4194304
batch     0,  loss 0.006132, eval 0.195007
batches: 16,  total time: 4.30,  1k average: 4.30
batch number: 8  add: 1120  replay_len: 19190/4194304
batch     0,  loss 0.023210, eval 0.261206
batches: 8,  total time: 2.11,  1k average: 2.11
batch number: 19  add: 2510  replay_len: 21700/4194304
batch     0,  loss 0.011049, eval 0.287387
batches: 19,  total time: 4.63,  1k average: 4.63
batch number: 9  add: 1270  replay_len: 22970/4194304
batch     0,  loss 0.010874, eval 0.341891
batches: 9,  total time: 3.25,  1k average: 3.25
batch number: 13  add: 1740  replay_len: 24710/4194304
batch     0,  loss 0.021735, eval 0.397670
batches: 13,  total time: 3.58,  1k average: 3.58
batch number: 10  add: 1310  replay_len: 26020/4194304
batch     0,  loss 0.017062, eval 0.438763
batches: 10,  total time: 3.17,  1k average: 3.17
batch number: 19  add: 2510  replay_len: 28530/4194304
batch     0,  loss 0.013200, eval 0.439290
batches: 19,  total time: 5.30,  1k average: 5.30
batch number: 19  add: 2510  replay_len: 31040/4194304
batch     0,  loss 0.012361, eval 0.483808
batches: 19,  total time: 4.92,  1k average: 4.92
batch number: 19  add: 2510  replay_len: 33550/4194304
batch     0,  loss 0.016967, eval 0.531317
batches: 19,  total time: 4.58,  1k average: 4.58
batch number: 19  add: 2510  replay_len: 36060/4194304
batch     0,  loss 0.016893, eval 0.575405
batches: 19,  total time: 5.20,  1k average: 5.20
batch number: 14  add: 1860  replay_len: 37920/4194304
batch     0,  loss 0.015584, eval 0.607434
batches: 14,  total time: 3.45,  1k average: 3.45
batch number: 15  add: 1920  replay_len: 39840/4194304
batch     0,  loss 0.009215, eval 0.638497
batches: 15,  total time: 4.43,  1k average: 4.43
batch number: 19  add: 2510  replay_len: 42350/4194304
batch     0,  loss 0.007254, eval 0.680726
batches: 19,  total time: 5.53,  1k average: 5.53
batch number: 13  add: 1730  replay_len: 44080/4194304
batch     0,  loss 0.013711, eval 0.731336
batches: 13,  total time: 3.60,  1k average: 3.60
batch number: 19  add: 2510  replay_len: 46590/4194304
batch     0,  loss 0.007470, eval 0.717349
batches: 19,  total time: 4.83,  1k average: 4.83
batch number: 16  add: 2060  replay_len: 48650/4194304
batch     0,  loss 0.011935, eval 0.758042
batches: 16,  total time: 4.28,  1k average: 4.28
batch number: 13  add: 1670  replay_len: 50320/4194304
batch     0,  loss 0.018903, eval 0.795231
batches: 13,  total time: 3.47,  1k average: 3.47
batch number: 5  add: 720  replay_len: 51040/4194304
batch     0,  loss 0.015082, eval 0.811145
batches: 5,  total time: 1.38,  1k average: 1.38
batch number: 6  add: 790  replay_len: 51830/4194304
batch     0,  loss 0.015708, eval 0.851133
batches: 6,  total time: 1.47,  1k average: 1.47
batch number: 15  add: 1940  replay_len: 53770/4194304
batch     0,  loss 0.013136, eval 0.892449
batches: 15,  total time: 4.37,  1k average: 4.37
batch number: 8  add: 1090  replay_len: 54860/4194304
batch     0,  loss 0.008464, eval 0.936198
batches: 8,  total time: 2.26,  1k average: 2.26
batch number: 15  add: 1970  replay_len: 56830/4194304
batch     0,  loss 0.015072, eval 0.996525
batches: 15,  total time: 4.22,  1k average: 4.22
batch number: 7  add: 970  replay_len: 57800/4194304
batch     0,  loss 0.014620, eval 1.001405
batches: 7,  total time: 2.25,  1k average: 2.25
batch number: 13  add: 1690  replay_len: 59490/4194304
batch     0,  loss 0.012539, eval 1.050984
batches: 13,  total time: 3.79,  1k average: 3.79
batch number: 8  add: 1130  replay_len: 60620/4194304
batch     0,  loss 0.017497, eval 1.081833
batches: 8,  total time: 2.42,  1k average: 2.42
batch number: 19  add: 2510  replay_len: 63130/4194304
batch     0,  loss 0.019368, eval 1.152158
batches: 19,  total time: 5.32,  1k average: 5.32
batch number: 19  add: 2510  replay_len: 65640/4194304
batch     0,  loss 0.021171, eval 1.181951
batches: 19,  total time: 5.30,  1k average: 5.30
batch number: 6  add: 880  replay_len: 66520/4194304
batch     0,  loss 0.019590, eval 1.242280
batches: 6,  total time: 1.70,  1k average: 1.70
batch number: 3  add: 420  replay_len: 66940/4194304
batch     0,  loss 0.024768, eval 1.295881
batches: 3,  total time: 0.76,  1k average: 0.76
batch number: 8  add: 1140  replay_len: 68080/4194304
batch     0,  loss 0.033886, eval 1.381500
batches: 8,  total time: 2.16,  1k average: 2.16
batch number: 7  add: 980  replay_len: 69060/4194304
batch     0,  loss 0.036449, eval 1.495755
batches: 7,  total time: 1.74,  1k average: 1.74
batch number: 8  add: 1040  replay_len: 70100/4194304
batch     0,  loss 0.030151, eval 1.418801
batches: 8,  total time: 1.98,  1k average: 1.98
batch number: 9  add: 1210  replay_len: 71310/4194304
batch     0,  loss 0.035160, eval 1.477326
batches: 9,  total time: 2.18,  1k average: 2.18
batch number: 4  add: 610  replay_len: 71920/4194304
batch     0,  loss 0.034894, eval 1.519373
batches: 4,  total time: 1.07,  1k average: 1.07
batch number: 4  add: 610  replay_len: 72530/4194304
batch     0,  loss 0.034744, eval 1.586529
batches: 4,  total time: 0.97,  1k average: 0.97
batch number: 8  add: 1110  replay_len: 73640/4194304
batch     0,  loss 0.054320, eval 1.685936
batches: 8,  total time: 2.29,  1k average: 2.29
batch number: 6  add: 880  replay_len: 74520/4194304
batch     0,  loss 0.040743, eval 1.878712
batches: 6,  total time: 1.53,  1k average: 1.53
batch number: 5  add: 640  replay_len: 75160/4194304
batch     0,  loss 0.047543, eval 1.919819
batches: 5,  total time: 1.27,  1k average: 1.27
batch number: 4  add: 560  replay_len: 75720/4194304
batch     0,  loss 0.073140, eval 2.053355
batches: 4,  total time: 1.08,  1k average: 1.08
batch number: 5  add: 750  replay_len: 76470/4194304
batch     0,  loss 0.070974, eval 2.339861
batches: 5,  total time: 1.30,  1k average: 1.30
batch number: 4  add: 530  replay_len: 77000/4194304
batch     0,  loss 0.071045, eval 2.490311
batches: 4,  total time: 0.95,  1k average: 0.95
batch number: 6  add: 850  replay_len: 77850/4194304
batch     0,  loss 0.072410, eval 2.570559
batches: 6,  total time: 1.55,  1k average: 1.55
batch number: 3  add: 440  replay_len: 78290/4194304
batch     0,  loss 0.094002, eval 2.780187
batches: 3,  total time: 0.97,  1k average: 0.97
batch number: 5  add: 640  replay_len: 78930/4194304
batch     0,  loss 0.081760, eval 2.665390
batches: 5,  total time: 1.36,  1k average: 1.36
batch number: 2  add: 380  replay_len: 79310/4194304
batch     0,  loss 0.094958, eval 2.846244
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 9  add: 1200  replay_len: 80510/4194304
batch     0,  loss 0.116507, eval 2.988659
batches: 9,  total time: 2.19,  1k average: 2.19
batch number: 11  add: 1430  replay_len: 81940/4194304
batch     0,  loss 0.099248, eval 2.933534
batches: 11,  total time: 2.85,  1k average: 2.85
batch number: 3  add: 430  replay_len: 82370/4194304
batch     0,  loss 0.118874, eval 2.997402
batches: 3,  total time: 0.73,  1k average: 0.73
batch number: 5  add: 730  replay_len: 83100/4194304
batch     0,  loss 0.119163, eval 3.115045
batches: 5,  total time: 1.27,  1k average: 1.27round 55	 loss: [0.12, 0.09]	 reward: [-57.8, -20.0]	 value: [3.34, 1.85]
batch number: 7  add: 988  replay_len: 988/4194304
batch     0,  loss 0.022077, eval -0.056339
batches: 7,  total time: 2.40,  1k average: 2.40
batch number: 9  add: 1255  replay_len: 2243/4194304
batch     0,  loss 0.021590, eval 0.029358
batches: 9,  total time: 2.58,  1k average: 2.58
batch number: 4  add: 631  replay_len: 2874/4194304
batch     0,  loss 0.029245, eval 0.103477
batches: 4,  total time: 1.14,  1k average: 1.14
batch number: 8  add: 1061  replay_len: 3935/4194304
batch     0,  loss 0.020010, eval 0.211957
batches: 8,  total time: 2.26,  1k average: 2.26
batch number: 8  add: 1102  replay_len: 5037/4194304
batch     0,  loss 0.028084, eval 0.253508
batches: 8,  total time: 2.20,  1k average: 2.20
batch number: 10  add: 1343  replay_len: 6380/4194304
batch     0,  loss 0.023747, eval 0.354979
batches: 10,  total time: 2.82,  1k average: 2.82
batch number: 7  add: 899  replay_len: 7279/4194304
batch     0,  loss 0.029699, eval 0.446106
batches: 7,  total time: 1.92,  1k average: 1.92
batch number: 8  add: 1070  replay_len: 8349/4194304
batch     0,  loss 0.021446, eval 0.548554
batches: 8,  total time: 2.21,  1k average: 2.21
batch number: 4  add: 628  replay_len: 8977/4194304
batch     0,  loss 0.030351, eval 0.597907
batches: 4,  total time: 1.16,  1k average: 1.16
batch number: 6  add: 782  replay_len: 9759/4194304
batch     0,  loss 0.028251, eval 0.671856
batches: 6,  total time: 1.68,  1k average: 1.68
batch number: 5  add: 672  replay_len: 10431/4194304
batch     0,  loss 0.016479, eval 0.697478
batches: 5,  total time: 1.87,  1k average: 1.87
batch number: 6  add: 845  replay_len: 11276/4194304
batch     0,  loss 0.020292, eval 0.749960
batches: 6,  total time: 1.80,  1k average: 1.80
batch number: 5  add: 718  replay_len: 11994/4194304
batch     0,  loss 0.030144, eval 0.787504
batches: 5,  total time: 1.92,  1k average: 1.92
batch number: 9  add: 1155  replay_len: 13149/4194304
batch     0,  loss 0.031769, eval 0.842843
batches: 9,  total time: 2.69,  1k average: 2.69
batch number: 6  add: 816  replay_len: 13965/4194304
batch     0,  loss 0.032374, eval 0.861080
batches: 6,  total time: 2.03,  1k average: 2.03
batch number: 6  add: 861  replay_len: 14826/4194304
batch     0,  loss 0.020896, eval 0.884620
batches: 6,  total time: 1.66,  1k average: 1.66
batch number: 8  add: 1085  replay_len: 15911/4194304
batch     0,  loss 0.020814, eval 0.897777
batches: 8,  total time: 2.52,  1k average: 2.52
batch number: 5  add: 670  replay_len: 16581/4194304
batch     0,  loss 0.036559, eval 0.932178
batches: 5,  total time: 1.34,  1k average: 1.34
batch number: 6  add: 841  replay_len: 17422/4194304
batch     0,  loss 0.040566, eval 0.949837
batches: 6,  total time: 2.01,  1k average: 2.01
batch number: 8  add: 1041  replay_len: 18463/4194304
batch     0,  loss 0.046581, eval 0.981401
batches: 8,  total time: 2.32,  1k average: 2.32
batch number: 8  add: 1054  replay_len: 19517/4194304
batch     0,  loss 0.041875, eval 1.007391
batches: 8,  total time: 2.27,  1k average: 2.27
batch number: 7  add: 907  replay_len: 20424/4194304
batch     0,  loss 0.034259, eval 1.045902
batches: 7,  total time: 2.00,  1k average: 2.00
batch number: 6  add: 809  replay_len: 21233/4194304
batch     0,  loss 0.027028, eval 1.050372
batches: 6,  total time: 1.83,  1k average: 1.83
batch number: 6  add: 824  replay_len: 22057/4194304
batch     0,  loss 0.034004, eval 1.054103
batches: 6,  total time: 1.81,  1k average: 1.81
batch number: 3  add: 388  replay_len: 22445/4194304
batch     0,  loss 0.034675, eval 1.105171
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 2  add: 352  replay_len: 22797/4194304
batch     0,  loss 0.023383, eval 1.140978
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 5  add: 698  replay_len: 23495/4194304
batch     0,  loss 0.043129, eval 1.152138
batches: 5,  total time: 1.36,  1k average: 1.36
batch number: 4  add: 519  replay_len: 24014/4194304
batch     0,  loss 0.059385, eval 1.177623
batches: 4,  total time: 1.13,  1k average: 1.13
batch number: 5  add: 653  replay_len: 24667/4194304
batch     0,  loss 0.051234, eval 1.195484
batches: 5,  total time: 1.36,  1k average: 1.36
batch number: 3  add: 411  replay_len: 25078/4194304
batch     0,  loss 0.041842, eval 1.208201
batches: 3,  total time: 1.01,  1k average: 1.01
batch number: 3  add: 498  replay_len: 25576/4194304
batch     0,  loss 0.046556, eval 1.241010
batches: 3,  total time: 1.04,  1k average: 1.04
batch number: 3  add: 424  replay_len: 26000/4194304
batch     0,  loss 0.049018, eval 1.254290
batches: 3,  total time: 0.97,  1k average: 0.97
batch number: 4  add: 564  replay_len: 26564/4194304
batch     0,  loss 0.045952, eval 1.281424
batches: 4,  total time: 1.22,  1k average: 1.22
batch number: 6  add: 780  replay_len: 27344/4194304
batch     0,  loss 0.048218, eval 1.313312
batches: 6,  total time: 1.99,  1k average: 1.99
batch number: 3  add: 428  replay_len: 27772/4194304
batch     0,  loss 0.046116, eval 1.337933
batches: 3,  total time: 0.87,  1k average: 0.87
batch number: 1  add: 255  replay_len: 28027/4194304
batch     0,  loss 0.041003, eval 1.352885
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 4  add: 556  replay_len: 28583/4194304
batch     0,  loss 0.054272, eval 1.383820
batches: 4,  total time: 1.14,  1k average: 1.14
batch number: 2  add: 273  replay_len: 28856/4194304
batch     0,  loss 0.054839, eval 1.394287
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 2  add: 371  replay_len: 29227/4194304
batch     0,  loss 0.058178, eval 1.422736
batches: 2,  total time: 0.54,  1k average: 0.54
batch number: 2  add: 348  replay_len: 29575/4194304
batch     0,  loss 0.058290, eval 1.416925
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 2  add: 360  replay_len: 29935/4194304
batch     0,  loss 0.050061, eval 1.437376
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 1  add: 251  replay_len: 30186/4194304
batch     0,  loss 0.064764, eval 1.475886
batches: 1,  total time: 0.29,  1k average: 0.29
batch number: 3  add: 439  replay_len: 30625/4194304
batch     0,  loss 0.089141, eval 1.497311
batches: 3,  total time: 0.97,  1k average: 0.97
batch number: 2  add: 350  replay_len: 30975/4194304
batch     0,  loss 0.084093, eval 1.499512
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 2  add: 300  replay_len: 31275/4194304
batch     0,  loss 0.058954, eval 1.547398
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 2  add: 301  replay_len: 31576/4194304
batch     0,  loss 0.067938, eval 1.548192
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 2  add: 361  replay_len: 31937/4194304
batch     0,  loss 0.077555, eval 1.609507
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 192  replay_len: 32129/4194304
batch     0,  loss 0.058455, eval 1.694332
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 2  add: 338  replay_len: 32467/4194304
batch     0,  loss 0.072934, eval 1.719310
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 2  add: 282  replay_len: 32749/4194304
batch     0,  loss 0.095229, eval 1.730333
batches: 2,  total time: 0.69,  1k average: 0.69
batch number: 2  add: 293  replay_len: 33042/4194304
batch     0,  loss 0.090353, eval 1.763340
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 2  add: 285  replay_len: 33327/4194304
batch     0,  loss 0.076767, eval 1.772284
batches: 2,  total time: 0.54,  1k average: 0.54
batch number: 2  add: 294  replay_len: 33621/4194304
batch     0,  loss 0.096212, eval 1.757984
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 4  add: 577  replay_len: 34198/4194304
batch     0,  loss 0.074946, eval 1.792396
batches: 4,  total time: 1.18,  1k average: 1.18
batch number: 1  add: 250  replay_len: 34448/4194304
batch     0,  loss 0.089081, eval 1.854840
batches: 1,  total time: 0.29,  1k average: 0.29
batch number: 2  add: 347  replay_len: 34795/4194304
batch     0,  loss 0.074434, eval 1.841888
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 1  add: 221  replay_len: 35016/4194304
batch     0,  loss 0.115817, eval 1.805968round 56	 loss: [0.11, 0.12]	 reward: [-32.8, -20.0]	 value: [3.48, 1.81]
round 57	 loss: [0.12, 0.09]	 reward: [-26.6, -20.0]	 value: [3.59, 1.85]
round 58	 loss: [0.12, 0.09]	 reward: [-28.6, -20.0]	 value: [3.79, 1.89]
round 59	 loss: [0.16, 0.13]	 reward: [-34.8, -20.0]	 value: [4.07, 1.84]
round 60	 loss: [0.16, 0.1]	 reward: [-22.8, -20.0]	 value: [4.23, 1.9]
round 61	 loss: [0.17, 0.1]	 reward: [-35.2, -20.0]	 value: [4.43, 1.97]
round 62	 loss: [0.19, 0.11]	 reward: [-86.8, -20.0]	 value: [4.63, 2.0]
round 63	 loss: [0.2, 0.1]	 reward: [-50.0, -20.0]	 value: [4.61, 2.04]

===== train =====
train_time 2.17
round time 2.64  total time 191.55

===== sample =====
eps 0.852 number [10, 10]
step   0,  reward: [ 0.2 -2. ],  total_reward: [ 0.2 -2. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-36.8 -20. ] 
steps: 98,  total time: 0.41,  step average 0.00
===== train =====
train_time 1.75
round time 2.17  total time 193.72

save model... 
===== sample =====
eps 0.848 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-44.4 -17. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-99.4 -20. ] 
steps: 104,  total time: 0.44,  step average 0.00
===== train =====
train_time 1.98
round time 2.42  total time 196.61

===== sample =====
eps 0.844 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-40.2 -19. ] 
step 100,  reward: [-1.  0.],  total_reward: [-98.8 -20. ] 
steps: 121,  total time: 2.26,  step average 0.02
===== train =====
train_time 2.19
round time 4.45  total time 201.06

save model... 
===== sample =====
eps 0.84 number [10, 10]
step   0,  reward: [-0.6 -1. ],  total_reward: [-0.6 -1. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35.8 -20. ] 
steps: 61,  total time: 0.25,  step average 0.00
===== train =====
train_time 1.08
round time 1.33  total time 202.99

===== sample =====
eps 0.836 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-36.8 -20. ] 
steps: 61,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.97
round time 1.22  total time 204.21

save model... 
===== sample =====
eps 0.832 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35.2 -16. ] 
step 100,  reward: [-1.6  0. ],  total_reward: [-90.4 -18. ] 
steps: 111,  total time: 0.56,  step average 0.01
===== train =====
train_time 2.31
round time 2.87  total time 207.67

===== sample =====
eps 0.828 number [10, 10]
step   0,  reward: [-1.2  0. ],  total_reward: [-1.2  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-37.8 -16. ] 
steps: 88,  total time: 0.36,  step average 0.00
===== train =====
train_time 1.54
round time 1.90  total time 209.57

save model... 
===== sample =====
eps 0.8240000000000001 number [10, 10]
step   0,  reward: [-1.8  0. ],  total_reward: [-1.8  0. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35.2 -19. ] 
steps: 64,  total time: 0.26,  step average 0.00
===== train =====
train_time 1.28
round time 1.54  total time 211.60

===== sample =====
eps 0.8200000000000001 number [10, 10]
step   0,  reward: [-1.2  0. ],  total_reward: [-1.2  0. ] 
step  50,  reward: [-2.  0.],  total_reward: [-35. -20.] 
steps: 56,  total time: 0.22,  step average 0.00
===== train =====
train_time 1.09
round time 1.31  total time 212.91

save model... 
===== sample =====
eps 0.8160000000000001 number [10, 10]
step   0,  reward: [ 0. -1.],  total_reward: [ 0. -1.] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35. -18.] 
steps: 75,  total time: 0.33,  step average 0.00
===== train =====
train_time 1.30
round time 1.63  total time 215.12

===== sample =====
eps 0.812 number [10, 10]
step   0,  reward: [-0.6 -1. ],  total_reward: [-0.6 -1. ] 
step  50,  reward: [-1.8  0. ],  total_reward: [-32.8 -20. ] 
steps: 53,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.96
round time 1.18  total time 216.29

save model... 
===== sample =====
eps 0.808 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [-37. -18.] 
steps: 85,  total time: 0.37,  step average 0.00
===== train =====
train_time 1.56
round time 1.93  total time 218.85

===== sample =====
eps 0.804 number [10, 10]
step   0,  reward: [ 1.4 -3. ],  total_reward: [ 1.4 -3. ] 
steps: 44,  total time: 0.65,  step average 0.01
===== train =====
train_time 0.98
round time 1.62  total time 220.48

save model... 
===== sample =====
eps 0.8 number [10, 10]
step   0,  reward: [ 1. -2.],  total_reward: [ 1. -2.] 
step  50,  reward: [-1.4  0. ],  total_reward: [-33.6 -20. ] 
steps: 64,  total time: 0.29,  step average 0.00
===== train =====
train_time 1.37
round time 1.66  total time 222.67

===== sample =====
eps 0.796 number [10, 10]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
steps: 38,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.66
round time 0.82  total time 223.49

save model... 
===== sample =====
eps 0.792 number [10, 10]
step   0,  reward: [-1.2  0. ],  total_reward: [-1.2  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-36.8 -18. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-92. -19.] 
steps: 120,  total time: 0.54,  step average 0.00
===== train =====
train_time 2.20
round time 2.74  total time 226.86

===== sample =====
eps 0.788 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-38.8 -18. ] 
step 100,  reward: [-1.8  0. ],  total_reward: [-93. -18.] 
steps: 143,  total time: 0.58,  step average 0.00
===== train =====
train_time 2.86
round time 3.44  total time 230.31

save model... 
===== sample =====
eps 0.784 number [10, 10]
step   0,  reward: [ 0.4 -2. ],  total_reward: [ 0.4 -2. ] 
steps: 43,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.73
round time 0.92  total time 231.87

===== sample =====
eps 0.78 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-33.8 -19. ] 
steps: 73,  total time: 0.30,  step average 0.00
===== train =====
train_time 1.28
round time 1.57  total time 233.45

save model... 
===== sample =====
eps 0.776 number [10, 10]
step   0,  reward: [ 1. -2.],  total_reward: [ 1. -2.] 
step  50,  reward: [0.2 0. ],  total_reward: [-32.8 -20. ] 
steps: 51,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.80
round time 1.03  total time 235.05

===== sample =====
eps 0.772 number [10, 10]
step   0,  reward: [-0.6 -1. ],  total_reward: [-0.6 -1. ] 
steps: 49,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.90
round time 1.15  total time 236.19

save model... 
===== sample =====
eps 0.768 number [10, 10]
step   0,  reward: [ 0.6 -2. ],  total_reward: [ 0.6 -2. ] 
steps: 50,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.88
round time 1.08  total time 238.00

===== sample =====
eps 0.764 number [10, 10]
step   0,  reward: [-0.2 -1. ],  total_reward: [-0.2 -1. ] 
step  50,  reward: [-1.2  0. ],  total_reward: [-29. -20.] 
steps: 58,  total time: 0.99,  step average 0.02
===== train =====
train_time 1.15
round time 2.15  total time 240.15

save model... 
===== sample =====
eps 0.76 number [10, 10]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
steps: 45,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.77
round time 0.98  total time 241.78

===== sample =====
eps 0.756 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-30.2 -20. ] 
steps: 57,  total time: 0.23,  step average 0.00
===== train =====
train_time 1.06
round time 1.30  total time 243.07

save model... 
===== sample =====
eps 0.752 number [10, 10]
step   0,  reward: [ 0. -1.],  total_reward: [ 0. -1.] 
step  50,  reward: [-1.4  0. ],  total_reward: [-32.8 -19. ] 
step 100,  reward: [-1.2  0. ],  total_reward: [-82. -20.] 
steps: 107,  total time: 0.47,  step average 0.00
===== train =====
train_time 2.06
round time 2.53  total time 246.33

===== sample =====
eps 0.748 number [10, 10]
step   0,  reward: [ 0.8 -2. ],  total_reward: [ 0.8 -2. ] 
step  50,  reward: [-1.  0.],  total_reward: [-32.6 -19. ] 
steps: 70,  total time: 0.28,  step average 0.00
===== train =====
train_time 1.29
round time 1.57  total time 247.90

save model... 
===== sample =====
eps 0.744 number [10, 10]
step   0,  reward: [-1.4  0. ],  total_reward: [-1.4  0. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35.4 -20. ] 
steps: 85,  total time: 0.36,  step average 0.00round 64	 loss: [0.22, 0.11]	 reward: [-69.8, -20.0]	 value: [5.09, 2.06]
round 65	 loss: [0.22, 0.1]	 reward: [-99.2, -20.0]	 value: [5.2, 2.13]
round 66	 loss: [0.22, 0.12]	 reward: [-50.8, -20.0]	 value: [5.32, 2.13]
round 67	 loss: [0.23, 0.13]	 reward: [-22.6, -20.0]	 value: [5.47, 2.1]
round 68	 loss: [0.24, 0.12]	 reward: [-114.4, -20.0]	 value: [5.67, 2.11]
round 69	 loss: [0.27, 0.14]	 reward: [-33.4, -20.0]	 value: [5.79, 2.2]
round 70	 loss: [0.29, 0.14]	 reward: [-23.0, -20.0]	 value: [6.24, 2.17]
round 71	 loss: [0.28, 0.12]	 reward: [-20.0, -20.0]	 value: [6.24, 2.23]
round 72	 loss: [0.29, 0.12]	 reward: [-62.0, -20.0]	 value: [6.49, 2.24]

===== train =====
train_time 1.62
round time 1.98  total time 250.54

===== sample =====
eps 0.74 number [10, 10]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.2 -1. ],  total_reward: [-32.6 -18. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-76.6 -20. ] 
steps: 123,  total time: 0.48,  step average 0.00
===== train =====
train_time 2.31
round time 2.80  total time 253.33

save model... 
===== sample =====
eps 0.736 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-35.6 -20. ] 
steps: 69,  total time: 0.30,  step average 0.00
===== train =====
train_time 1.38
round time 1.68  total time 255.67

===== sample =====
eps 0.732 number [10, 10]
step   0,  reward: [ 0.8 -2. ],  total_reward: [ 0.8 -2. ] 
steps: 46,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.86
round time 1.05  total time 256.72

save model... 
===== sample =====
eps 0.728 number [10, 10]
step   0,  reward: [ 0.8 -2. ],  total_reward: [ 0.8 -2. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-28. -18.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-76.2 -18. ] 
steps: 141,  total time: 0.56,  step average 0.00
===== train =====
train_time 2.59
round time 3.15  total time 260.54

===== sample =====
eps 0.724 number [10, 10]
step   0,  reward: [-0.4 -1. ],  total_reward: [-0.4 -1. ] 
step  50,  reward: [-1.6  0. ],  total_reward: [-29. -19.] 
steps: 57,  total time: 0.90,  step average 0.02
===== train =====
train_time 1.22
round time 2.13  total time 262.67

save model... 
===== sample =====
eps 0.72 number [10, 10]
step   0,  reward: [ 0.4 -2. ],  total_reward: [ 0.4 -2. ] 
steps: 50,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.86
round time 1.09  total time 264.54

===== sample =====
eps 0.716 number [10, 10]
step   0,  reward: [ 0.8 -2. ],  total_reward: [ 0.8 -2. ] 
steps: 49,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.81
round time 1.03  total time 265.57

save model... 
===== sample =====
eps 0.712 number [10, 10]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [-1.4  0. ],  total_reward: [-26.2 -20. ] 
steps: 87,  total time: 0.36,  step average 0.00
===== train =====
train_time 1.50
round time 1.86  total time 268.09

===== sample =====
eps 0.708 number [10, 10]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [-1.  0.],  total_reward: [-25.2 -19. ] 
steps: 71,  total time: 0.29,  step average 0.00
===== train =====

batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 2  add: 331  replay_len: 35347/4194304
batch     0,  loss 0.103193, eval 1.812733
batches: 2,  total time: 0.54,  1k average: 0.54
batch number: 2  add: 269  replay_len: 35616/4194304
batch     0,  loss 0.097675, eval 1.831300
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 255  replay_len: 35871/4194304
batch     0,  loss 0.129663, eval 1.844688
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 255  replay_len: 36126/4194304
batch     0,  loss 0.095509, eval 1.899908
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 2  add: 329  replay_len: 36455/4194304
batch     0,  loss 0.103144, eval 1.925380
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 3  add: 393  replay_len: 36848/4194304
batch     0,  loss 0.116230, eval 1.932602
batches: 3,  total time: 0.94,  1k average: 0.94
batch number: 2  add: 315  replay_len: 37163/4194304
batch     0,  loss 0.105152, eval 2.024078
batches: 2,  total time: 0.53,  1k average: 0.53
batch number: 3  add: 407  replay_len: 37570/4194304
batch     0,  loss 0.081771, eval 2.086742
batches: 3,  total time: 0.92,  1k average: 0.92
batch number: 3  add: 474  replay_len: 38044/4194304
batch     0,  loss 0.096838, eval 2.046308
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 3  add: 484  replay_len: 38528/4194304
batch     0,  loss 0.141864, eval 2.070790
batches: 3,  total time: 0.83,  1k average: 0.83
batch number: 2  add: 290  replay_len: 38818/4194304
batch     0,  loss 0.124033, eval 2.129881
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 2  add: 370  replay_len: 39188/4194304
batch     0,  loss 0.101616, eval 2.163524
batches: 2,  total time: 0.54,  1k average: 0.54
batch number: 2  add: 296  replay_len: 39484/4194304
batch     0,  loss 0.135998, eval 2.204375
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 2  add: 290  replay_len: 39774/4194304
batch     0,  loss 0.135450, eval 2.209447
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 252  replay_len: 40026/4194304
batch     0,  loss 0.115581, eval 2.232628
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 324  replay_len: 40350/4194304
batch     0,  loss 0.101457, eval 2.258235
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 2  add: 333  replay_len: 40683/4194304
batch     0,  loss 0.124418, eval 2.241018
batches: 2,  total time: 0.50,  1k average: 0.50
Process Process-2:
Traceback (most recent call last):
  File "/usr/lib/python3.6/multiprocessing/process.py", line 258, in _bootstrap
    self.run()
  File "/usr/lib/python3.6/multiprocessing/process.py", line 93, in run
    self._target(*self._args, **self._kwargs)
  File "/home/dailin/MAgent/python/magent/model.py", line 309, in model_client
    cmd = conn.recv()
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/usr/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
WARNING:tensorflow:From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-11-18 20:13:48.782662: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
From /home/dailin/MAgent/python/magent/builtin/tf_model/dqn.py:185: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
2018-11-18 20:13:50.264396: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
round 0	 loss: [0.03, 0.03]	 reward: [-154.6, -10.0]	 value: [-0.0, 0.22]
round 1	 loss: [0.02, 0.02]	 reward: [-164.2, -8.0]	 value: [-0.01, 0.27]
round 2	 loss: [0.02, 0.02]	 reward: [-156.4, -10.0]	 value: [0.01, 0.37]
round 3	 loss: [0.02, 0.02]	 reward: [-159.2, -10.0]	 value: [0.07, 0.46]
round 4	 loss: [0.02, 0.02]	 reward: [-116.4, -10.0]	 value: [0.18, 0.57]
round 5	 loss: [0.01, 0.02]	 reward: [-156.8, -10.0]	 value: [0.27, 0.71]
round 6	 loss: [0.02, 0.02]	 reward: [-128.6, -10.0]	 value: [0.35, 0.81]
round 7	 loss: [0.01, 0.02]	 reward: [-153.4, -10.0]	 value: [0.39, 0.91]
round 8	 loss: [0.02, 0.03]	 reward: [-136.4, -10.0]	 value: [0.46, 1.04]
round 9	 loss: [0.02, 0.03]	 reward: [-162.4, -6.0]	 value: [0.56, 1.16]
round 10	 loss: [0.02, 0.03]	 reward: [-159.0, -8.0]	 value: [0.62, 1.26]
round 11	 loss: [0.01, 0.03]	 reward: [-161.0, -7.0]	 value: [0.7, 1.34]
round 12	 loss: [0.01, 0.03]	 reward: [-154.4, -10.0]	 value: [0.74, 1.46]
round 13	 loss: [0.01, 0.03]	 reward: [-154.8, -9.0]	 value: [0.81, 1.54]
round 14	 loss: [0.01, 0.04]	 reward: [-157.0, -9.0]	 value: [0.87, 1.62]
Namespace(eval=False, greedy=False, load_from=None, map_size=15, n_round=500, name='pursuit', render=False, render_every=10, save_every=2, train=True)
view_space (11, 11, 7)
feature_space (20,)
===== sample =====
eps 1.0 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-32.2  -3. ] 
step 100,  reward: [-1.  0.],  total_reward: [-62.4  -7. ] 
step 150,  reward: [-1.  0.],  total_reward: [-97.6  -8. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-131.2  -10. ] 
steps: 239,  total time: 1.46,  step average 0.01
===== train =====
train_time 3.96
round time 5.42  total time 5.42

===== sample =====
eps 0.996 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-29.4  -4. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-62.  -4.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-92.4  -7. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-129.2   -7. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-164.2   -8. ] 
steps: 251,  total time: 0.94,  step average 0.00
===== train =====
train_time 2.79
round time 3.73  total time 9.14

save model... 
===== sample =====
eps 0.992 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0.2 0. ],  total_reward: [-30.2  -4. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-65.  -5.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-98.4  -7. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-128.6  -10. ] 
steps: 243,  total time: 1.00,  step average 0.00
===== train =====
train_time 4.17
round time 5.18  total time 14.66

===== sample =====
eps 0.988 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [0.4 0. ],  total_reward: [-30.8  -4. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-58.2  -7. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-87.4 -10. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-121.6  -10. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-159.2  -10. ] 
steps: 251,  total time: 1.62,  step average 0.01
===== train =====
train_time 2.68
round time 4.29  total time 18.95

save model... 
===== sample =====
eps 0.984 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.6  0. ],  total_reward: [-31.4  -3. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-64.6  -4. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-92.2 -10. ] 
steps: 195,  total time: 0.78,  step average 0.00
===== train =====
train_time 2.41
round time 3.18  total time 22.50

===== sample =====
eps 0.98 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-30.8  -3. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-63.  -7.] 
step 150,  reward: [-0.8  0. ],  total_reward: [-95.6  -7. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-124.8  -10. ] 
step 250,  reward: [0.4 0. ],  total_reward: [-156.8  -10. ] 
steps: 251,  total time: 1.17,  step average 0.00
===== train =====
train_time 2.71
round time 3.88  total time 26.38

save model... 
===== sample =====
eps 0.976 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-29.2  -6. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-62.  -8.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-95.4  -9. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-128.6  -10. ] 
steps: 202,  total time: 0.82,  step average 0.00
===== train =====
train_time 2.44
round time 3.27  total time 30.05

===== sample =====
eps 0.972 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-29.6  -5. ] 
step 100,  reward: [-1.  0.],  total_reward: [-55. -10.] 
step 150,  reward: [-0.2  0. ],  total_reward: [-86.4 -10. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-122.4  -10. ] 
steps: 247,  total time: 0.87,  step average 0.00
===== train =====
train_time 2.77
round time 3.65  total time 33.70

save model... 
===== sample =====
eps 0.968 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-31.8  -4. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-61.6  -7. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-93.  -7.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-122.8  -10. ] 
steps: 224,  total time: 1.14,  step average 0.01
===== train =====
train_time 2.87
round time 4.01  total time 38.39

===== sample =====
eps 0.964 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-34.  -1.] 
step 100,  reward: [-1.  0.],  total_reward: [-62.8  -4. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-92.8  -4. ] 
step 200,  reward: [-1.  0.],  total_reward: [-124.4   -6. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-162.4   -6. ] 
steps: 251,  total time: 3.66,  step average 0.01
===== train =====
train_time 3.32
round time 6.98  total time 45.37

save model... 
===== sample =====
eps 0.96 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-1.  0.],  total_reward: [-31.2  -5. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-61.4  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-95.2  -7. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-126.6   -7. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-159.   -8.] 
steps: 251,  total time: 1.67,  step average 0.01
===== train =====
train_time 3.87
round time 5.54  total time 51.53

===== sample =====
eps 0.956 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.6  0. ],  total_reward: [-30.8  -4. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-61.8  -6. ] 
step 150,  reward: [-1.  0.],  total_reward: [-92.8  -7. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-128.2   -7. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-161.   -7.] 
steps: 251,  total time: 1.29,  step average 0.01
===== train =====
train_time 3.40
round time 4.69  total time 56.22

save model... 
===== sample =====
eps 0.952 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-28.8  -3. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-59.6  -7. ] 
step 150,  reward: [-1.  0.],  total_reward: [-91.8  -8. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-122.8   -8. ] 
step 250,  reward: [ 0. -1.],  total_reward: [-154.4  -10. ] 
steps: 251,  total time: 1.05,  step average 0.00
===== train =====
train_time 3.95
round time 5.00  total time 61.57

===== sample =====
eps 0.948 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-25.4  -7. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-58.4  -7. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-91.4  -7. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-123.8   -8. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-154.8   -9. ] 
steps: 251,  total time: 1.04,  step average 0.00
===== train =====
train_time 2.61
round time 3.65  total time 65.23

save model... 
===== sample =====
eps 0.944 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-32.2  -1. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-59.4  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-92.2  -6. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-120.   -9.] 
step 250,  reward: [-0.8  0. ],  total_reward: [-157.   -9.] 
steps: 251,  total time: 1.02,  step average 0.00
===== train =====
train_time 3.36
round time 4.38  total time 70.01

===== sample =====
eps 0.94 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-28.8  -3. ] 
step 100,  reward: [-1.  0.],  total_reward: [-59.2  -3. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-92.2  -6. ] 
step 200,  reward: [-1.  0.],  total_reward: [-119.2   -8. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-153.   -8.] round 15	 loss: [0.01, 0.04]	 reward: [-153.0, -8.0]	 value: [0.9, 1.71]
round 16	 loss: [0.01, 0.04]	 reward: [-149.6, -9.0]	 value: [0.97, 1.8]
round 17	 loss: [0.01, 0.04]	 reward: [-153.8, -10.0]	 value: [1.02, 1.9]
round 18	 loss: [0.01, 0.04]	 reward: [-149.4, -9.0]	 value: [1.07, 1.98]
round 19	 loss: [0.01, 0.05]	 reward: [-133.4, -10.0]	 value: [1.12, 2.1]
round 20	 loss: [0.01, 0.05]	 reward: [-145.0, -8.0]	 value: [1.17, 2.22]
round 21	 loss: [0.01, 0.05]	 reward: [-151.0, -8.0]	 value: [1.2, 2.31]
round 22	 loss: [0.01, 0.06]	 reward: [-149.8, -10.0]	 value: [1.24, 2.42]
round 23	 loss: [0.01, 0.05]	 reward: [-147.4, -9.0]	 value: [1.27, 2.53]
round 24	 loss: [0.01, 0.07]	 reward: [-152.0, -9.0]	 value: [1.27, 2.62]
round 25	 loss: [0.01, 0.06]	 reward: [-113.2, -10.0]	 value: [1.33, 2.76]
round 26	 loss: [0.01, 0.07]	 reward: [-142.6, -8.0]	 value: [1.37, 2.81]
round 27	 loss: [0.01, 0.07]	 reward: [-133.6, -10.0]	 value: [1.4, 2.92]
round 28	 loss: [0.01, 0.08]	 reward: [-103.0, -10.0]	 value: [1.42, 2.96]
round 29	 loss: [0.01, 0.08]	 reward: [-144.6, -8.0]	 value: [1.46, 3.11]
round 30	 loss: [0.01, 0.08]	 reward: [-144.0, -9.0]	 value: [1.49, 3.21]

steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 3.57
round time 4.57  total time 74.58

save model... 
===== sample =====
eps 0.9359999999999999 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-27.  -3.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-52.2  -8. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-82.8  -9. ] 
step 200,  reward: [-1.  0.],  total_reward: [-117.6   -9. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-149.6   -9. ] 
steps: 251,  total time: 0.87,  step average 0.00
===== train =====
train_time 3.48
round time 4.35  total time 79.34

===== sample =====
eps 0.9319999999999999 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-28.  -5.] 
step 100,  reward: [-0.2  0. ],  total_reward: [-57.6  -8. ] 
step 150,  reward: [-1.  0.],  total_reward: [-89.6  -9. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-121.2  -10. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-153.8  -10. ] 
steps: 251,  total time: 0.88,  step average 0.00
===== train =====
train_time 2.83
round time 3.71  total time 83.05

save model... 
===== sample =====
eps 0.9279999999999999 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-30.2  -3. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-57.4  -7. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-85.8  -9. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-116.   -9.] 
step 250,  reward: [-1.  0.],  total_reward: [-149.4   -9. ] 
steps: 251,  total time: 1.01,  step average 0.00
===== train =====
train_time 2.90
round time 3.91  total time 87.51

===== sample =====
eps 0.924 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-30.  -4.] 
step 100,  reward: [-0.4  0. ],  total_reward: [-58.  -8.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-89.6  -9. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-118.2  -10. ] 
steps: 225,  total time: 3.77,  step average 0.02
===== train =====
train_time 2.36
round time 6.13  total time 93.64

save model... 
===== sample =====
eps 0.92 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-25.  -7.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-57.  -8.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-81.4  -8. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-113.   -8.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-145.   -8.] 
steps: 251,  total time: 0.87,  step average 0.00
===== train =====
train_time 2.40
round time 3.27  total time 97.32

===== sample =====
eps 0.916 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.8  0. ],  total_reward: [-25.6  -6. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-53.4  -8. ] 
step 150,  reward: [-1.  0.],  total_reward: [-86.2  -8. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-119.4   -8. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-151.   -8.] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.56
round time 3.45  total time 100.76

save model... 
===== sample =====
eps 0.912 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-30.  -3.] 
step 100,  reward: [-0.4  0. ],  total_reward: [-56.6  -6. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-84.2 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-115.  -10.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-149.8  -10. ] 
steps: 251,  total time: 1.43,  step average 0.01
===== train =====
train_time 2.97
round time 4.40  total time 105.74

===== sample =====
eps 0.908 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-26.8  -5. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-53.  -8.] 
step 150,  reward: [-0.8  0. ],  total_reward: [-83.6  -8. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-116.4   -9. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-147.4   -9. ] 
steps: 251,  total time: 0.88,  step average 0.00
===== train =====
train_time 2.52
round time 3.40  total time 109.14

save model... 
===== sample =====
eps 0.904 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-29.  -5.] 
step 100,  reward: [-0.6  0. ],  total_reward: [-58.8  -7. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-88.4  -8. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-117.8   -8. ] 
step 250,  reward: [-1.  0.],  total_reward: [-152.   -9.] 
steps: 251,  total time: 0.95,  step average 0.00
===== train =====
train_time 2.71
round time 3.66  total time 113.27

===== sample =====
eps 0.9 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.8  0. ],  total_reward: [-31.4  -1. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-60.6  -5. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-84.6  -7. ] 
step 200,  reward: [0.4 0. ],  total_reward: [-113.2  -10. ] 
steps: 201,  total time: 0.75,  step average 0.00
===== train =====
train_time 2.10
round time 2.85  total time 116.11

save model... 
===== sample =====
eps 0.896 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-27.2  -3. ] 
step 100,  reward: [0. 0.],  total_reward: [-55.4  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-81.8  -7. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-110.8   -8. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-142.6   -8. ] 
steps: 251,  total time: 0.91,  step average 0.00
===== train =====
train_time 2.70
round time 3.60  total time 120.16

===== sample =====
eps 0.892 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-28.  -6.] 
step 100,  reward: [-0.4  0. ],  total_reward: [-56.4  -7. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-86.2  -8. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-116.6  -10. ] 
steps: 230,  total time: 0.81,  step average 0.00
===== train =====
train_time 2.26
round time 3.07  total time 123.23

save model... 
===== sample =====
eps 0.888 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-23.8  -7. ] 
step 100,  reward: [-1.  0.],  total_reward: [-51.4  -8. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-81.8  -9. ] 
steps: 189,  total time: 0.92,  step average 0.00
===== train =====
train_time 2.27
round time 3.19  total time 126.86

===== sample =====
eps 0.884 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-27.8  -4. ] 
step 100,  reward: [0.2 0. ],  total_reward: [-50.6  -7. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-80.  -8.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-113.4   -8. ] 
step 250,  reward: [-1.  0.],  total_reward: [-144.6   -8. ] 
steps: 251,  total time: 4.10,  step average 0.02
===== train =====
train_time 2.95
round time 7.06  total time 133.92

save model... 
===== sample =====
eps 0.88 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-28.4  -3. ] 
step 100,  reward: [-1.  0.],  total_reward: [-55.8  -7. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-87.6  -7. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-120.   -7.] 
step 250,  reward: [-1.  0.],  total_reward: [-144.   -9.] 
steps: 251,  total time: 1.03,  step average 0.00
===== train =====
train_time 3.57
round time 4.61  total time 139.03

===== sample =====
eps 0.876 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-25.  -3.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-53.2  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-83.8  -6. ] round 31	 loss: [0.01, 0.09]	 reward: [-147.8, -6.0]	 value: [1.5, 3.27]
round 32	 loss: [0.01, 0.1]	 reward: [-144.6, -10.0]	 value: [1.52, 3.41]
round 33	 loss: [0.01, 0.08]	 reward: [-142.2, -9.0]	 value: [1.55, 3.39]
round 34	 loss: [0.01, 0.1]	 reward: [-133.0, -8.0]	 value: [1.55, 3.47]
round 35	 loss: [0.01, 0.09]	 reward: [-145.0, -10.0]	 value: [1.56, 3.47]
round 36	 loss: [0.01, 0.09]	 reward: [-144.6, -7.0]	 value: [1.58, 3.61]
round 37	 loss: [0.01, 0.07]	 reward: [-104.6, -10.0]	 value: [1.6, 3.69]
round 38	 loss: [0.01, 0.11]	 reward: [-96.2, -10.0]	 value: [1.62, 3.72]
round 39	 loss: [0.01, 0.1]	 reward: [-140.6, -10.0]	 value: [1.63, 3.84]
round 40	 loss: [0.01, 0.11]	 reward: [-142.2, -10.0]	 value: [1.64, 3.88]
round 41	 loss: [0.01, 0.12]	 reward: [-137.0, -10.0]	 value: [1.62, 4.02]
round 42	 loss: [0.01, 0.12]	 reward: [-99.2, -10.0]	 value: [1.62, 4.11]
round 43	 loss: [0.01, 0.11]	 reward: [-131.8, -10.0]	 value: [1.67, 4.18]
round 44	 loss: [0.01, 0.12]	 reward: [-70.8, -10.0]	 value: [1.68, 4.17]
round 45	 loss: [0.01, 0.15]	 reward: [-73.4, -10.0]	 value: [1.7, 4.15]
round 46	 loss: [0.01, 0.09]	 reward: [-135.0, -10.0]	 value: [1.78, 4.33]
round 47	 loss: [0.01, 0.18]	 reward: [-55.4, -10.0]	 value: [1.76, 4.37]

step 200,  reward: [-1.  0.],  total_reward: [-115.8   -6. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-147.8   -6. ] 
steps: 251,  total time: 0.94,  step average 0.00
===== train =====
train_time 2.81
round time 3.74  total time 142.77

save model... 
===== sample =====
eps 0.872 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-27.8  -2. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-57.2  -4. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-85.6  -8. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-113.4  -10. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-144.6  -10. ] 
steps: 251,  total time: 1.03,  step average 0.00
===== train =====
train_time 3.00
round time 4.03  total time 147.50

===== sample =====
eps 0.868 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-25.6  -5. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-54.2  -7. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-84.2  -7. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-110.8   -9. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-142.2   -9. ] 
steps: 251,  total time: 0.93,  step average 0.00
===== train =====
train_time 2.67
round time 3.60  total time 151.10

save model... 
===== sample =====
eps 0.864 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [ 0.2 -1. ],  total_reward: [-22.4  -5. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-50.  -6.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-76.2  -8. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-106.   -8.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-133.   -8.] 
steps: 251,  total time: 1.08,  step average 0.00
===== train =====
train_time 2.90
round time 3.98  total time 155.60

===== sample =====
eps 0.86 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-27.8  -4. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-56.2  -6. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-86.8  -8. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-116.   -8.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-145.  -10.] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 2.77
round time 3.76  total time 159.36

save model... 
===== sample =====
eps 0.856 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-27.8  -4. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-55.4  -6. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-86.8  -6. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-114.   -7.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-144.6   -7. ] 
steps: 251,  total time: 0.93,  step average 0.00
===== train =====
train_time 3.31
round time 4.24  total time 164.10

===== sample =====
eps 0.852 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-25.8  -6. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-50.4  -9. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-80.4  -9. ] 
steps: 197,  total time: 1.17,  step average 0.01
===== train =====
train_time 2.25
round time 3.43  total time 167.53

save model... 
===== sample =====
eps 0.848 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-20.8  -7. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-44.4  -9. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-75.6  -9. ] 
steps: 188,  total time: 0.68,  step average 0.00
===== train =====
train_time 2.16
round time 2.84  total time 170.92

===== sample =====
eps 0.844 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-21.4  -6. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-47.  -9.] 
step 150,  reward: [-0.8  0. ],  total_reward: [-79.  -9.] 
step 200,  reward: [-0.8  0. ],  total_reward: [-110.2   -9. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-140.6  -10. ] 
steps: 251,  total time: 4.71,  step average 0.02
===== train =====
train_time 2.57
round time 7.29  total time 178.21

save model... 
===== sample =====
eps 0.84 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-25.4  -6. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-53.4  -7. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-83.6  -9. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-110.8  -10. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-142.2  -10. ] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.55
round time 3.45  total time 182.22

===== sample =====
eps 0.836 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-28.  -4.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-54.4  -6. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-77.8  -9. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-106.4  -10. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-137.  -10.] 
steps: 251,  total time: 0.97,  step average 0.00
===== train =====
train_time 2.59
round time 3.56  total time 185.79

save model... 
===== sample =====
eps 0.832 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-20.4  -8. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-49.4  -8. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-73.8  -9. ] 
steps: 197,  total time: 0.97,  step average 0.00
===== train =====
train_time 1.94
round time 2.91  total time 189.19

===== sample =====
eps 0.828 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-23.2  -3. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-51.2  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-82.4  -5. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-109.8   -6. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-131.8  -10. ] 
steps: 251,  total time: 0.94,  step average 0.00
===== train =====
train_time 3.16
round time 4.10  total time 193.30

save model... 
===== sample =====
eps 0.8240000000000001 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-23.4  -7. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-46.4 -10. ] 
steps: 142,  total time: 0.50,  step average 0.00
===== train =====
train_time 1.53
round time 2.03  total time 195.90

===== sample =====
eps 0.8200000000000001 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-19.8 -10. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-44.6 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-71.4 -10. ] 
steps: 156,  total time: 0.75,  step average 0.00
===== train =====
train_time 1.94
round time 2.70  total time 198.60

save model... 
===== sample =====
eps 0.8160000000000001 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-18.4 -10. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-47.8 -10. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-76.6 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-103.4  -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-135.  -10.] 
steps: 251,  total time: 0.97,  step average 0.00
===== train =====
train_time 3.00
round time 3.97  total time 203.15

===== sample =====
eps 0.812 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-17.2  -8. ] 
step 100,  reward: [0.2 0. ],  total_reward: [-44. -10.] 
steps: 121,  total time: 0.98,  step average 0.01
===== train =====
train_time 1.57
round time 2.55  total time 205.69

save model... 
===== sample =====
eps 0.808 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-20.6  -7. ] round 48	 loss: [0.01, 0.16]	 reward: [-127.0, -10.0]	 value: [1.83, 4.43]
round 49	 loss: [0.02, 0.16]	 reward: [-125.8, -10.0]	 value: [1.83, 4.44]
round 50	 loss: [0.02, 0.15]	 reward: [-61.0, -10.0]	 value: [1.87, 4.43]
round 51	 loss: [0.02, 0.15]	 reward: [-91.6, -10.0]	 value: [1.92, 4.51]
round 52	 loss: [0.02, 0.14]	 reward: [-122.6, -10.0]	 value: [1.93, 4.52]
round 53	 loss: [0.02, 0.14]	 reward: [-123.8, -8.0]	 value: [1.92, 4.57]
round 54	 loss: [0.02, 0.15]	 reward: [-71.8, -10.0]	 value: [1.93, 4.54]
round 55	 loss: [0.02, 0.15]	 reward: [-132.6, -10.0]	 value: [1.98, 4.59]
batch number: 9  add: 1195  replay_len: 1195/4194304
batch     0,  loss 0.055683, eval -0.108794
batches: 9,  total time: 3.94,  1k average: 3.94
batch number: 9  add: 1255  replay_len: 2450/4194304
batch     0,  loss 0.026479, eval 0.002458
batches: 9,  total time: 2.78,  1k average: 2.78
batch number: 9  add: 1215  replay_len: 3665/4194304
batch     0,  loss 0.010709, eval -0.024625
batches: 9,  total time: 4.16,  1k average: 4.16
batch number: 9  add: 1255  replay_len: 4920/4194304
batch     0,  loss 0.011781, eval 0.007971
batches: 9,  total time: 2.65,  1k average: 2.65
batch number: 7  add: 975  replay_len: 5895/4194304
batch     0,  loss 0.019787, eval 0.079485
batches: 7,  total time: 2.40,  1k average: 2.40
batch number: 9  add: 1255  replay_len: 7150/4194304
batch     0,  loss 0.014745, eval 0.175019
batches: 9,  total time: 2.70,  1k average: 2.70
batch number: 7  add: 1010  replay_len: 8160/4194304
batch     0,  loss 0.010517, eval 0.256654
batches: 7,  total time: 2.43,  1k average: 2.43
batch number: 9  add: 1235  replay_len: 9395/4194304
batch     0,  loss 0.013491, eval 0.341677
batches: 9,  total time: 2.76,  1k average: 2.76
batch number: 8  add: 1120  replay_len: 10515/4194304
batch     0,  loss 0.017093, eval 0.401136
batches: 8,  total time: 2.85,  1k average: 2.85
batch number: 9  add: 1255  replay_len: 11770/4194304
batch     0,  loss 0.015737, eval 0.468863
batches: 9,  total time: 3.31,  1k average: 3.31
batch number: 9  add: 1255  replay_len: 13025/4194304
batch     0,  loss 0.018222, eval 0.555507
batches: 9,  total time: 3.85,  1k average: 3.85
batch number: 9  add: 1255  replay_len: 14280/4194304
batch     0,  loss 0.013585, eval 0.635532
batches: 9,  total time: 3.39,  1k average: 3.39
batch number: 9  add: 1255  replay_len: 15535/4194304
batch     0,  loss 0.018479, eval 0.697899
batches: 9,  total time: 3.93,  1k average: 3.93
batch number: 9  add: 1255  replay_len: 16790/4194304
batch     0,  loss 0.014442, eval 0.755499
batches: 9,  total time: 2.60,  1k average: 2.60
batch number: 9  add: 1255  replay_len: 18045/4194304
batch     0,  loss 0.010311, eval 0.816242
batches: 9,  total time: 3.34,  1k average: 3.34
batch number: 9  add: 1255  replay_len: 19300/4194304
batch     0,  loss 0.014503, eval 0.860158
batches: 9,  total time: 3.56,  1k average: 3.56
batch number: 9  add: 1255  replay_len: 20555/4194304
batch     0,  loss 0.010560, eval 0.904223
batches: 9,  total time: 3.47,  1k average: 3.47
batch number: 9  add: 1255  replay_len: 21810/4194304
batch     0,  loss 0.010079, eval 0.965885
batches: 9,  total time: 2.81,  1k average: 2.81
batch number: 9  add: 1255  replay_len: 23065/4194304
batch     0,  loss 0.011613, eval 1.003620
batches: 9,  total time: 2.88,  1k average: 2.88
batch number: 8  add: 1125  replay_len: 24190/4194304
batch     0,  loss 0.014056, eval 1.076604
batches: 8,  total time: 2.35,  1k average: 2.35
batch number: 9  add: 1255  replay_len: 25445/4194304
batch     0,  loss 0.011725, eval 1.089690
batches: 9,  total time: 2.39,  1k average: 2.39
batch number: 9  add: 1255  replay_len: 26700/4194304
batch     0,  loss 0.007310, eval 1.154828
batches: 9,  total time: 2.55,  1k average: 2.55
batch number: 9  add: 1255  replay_len: 27955/4194304
batch     0,  loss 0.012639, eval 1.213105
batches: 9,  total time: 2.95,  1k average: 2.95
batch number: 9  add: 1255  replay_len: 29210/4194304
batch     0,  loss 0.017104, eval 1.258073
batches: 9,  total time: 2.51,  1k average: 2.51
batch number: 9  add: 1255  replay_len: 30465/4194304
batch     0,  loss 0.014082, eval 1.261431
batches: 9,  total time: 2.70,  1k average: 2.70
batch number: 7  add: 1005  replay_len: 31470/4194304
batch     0,  loss 0.014381, eval 1.302386
batches: 7,  total time: 2.09,  1k average: 2.09
batch number: 9  add: 1255  replay_len: 32725/4194304
batch     0,  loss 0.007911, eval 1.331126
batches: 9,  total time: 2.68,  1k average: 2.68
batch number: 8  add: 1150  replay_len: 33875/4194304
batch     0,  loss 0.009386, eval 1.380905
batches: 8,  total time: 2.25,  1k average: 2.25
batch number: 7  add: 945  replay_len: 34820/4194304
batch     0,  loss 0.015025, eval 1.393834
batches: 7,  total time: 2.26,  1k average: 2.26
batch number: 9  add: 1255  replay_len: 36075/4194304
batch     0,  loss 0.012769, eval 1.427549
batches: 9,  total time: 2.94,  1k average: 2.94
batch number: 9  add: 1255  replay_len: 37330/4194304
batch     0,  loss 0.012216, eval 1.466021
batches: 9,  total time: 3.56,  1k average: 3.56
batch number: 9  add: 1255  replay_len: 38585/4194304
batch     0,  loss 0.013851, eval 1.493107
batches: 9,  total time: 2.79,  1k average: 2.79
batch number: 9  add: 1255  replay_len: 39840/4194304
batch     0,  loss 0.009318, eval 1.508194
batches: 9,  total time: 2.99,  1k average: 2.99
batch number: 9  add: 1255  replay_len: 41095/4194304
batch     0,  loss 0.011768, eval 1.505171
batches: 9,  total time: 2.65,  1k average: 2.65
batch number: 9  add: 1255  replay_len: 42350/4194304
batch     0,  loss 0.008957, eval 1.540277
batches: 9,  total time: 2.89,  1k average: 2.89
batch number: 9  add: 1255  replay_len: 43605/4194304
batch     0,  loss 0.012968, eval 1.531718
batches: 9,  total time: 2.76,  1k average: 2.76
batch number: 9  add: 1255  replay_len: 44860/4194304
batch     0,  loss 0.010548, eval 1.556730
batches: 9,  total time: 3.28,  1k average: 3.28
batch number: 7  add: 985  replay_len: 45845/4194304
batch     0,  loss 0.014417, eval 1.572732
batches: 7,  total time: 2.24,  1k average: 2.24
batch number: 7  add: 940  replay_len: 46785/4194304
batch     0,  loss 0.015436, eval 1.633120
batches: 7,  total time: 2.15,  1k average: 2.15
batch number: 9  add: 1255  replay_len: 48040/4194304
batch     0,  loss 0.012110, eval 1.598730
batches: 9,  total time: 2.56,  1k average: 2.56
batch number: 9  add: 1255  replay_len: 49295/4194304
batch     0,  loss 0.008848, eval 1.604547
batches: 9,  total time: 2.54,  1k average: 2.54
batch number: 9  add: 1255  replay_len: 50550/4194304
batch     0,  loss 0.015339, eval 1.632174
batches: 9,  total time: 2.58,  1k average: 2.58
batch number: 7  add: 985  replay_len: 51535/4194304
batch     0,  loss 0.013589, eval 1.617255
batches: 7,  total time: 1.93,  1k average: 1.93
batch number: 9  add: 1255  replay_len: 52790/4194304
batch     0,  loss 0.011445, eval 1.642192
batches: 9,  total time: 3.13,  1k average: 3.13
batch number: 5  add: 710  replay_len: 53500/4194304
batch     0,  loss 0.016326, eval 1.652851
batches: 5,  total time: 1.52,  1k average: 1.52
batch number: 6  add: 780  replay_len: 54280/4194304
batch     0,  loss 0.012709, eval 1.676450
batches: 6,  total time: 1.93,  1k average: 1.93
batch number: 9  add: 1255  replay_len: 55535/4194304
batch     0,  loss 0.014393, eval 1.698269
batches: 9,  total time: 2.98,  1k average: 2.98
batch number: 4  add: 605  replay_len: 56140/4194304
batch     0,  loss 0.013901, eval 1.751455
batches: 4,  total time: 1.56,  1k average: 1.56
batch number: 9  add: 1255  replay_len: 57395/4194304
batch     0,  loss 0.014939, eval 1.787189
batches: 9,  total time: 3.04,  1k average: 3.04
batch number: 9  add: 1235  replay_len: 58630/4194304
batch     0,  loss 0.015501, eval 1.831064
batches: 9,  total time: 2.90,  1k average: 2.90
batch number: 5  add: 640  replay_len: 59270/4194304
batch     0,  loss 0.015025, eval 1.845547
batches: 5,  total time: 1.62,  1k average: 1.62
batch number: 7  add: 955  replay_len: 60225/4194304
batch     0,  loss 0.012546, eval 1.865726
batches: 7,  total time: 2.20,  1k average: 2.20
batch number: 9  add: 1230  replay_len: 61455/4194304
batch     0,  loss 0.015067, eval 1.918255
batches: 9,  total time: 3.00,  1k average: 3.00
batch number: 9  add: 1255  replay_len: 62710/4194304
batch     0,  loss 0.015521, eval 1.900689
batches: 9,  total time: 4.40,  1k average: 4.40
batch number: 5  add: 750  replay_len: 63460/4194304
batch     0,  loss 0.017983, eval 1.908518
batches: 5,  total time: 1.52,  1k average: 1.52
batch number: 9  add: 1255  replay_len: 64715/4194304
batch     0,  loss 0.014258, eval 1.906589
batches: 9,  total time: 3.03,  1k average: 3.03
batch number: 3  add: 400  replay_len: 65115/4194304batch number: 6  add: 832  replay_len: 832/4194304
batch     0,  loss 0.046890, eval 0.043012
batches: 6,  total time: 2.80,  1k average: 2.80
batch number: 6  add: 892  replay_len: 1724/4194304
batch     0,  loss 0.020954, eval 0.191881
batches: 6,  total time: 1.76,  1k average: 1.76
batch number: 6  add: 888  replay_len: 2612/4194304
batch     0,  loss 0.013579, eval 0.284871
batches: 6,  total time: 2.95,  1k average: 2.95
batch number: 5  add: 648  replay_len: 3260/4194304
batch     0,  loss 0.012520, eval 0.379487
batches: 5,  total time: 1.47,  1k average: 1.47
batch number: 5  add: 734  replay_len: 3994/4194304
batch     0,  loss 0.014495, eval 0.477230
batches: 5,  total time: 1.67,  1k average: 1.67
batch number: 6  add: 827  replay_len: 4821/4194304
batch     0,  loss 0.020675, eval 0.570307
batches: 6,  total time: 1.74,  1k average: 1.74
batch number: 5  add: 690  replay_len: 5511/4194304
batch     0,  loss 0.020632, eval 0.685321
batches: 5,  total time: 1.77,  1k average: 1.77
batch number: 4  add: 548  replay_len: 6059/4194304
batch     0,  loss 0.017319, eval 0.808097
batches: 4,  total time: 1.33,  1k average: 1.33
batch number: 5  add: 733  replay_len: 6792/4194304
batch     0,  loss 0.022455, eval 0.917517
batches: 5,  total time: 1.90,  1k average: 1.90
batch number: 8  add: 1040  replay_len: 7832/4194304
batch     0,  loss 0.034425, eval 1.027985
batches: 8,  total time: 2.66,  1k average: 2.66
batch number: 6  add: 818  replay_len: 8650/4194304
batch     0,  loss 0.038195, eval 1.167235
batches: 6,  total time: 2.70,  1k average: 2.70
batch number: 6  add: 824  replay_len: 9474/4194304
batch     0,  loss 0.029953, eval 1.261731
batches: 6,  total time: 2.25,  1k average: 2.25
batch number: 6  add: 825  replay_len: 10299/4194304
batch     0,  loss 0.029582, eval 1.345232
batches: 6,  total time: 2.83,  1k average: 2.83
batch number: 4  add: 568  replay_len: 10867/4194304
batch     0,  loss 0.024872, eval 1.450233
batches: 4,  total time: 1.27,  1k average: 1.27
batch number: 7  add: 988  replay_len: 11855/4194304
batch     0,  loss 0.041063, eval 1.530051
batches: 7,  total time: 2.52,  1k average: 2.52
batch number: 6  add: 889  replay_len: 12744/4194304
batch     0,  loss 0.044818, eval 1.634609
batches: 6,  total time: 2.57,  1k average: 2.57
batch number: 6  add: 808  replay_len: 13552/4194304
batch     0,  loss 0.056431, eval 1.666288
batches: 6,  total time: 2.54,  1k average: 2.54
batch number: 8  add: 1059  replay_len: 14611/4194304
batch     0,  loss 0.028241, eval 1.803375
batches: 8,  total time: 2.26,  1k average: 2.26
batch number: 7  add: 946  replay_len: 15557/4194304
batch     0,  loss 0.030578, eval 1.900453
batches: 7,  total time: 2.04,  1k average: 2.04
batch number: 5  add: 689  replay_len: 16246/4194304
batch     0,  loss 0.044163, eval 1.996820
batches: 5,  total time: 1.44,  1k average: 1.44
batch number: 4  add: 518  replay_len: 16764/4194304
batch     0,  loss 0.058777, eval 2.066072
batches: 4,  total time: 1.17,  1k average: 1.17
batch number: 5  add: 721  replay_len: 17485/4194304
batch     0,  loss 0.037668, eval 2.226814
batches: 5,  total time: 1.39,  1k average: 1.39
batch number: 5  add: 671  replay_len: 18156/4194304
batch     0,  loss 0.067617, eval 2.338794
batches: 5,  total time: 1.75,  1k average: 1.75
batch number: 4  add: 595  replay_len: 18751/4194304
batch     0,  loss 0.061449, eval 2.416551
batches: 4,  total time: 1.21,  1k average: 1.21
batch number: 6  add: 868  replay_len: 19619/4194304
batch     0,  loss 0.047979, eval 2.553919
batches: 6,  total time: 1.84,  1k average: 1.84
batch number: 5  add: 700  replay_len: 20319/4194304
batch     0,  loss 0.075249, eval 2.638999
batches: 5,  total time: 1.49,  1k average: 1.49
batch number: 6  add: 888  replay_len: 21207/4194304
batch     0,  loss 0.048567, eval 2.727093
batches: 6,  total time: 1.79,  1k average: 1.79
batch number: 4  add: 623  replay_len: 21830/4194304
batch     0,  loss 0.076431, eval 2.859947
batches: 4,  total time: 1.15,  1k average: 1.15
batch number: 3  add: 466  replay_len: 22296/4194304
batch     0,  loss 0.086070, eval 2.906374
batches: 3,  total time: 1.07,  1k average: 1.07
batch number: 5  add: 650  replay_len: 22946/4194304
batch     0,  loss 0.087022, eval 2.954023
batches: 5,  total time: 1.75,  1k average: 1.75
batch number: 8  add: 1112  replay_len: 24058/4194304
batch     0,  loss 0.073553, eval 3.087478
batches: 8,  total time: 2.99,  1k average: 2.99
batch number: 6  add: 771  replay_len: 24829/4194304
batch     0,  loss 0.112035, eval 3.165350
batches: 6,  total time: 1.82,  1k average: 1.82
batch number: 7  add: 944  replay_len: 25773/4194304
batch     0,  loss 0.066720, eval 3.289355
batches: 7,  total time: 2.19,  1k average: 2.19
batch number: 4  add: 594  replay_len: 26367/4194304
batch     0,  loss 0.127934, eval 3.378640
batches: 4,  total time: 1.28,  1k average: 1.28
batch number: 7  add: 943  replay_len: 27310/4194304
batch     0,  loss 0.084017, eval 3.434375
batches: 7,  total time: 2.00,  1k average: 2.00
batch number: 6  add: 783  replay_len: 28093/4194304
batch     0,  loss 0.057002, eval 3.464241
batches: 6,  total time: 1.77,  1k average: 1.77
batch number: 8  add: 1076  replay_len: 29169/4194304
batch     0,  loss 0.070524, eval 3.523600
batches: 8,  total time: 2.52,  1k average: 2.52
batch number: 4  add: 536  replay_len: 29705/4194304
batch     0,  loss 0.052591, eval 3.666612
batches: 4,  total time: 1.36,  1k average: 1.36
batch number: 3  add: 417  replay_len: 30122/4194304
batch     0,  loss 0.078675, eval 3.639933
batches: 3,  total time: 0.99,  1k average: 0.99
batch number: 4  add: 522  replay_len: 30644/4194304
batch     0,  loss 0.063546, eval 3.709903
batches: 4,  total time: 1.28,  1k average: 1.28
batch number: 4  add: 603  replay_len: 31247/4194304
batch     0,  loss 0.063206, eval 3.824468
batches: 4,  total time: 1.19,  1k average: 1.19
batch number: 5  add: 657  replay_len: 31904/4194304
batch     0,  loss 0.066567, eval 3.951648
batches: 5,  total time: 1.45,  1k average: 1.45
batch number: 2  add: 349  replay_len: 32253/4194304
batch     0,  loss 0.096885, eval 4.029519
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 7  add: 1004  replay_len: 33257/4194304
batch     0,  loss 0.135255, eval 4.107949
batches: 7,  total time: 2.31,  1k average: 2.31
batch number: 3  add: 405  replay_len: 33662/4194304
batch     0,  loss 0.156337, eval 4.124060
batches: 3,  total time: 0.94,  1k average: 0.94
batch number: 3  add: 499  replay_len: 34161/4194304
batch     0,  loss 0.117949, eval 4.172120
batches: 3,  total time: 0.99,  1k average: 0.99
batch number: 3  add: 482  replay_len: 34643/4194304
batch     0,  loss 0.058138, eval 4.214842
batches: 3,  total time: 0.90,  1k average: 0.90
batch number: 2  add: 364  replay_len: 35007/4194304
batch     0,  loss 0.181158, eval 4.276644
batches: 2,  total time: 0.73,  1k average: 0.73
batch number: 6  add: 805  replay_len: 35812/4194304
batch     0,  loss 0.147167, eval 4.379883
batches: 6,  total time: 2.17,  1k average: 2.17
batch number: 3  add: 497  replay_len: 36309/4194304
batch     0,  loss 0.144725, eval 4.459225
batches: 3,  total time: 0.91,  1k average: 0.91
batch number: 2  add: 317  replay_len: 36626/4194304
batch     0,  loss 0.160120, eval 4.418551
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 4  add: 639  replay_len: 37265/4194304
batch     0,  loss 0.234028, eval 4.414421
batches: 4,  total time: 1.29,  1k average: 1.29
batch number: 5  add: 641  replay_len: 37906/4194304
batch     0,  loss 0.248793, eval 4.405081
batches: 5,  total time: 1.54,  1k average: 1.54
batch number: 3  add: 427  replay_len: 38333/4194304
batch     0,  loss 0.192751, eval 4.522421
batches: 3,  total time: 1.48,  1k average: 1.48
batch number: 1  add: 239  replay_len: 38572/4194304
batch     0,  loss 0.153172, eval 4.542446
batches: 1,  total time: 0.39,  1k average: 0.39
batch number: 3  add: 475  replay_len: 39047/4194304
batch     0,  loss 0.227619, eval 4.572286
batches: 3,  total time: 1.01,  1k average: 1.01
batch number: 1  add: 208  replay_len: 39255/4194304
batch     0,  loss 0.118976, eval 4.575413round 56	 loss: [0.02, 0.12]	 reward: [-33.4, -10.0]	 value: [2.0, 4.58]
round 57	 loss: [0.02, 0.12]	 reward: [-16.6, -10.0]	 value: [2.06, 4.65]
round 58	 loss: [0.02, 0.15]	 reward: [-77.2, -10.0]	 value: [2.1, 4.67]
round 59	 loss: [0.02, 0.2]	 reward: [-124.8, -9.0]	 value: [2.11, 4.76]
round 60	 loss: [0.02, 0.17]	 reward: [-126.0, -10.0]	 value: [2.14, 4.69]
round 61	 loss: [0.02, 0.11]	 reward: [-32.2, -10.0]	 value: [2.15, 4.75]
round 62	 loss: [0.02, 0.11]	 reward: [-27.2, -10.0]	 value: [2.24, 4.76]
round 63	 loss: [0.02, 0.22]	 reward: [-21.2, -10.0]	 value: [2.31, 4.82]
round 64	 loss: [0.03, 0.1]	 reward: [-19.2, -10.0]	 value: [2.4, 4.89]
round 65	 loss: [0.03, 0.18]	 reward: [-22.6, -10.0]	 value: [2.51, 4.91]
round 66	 loss: [0.03, 0.29]	 reward: [-53.0, -10.0]	 value: [2.67, 5.04]
round 67	 loss: [0.03, 0]	 reward: [-8.4, -10.0]	 value: [2.63, 0]
round 68	 loss: [0.03, 0.26]	 reward: [-18.4, -10.0]	 value: [2.65, 5.04]
round 69	 loss: [0.03, 0.11]	 reward: [-37.0, -10.0]	 value: [2.77, 5.16]

step 100,  reward: [-0.8  0. ],  total_reward: [-46.  -9.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-76.8  -9. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-103.6   -9. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-127.  -10.] 
steps: 251,  total time: 1.42,  step average 0.01
===== train =====
train_time 3.05
round time 4.47  total time 211.16

===== sample =====
eps 0.804 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-22.  -7.] 
step 100,  reward: [-1.  0.],  total_reward: [-47.  -7.] 
step 150,  reward: [-0.8  0. ],  total_reward: [-75.2  -8. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-101.6   -9. ] 
steps: 247,  total time: 4.24,  step average 0.02
===== train =====
train_time 2.92
round time 7.15  total time 218.31

save model... 
===== sample =====
eps 0.8 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-19.4  -8. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-48.4  -8. ] 
steps: 128,  total time: 0.57,  step average 0.00
===== train =====
train_time 1.63
round time 2.20  total time 221.23

===== sample =====
eps 0.796 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-25.2  -4. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-48.4  -6. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-72.8  -8. ] 
steps: 191,  total time: 1.50,  step average 0.01
===== train =====
train_time 2.21
round time 3.71  total time 224.94

save model... 
===== sample =====
eps 0.792 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-23.2  -4. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-48.8  -6. ] 
step 150,  reward: [ 1.2 -1. ],  total_reward: [-71.4  -9. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-97.6  -9. ] 
steps: 246,  total time: 1.00,  step average 0.00
===== train =====
train_time 3.03
round time 4.02  total time 229.66

===== sample =====
eps 0.788 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-19.8  -7. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-44.6  -8. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-70.2  -8. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-98.  -8.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-123.8   -8. ] 
steps: 251,  total time: 1.20,  step average 0.00
===== train =====
train_time 4.42
round time 5.62  total time 235.28

save model... 
===== sample =====
eps 0.784 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-18.  -8.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-49.  -8.] 
steps: 150,  total time: 0.81,  step average 0.01
===== train =====
train_time 1.55
round time 2.36  total time 238.41

===== sample =====
eps 0.78 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-23.2  -8. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-48.2 -10. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-76.2 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-104.  -10.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-132.6  -10. ] 
steps: 251,  total time: 1.49,  step average 0.01
===== train =====
train_time 3.05
round time 4.53  total time 242.95

save model... 
===== sample =====
eps 0.776 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-18. -10.] 
steps: 80,  total time: 0.52,  step average 0.01
===== train =====
train_time 1.15
round time 1.68  total time 245.64

===== sample =====
eps 0.772 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-13.8 -10. ] 
steps: 58,  total time: 0.48,  step average 0.01
===== train =====
train_time 0.66
round time 1.14  total time 246.78

save model... 
===== sample =====
eps 0.768 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-20.6  -8. ] 
step 100,  reward: [ 0.4 -1. ],  total_reward: [-49.4  -9. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-75.2 -10. ] 
steps: 158,  total time: 0.69,  step average 0.00
===== train =====
train_time 1.91
round time 2.60  total time 250.06

===== sample =====
eps 0.764 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [-22.  -5.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-49.2  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-78.4  -5. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-98.8  -9. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-124.8   -9. ] 
steps: 251,  total time: 4.87,  step average 0.02
===== train =====
train_time 2.79
round time 7.66  total time 257.71

save model... 
===== sample =====
eps 0.76 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-19.  -9.] 
step 100,  reward: [-0.6  0. ],  total_reward: [-43.8 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-71.2 -10. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-96.8 -10. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-126.  -10.] 
steps: 251,  total time: 1.25,  step average 0.00
===== train =====
train_time 2.48
round time 3.74  total time 262.04

===== sample =====
eps 0.756 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-19.6  -7. ] 
steps: 84,  total time: 0.35,  step average 0.00
===== train =====
train_time 0.89
round time 1.24  total time 263.28

save model... 
===== sample =====
eps 0.752 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-20.  -8.] 
steps: 70,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.59
round time 0.84  total time 264.78

===== sample =====
eps 0.748 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-11.2 -10. ] 
steps: 73,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.58
round time 0.85  total time 265.63

save model... 
===== sample =====
eps 0.744 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [ 0.4 -1. ],  total_reward: [-16.6  -9. ] 
steps: 62,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.58
round time 0.82  total time 267.22

===== sample =====
eps 0.74 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-20.  -8.] 
steps: 61,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.56
round time 0.78  total time 268.00

save model... 
===== sample =====
eps 0.736 number [5, 5]
step   0,  reward: [ 0. -1.],  total_reward: [ 0. -1.] 
step  50,  reward: [-0.8  0. ],  total_reward: [-25.2  -3. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-43.  -9.] 
steps: 125,  total time: 0.45,  step average 0.00
===== train =====
train_time 1.14
round time 1.58  total time 270.30

===== sample =====
eps 0.732 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
steps: 41,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.23
round time 0.37  total time 270.67

save model... 
===== sample =====
eps 0.728 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-18.8  -6. ] 
steps: 60,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.55
round time 0.76  total time 272.10

===== sample =====
eps 0.724 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-16.4  -8. ] 
steps: 93,  total time: 1.46,  step average 0.02
===== train =====
train_time 0.83
round time 2.29  total time 274.39
round 70	 loss: [0.03, 0.22]	 reward: [-52.0, -10.0]	 value: [2.86, 5.2]
round 71	 loss: [0.03, 0.24]	 reward: [-120.4, -9.0]	 value: [2.93, 5.18]
round 72	 loss: [0.04, 0.2]	 reward: [-127.2, -6.0]	 value: [2.97, 5.22]
round 73	 loss: [0.04, 0.17]	 reward: [-113.0, -10.0]	 value: [3.05, 5.24]
round 74	 loss: [0.04, 0]	 reward: [-3.4, -10.0]	 value: [3.05, 0]
round 75	 loss: [0.04, 0.14]	 reward: [-11.0, -10.0]	 value: [3.11, 5.23]
round 76	 loss: [0.05, 0.24]	 reward: [-30.6, -10.0]	 value: [3.3, 5.27]
round 77	 loss: [0.06, 0.27]	 reward: [-32.2, -10.0]	 value: [3.47, 5.28]
round 78	 loss: [0.05, 0.25]	 reward: [-116.6, -2.0]	 value: [3.47, 5.26]
round 79	 loss: [0.06, 0.2]	 reward: [-39.2, -10.0]	 value: [3.64, 5.26]
round 80	 loss: [0.06, 0.31]	 reward: [-7.8, -10.0]	 value: [3.63, 5.27]
round 81	 loss: [0.06, 0.31]	 reward: [-10.2, -10.0]	 value: [3.7, 5.29]
round 82	 loss: [0.06, 0.23]	 reward: [-37.8, -10.0]	 value: [3.68, 5.3]
round 83	 loss: [0.06, 0]	 reward: [0.0, -10.0]	 value: [3.64, 0]
round 84	 loss: [0.06, 0]	 reward: [-3.6, -10.0]	 value: [3.77, 0]
round 85	 loss: [0.06, 0]	 reward: [-0.8, -10.0]	 value: [3.76, 0]
round 86	 loss: [0.07, 0.19]	 reward: [-10.8, -10.0]	 value: [3.81, 5.27]
round 87	 loss: [0.06, 0]	 reward: [-11.4, -10.0]	 value: [3.8, 0]
round 88	 loss: [0.08, 0]	 reward: [-4.8, -10.0]	 value: [3.93, 0]
round 89	 loss: [0.08, 0]	 reward: [-9.4, -10.0]	 value: [4.02, 0]
round 90	 loss: [0.08, 0]	 reward: [-10.6, -10.0]	 value: [4.33, 0]
round 91	 loss: [0.09, 0.24]	 reward: [-17.4, -10.0]	 value: [4.64, 5.29]
round 92	 loss: [0.09, 0.28]	 reward: [-98.4, -6.0]	 value: [4.76, 5.19]
round 93	 loss: [0.09, 0.26]	 reward: [-8.4, -10.0]	 value: [4.7, 5.33]
round 94	 loss: [0.11, 0.22]	 reward: [-10.2, -10.0]	 value: [5.02, 5.33]

save model... 
===== sample =====
eps 0.72 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-20.2  -7. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-43.2  -8. ] 
steps: 124,  total time: 0.45,  step average 0.00
===== train =====
train_time 1.39
round time 1.84  total time 276.87

===== sample =====
eps 0.716 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-1.  0.],  total_reward: [-21.2  -5. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-47.6  -5. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-74.2  -5. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-100.   -6.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-120.4   -9. ] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 2.71
round time 3.71  total time 280.58

save model... 
===== sample =====
eps 0.712 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-27.6   0. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-51.6  -2. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-74.2  -4. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-98.4  -6. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-127.2   -6. ] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.83
round time 3.74  total time 284.96

===== sample =====
eps 0.708 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-15.  -8.] 
step 100,  reward: [-0.6  0. ],  total_reward: [-37.6 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-60.6 -10. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-87.2 -10. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-113.  -10.] 
steps: 251,  total time: 1.14,  step average 0.00
===== train =====
train_time 2.35
round time 3.49  total time 288.45

save model... 
===== sample =====
eps 0.704 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 34,  total time: 0.24,  step average 0.01
===== train =====
train_time 0.27
round time 0.51  total time 289.81

===== sample =====
eps 0.7 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 48,  total time: 0.36,  step average 0.01
===== train =====
train_time 0.36
round time 0.72  total time 290.53

save model... 
===== sample =====
eps 0.696 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-17.4  -8. ] 
steps: 83,  total time: 0.32,  step average 0.00
===== train =====
train_time 0.81
round time 1.14  total time 292.37

===== sample =====
eps 0.692 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-17.2  -7. ] 
steps: 92,  total time: 0.32,  step average 0.00
===== train =====
train_time 0.77
round time 1.09  total time 293.46

save model... 
===== sample =====
eps 0.688 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-21.4  -2. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-44.2  -2. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-66.2  -2. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-90.6  -2. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-116.6   -2. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.81
round time 3.70  total time 297.85

===== sample =====
eps 0.6839999999999999 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-14.2  -8. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-37.6  -8. ] 
steps: 110,  total time: 1.80,  step average 0.02
===== train =====
train_time 1.06
round time 2.86  total time 300.71

save model... 
===== sample =====
eps 0.6799999999999999 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 49,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.35
round time 0.55  total time 302.02

===== sample =====
eps 0.6759999999999999 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 46,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.32
round time 0.49  total time 302.51

save model... 
===== sample =====
eps 0.6719999999999999 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-11.4 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-35.8 -10. ] 
steps: 107,  total time: 0.40,  step average 0.00
===== train =====
train_time 1.03
round time 1.43  total time 304.80

===== sample =====
eps 0.6679999999999999 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 32,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 305.14

save model... 
===== sample =====
eps 0.6639999999999999 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 36,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.23
round time 0.38  total time 306.31

===== sample =====
eps 0.6599999999999999 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 31,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.33  total time 306.65

save model... 
===== sample =====
eps 0.6559999999999999 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 45,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.35
round time 0.53  total time 308.08

===== sample =====
eps 0.6519999999999999 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 50,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.23
round time 0.41  total time 308.49

save model... 
===== sample =====
eps 0.648 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 41,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.40  total time 309.62

===== sample =====
eps 0.644 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
steps: 45,  total time: 0.59,  step average 0.01
===== train =====
train_time 0.25
round time 0.83  total time 310.45

save model... 
===== sample =====
eps 0.64 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-10.8 -10. ] 
steps: 53,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.49
round time 0.70  total time 312.06

===== sample =====
eps 0.636 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-16.8  -8. ] 
steps: 60,  total time: 0.26,  step average 0.00
===== train =====
train_time 0.71
round time 0.97  total time 313.03

save model... 
===== sample =====
eps 0.632 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-14.4  -6. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-37.  -6.] 
step 150,  reward: [-0.6  0. ],  total_reward: [-57.  -6.] 
step 200,  reward: [-0.2  0. ],  total_reward: [-78.  -6.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-98.4  -6. ] 
steps: 251,  total time: 1.08,  step average 0.00
===== train =====
train_time 2.80
round time 3.88  total time 317.72

===== sample =====
eps 0.628 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 46,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.34
round time 0.50  total time 318.22

save model... 
===== sample =====
eps 0.624 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-10.8 -10. ] 
steps: 52,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.64
round time 0.87  total time 319.87

===== sample =====
eps 0.62 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-11.8 -10. ] round 95	 loss: [0.11, 0.27]	 reward: [-102.4, -10.0]	 value: [5.25, 5.27]
round 96	 loss: [0.12, 0]	 reward: [-0.8, -10.0]	 value: [5.06, 0]
round 97	 loss: [0.13, 0.18]	 reward: [-14.2, -10.0]	 value: [5.43, 5.29]
round 98	 loss: [0.12, 0.27]	 reward: [-7.2, -10.0]	 value: [5.44, 5.29]
round 99	 loss: [0.12, 0.26]	 reward: [-19.0, -10.0]	 value: [5.77, 5.31]
round 100	 loss: [0.16, 0.34]	 reward: [-18.8, -10.0]	 value: [6.02, 5.23]
round 101	 loss: [0.12, 0.19]	 reward: [-6.0, -10.0]	 value: [6.02, 5.34]
round 102	 loss: [0.14, 0.29]	 reward: [1.2, -10.0]	 value: [6.11, 5.28]
round 103	 loss: [0.16, 0.22]	 reward: [-22.8, -10.0]	 value: [6.46, 5.14]
round 104	 loss: [0, 0]	 reward: [4.0, -10.0]	 value: [0, 0]
round 105	 loss: [0.25, 0.18]	 reward: [-2.2, -10.0]	 value: [6.51, 5.13]
round 106	 loss: [0.17, 0.27]	 reward: [-13.6, -10.0]	 value: [6.83, 5.06]
round 107	 loss: [0.17, 0.25]	 reward: [-10.2, -10.0]	 value: [7.15, 5.06]
round 108	 loss: [0.17, 0.24]	 reward: [-3.2, -10.0]	 value: [7.04, 5.02]
round 109	 loss: [0.21, 0.26]	 reward: [-0.4, -10.0]	 value: [7.29, 4.94]
round 110	 loss: [0.16, 0.2]	 reward: [-0.8, -10.0]	 value: [7.35, 4.89]
round 111	 loss: [0.21, 0.39]	 reward: [-2.6, -10.0]	 value: [7.64, 4.86]
round 112	 loss: [0.2, 0]	 reward: [0.6, -10.0]	 value: [7.92, 0]

batch     0,  loss 0.018199, eval 1.975882
batches: 3,  total time: 1.15,  1k average: 1.15
batch number: 2  add: 290  replay_len: 65405/4194304
batch     0,  loss 0.015673, eval 1.996026
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 6  add: 790  replay_len: 66195/4194304
batch     0,  loss 0.016577, eval 2.038766
batches: 6,  total time: 1.90,  1k average: 1.90
batch number: 9  add: 1255  replay_len: 67450/4194304
batch     0,  loss 0.021436, eval 2.112293
batches: 9,  total time: 2.77,  1k average: 2.77
batch number: 9  add: 1255  replay_len: 68705/4194304
batch     0,  loss 0.018814, eval 2.085197
batches: 9,  total time: 2.47,  1k average: 2.47
batch number: 3  add: 420  replay_len: 69125/4194304
batch     0,  loss 0.019361, eval 2.132356
batches: 3,  total time: 0.89,  1k average: 0.89
batch number: 2  add: 350  replay_len: 69475/4194304
batch     0,  loss 0.024324, eval 2.177226
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 2  add: 365  replay_len: 69840/4194304
batch     0,  loss 0.020542, eval 2.276922
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 2  add: 310  replay_len: 70150/4194304
batch     0,  loss 0.021568, eval 2.312529
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 2  add: 305  replay_len: 70455/4194304
batch     0,  loss 0.028265, eval 2.388453
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 4  add: 625  replay_len: 71080/4194304
batch     0,  loss 0.027032, eval 2.515583
batches: 4,  total time: 1.13,  1k average: 1.13
batch number: 1  add: 205  replay_len: 71285/4194304
batch     0,  loss 0.034965, eval 2.629131
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 2  add: 300  replay_len: 71585/4194304
batch     0,  loss 0.034651, eval 2.743014
batches: 2,  total time: 0.55,  1k average: 0.55
batch number: 3  add: 465  replay_len: 72050/4194304
batch     0,  loss 0.028915, eval 2.682036
batches: 3,  total time: 0.82,  1k average: 0.82
batch number: 4  add: 620  replay_len: 72670/4194304
batch     0,  loss 0.032364, eval 2.741980
batches: 4,  total time: 1.38,  1k average: 1.38
batch number: 9  add: 1255  replay_len: 73925/4194304
batch     0,  loss 0.035150, eval 2.874635
batches: 9,  total time: 2.69,  1k average: 2.69
batch number: 9  add: 1255  replay_len: 75180/4194304
batch     0,  loss 0.035403, eval 2.949831
batches: 9,  total time: 2.82,  1k average: 2.82
batch number: 9  add: 1255  replay_len: 76435/4194304
batch     0,  loss 0.036213, eval 2.965009
batches: 9,  total time: 2.33,  1k average: 2.33
batch number: 1  add: 170  replay_len: 76605/4194304
batch     0,  loss 0.043543, eval 3.047414
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 240  replay_len: 76845/4194304
batch     0,  loss 0.042038, eval 3.105341
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 3  add: 415  replay_len: 77260/4194304
batch     0,  loss 0.049332, eval 3.228130
batches: 3,  total time: 0.81,  1k average: 0.81
batch number: 3  add: 460  replay_len: 77720/4194304
batch     0,  loss 0.073970, eval 3.355397
batches: 3,  total time: 0.77,  1k average: 0.77
batch number: 9  add: 1255  replay_len: 78975/4194304
batch     0,  loss 0.043789, eval 3.450274
batches: 9,  total time: 2.79,  1k average: 2.79
batch number: 4  add: 550  replay_len: 79525/4194304
batch     0,  loss 0.056523, eval 3.555287
batches: 4,  total time: 1.05,  1k average: 1.05
batch number: 1  add: 245  replay_len: 79770/4194304
batch     0,  loss 0.058696, eval 3.630273
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 230  replay_len: 80000/4194304
batch     0,  loss 0.057316, eval 3.700745
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 4  add: 535  replay_len: 80535/4194304
batch     0,  loss 0.057556, eval 3.676976
batches: 4,  total time: 1.02,  1k average: 1.02
batch number: 1  add: 160  replay_len: 80695/4194304
batch     0,  loss 0.059139, eval 3.638582
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 180  replay_len: 80875/4194304
batch     0,  loss 0.055990, eval 3.769602
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 155  replay_len: 81030/4194304
batch     0,  loss 0.062119, eval 3.756409
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 225  replay_len: 81255/4194304
batch     0,  loss 0.065394, eval 3.805566
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 250  replay_len: 81505/4194304
batch     0,  loss 0.063635, eval 3.803742
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 205  replay_len: 81710/4194304
batch     0,  loss 0.076266, eval 3.930851
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 225  replay_len: 81935/4194304
batch     0,  loss 0.079568, eval 4.022805
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 265  replay_len: 82200/4194304
batch     0,  loss 0.072109, eval 4.120396
batches: 2,  total time: 0.49,  1k average: 0.49
batch number: 2  add: 300  replay_len: 82500/4194304
batch     0,  loss 0.087568, eval 4.328698
batches: 2,  total time: 0.70,  1k average: 0.70
batch number: 9  add: 1255  replay_len: 83755/4194304
batch     0,  loss 0.084701, eval 4.598019
batches: 9,  total time: 2.79,  1k average: 2.79
batch number: 1  add: 230  replay_len: 83985/4194304
batch     0,  loss 0.086374, eval 4.703022
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 260  replay_len: 84245/4194304
batch     0,  loss 0.122298, eval 4.931438
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 9  add: 1255  replay_len: 85500/4194304
batch     0,  loss 0.091899, eval 5.027246
batches: 9,  total time: 2.24,  1k average: 2.24
batch number: 1  add: 155  replay_len: 85655/4194304
batch     0,  loss 0.116726, eval 5.060822
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 280  replay_len: 85935/4194304
batch     0,  loss 0.129706, eval 5.279538
batches: 2,  total time: 0.55,  1k average: 0.55
batch number: 1  add: 250  replay_len: 86185/4194304
batch     0,  loss 0.121228, eval 5.436313
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 3  add: 390  replay_len: 86575/4194304
batch     0,  loss 0.107772, eval 5.583424
batches: 3,  total time: 0.94,  1k average: 0.94
batch number: 2  add: 365  replay_len: 86940/4194304
batch     0,  loss 0.182134, eval 5.787027
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 225  replay_len: 87165/4194304
batch     0,  loss 0.119960, eval 6.020489
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 160  replay_len: 87325/4194304
batch     0,  loss 0.137633, eval 6.106693
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 3  add: 435  replay_len: 87760/4194304
batch     0,  loss 0.141291, eval 6.230460
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 1  add: 175  replay_len: 88045/4194304
batch     0,  loss 0.253167, eval 6.507254
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 2  add: 305  replay_len: 88350/4194304
batch     0,  loss 0.174895, eval 6.793808
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 2  add: 295  replay_len: 88645/4194304
batch     0,  loss 0.149552, eval 6.877311
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 1  add: 200  replay_len: 88845/4194304
batch     0,  loss 0.166688, eval 7.043568
batches: 1,  total time: 0.37,  1k average: 0.37
batch number: 1  add: 160  replay_len: 89005/4194304
batch     0,  loss 0.209114, eval 7.293929
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 1  add: 185  replay_len: 89190/4194304
batch     0,  loss 0.159980, eval 7.348695
batches: 1,  total time: 0.40,  1k average: 0.40
batch number: 1  add: 195  replay_len: 89385/4194304
batch     0,  loss 0.208406, eval 7.636389
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 160  replay_len: 89545/4194304
batch     0,  loss 0.198708, eval 7.924328
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 190  replay_len: 89735/4194304
batch     0,  loss 0.259648, eval 8.234395
batches: 1,  total time: 0.39,  1k average: 0.39round 113	 loss: [0.26, 0.35]	 reward: [-0.8, -10.0]	 value: [8.23, 4.76]
round 114	 loss: [0.22, 0]	 reward: [-7.2, -10.0]	 value: [8.62, 0]
round 115	 loss: [0.23, 0]	 reward: [2.0, -10.0]	 value: [8.97, 0]
round 116	 loss: [0.36, 0.23]	 reward: [-15.4, -10.0]	 value: [9.94, 4.66]
round 117	 loss: [0.59, 0]	 reward: [-3.2, -10.0]	 value: [9.99, 0]
round 118	 loss: [0.34, 0]	 reward: [2.0, -10.0]	 value: [10.88, 0]
round 119	 loss: [0.42, 0]	 reward: [-1.4, -10.0]	 value: [11.64, 0]
round 120	 loss: [0.41, 0]	 reward: [-0.4, -10.0]	 value: [11.87, 0]
round 121	 loss: [0.45, 0]	 reward: [2.2, -10.0]	 value: [12.43, 0]
round 122	 loss: [0.55, 0.26]	 reward: [-3.4, -10.0]	 value: [12.98, 4.6]
round 123	 loss: [0.48, 0]	 reward: [-0.4, -10.0]	 value: [13.63, 0]
round 124	 loss: [1.48, 0.21]	 reward: [-33.4, -10.0]	 value: [15.05, 4.44]
round 125	 loss: [1.82, 0.21]	 reward: [-19.6, -10.0]	 value: [17.23, 4.33]

step 100,  reward: [-0.4  0. ],  total_reward: [-36.4 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-57.8 -10. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-78.8 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-102.4  -10. ] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.25
round time 3.15  total time 323.02

save model... 
===== sample =====
eps 0.616 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 31,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.24
round time 0.37  total time 324.18

===== sample =====
eps 0.612 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-13. -10.] 
steps: 56,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.55
round time 0.75  total time 324.93

save model... 
===== sample =====
eps 0.608 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
steps: 50,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.34
round time 0.53  total time 326.32

===== sample =====
eps 0.604 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [-10.4 -10. ] 
steps: 78,  total time: 1.05,  step average 0.01
===== train =====
train_time 0.94
round time 1.99  total time 328.31

save model... 
===== sample =====
eps 0.6 number [5, 5]
step   0,  reward: [ 0. -1.],  total_reward: [ 0. -1.] 
step  50,  reward: [-0.4  0. ],  total_reward: [-10. -10.] 
steps: 73,  total time: 0.29,  step average 0.00
===== train =====
train_time 0.57
round time 0.86  total time 330.06

===== sample =====
eps 0.596 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 45,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.33
round time 0.49  total time 330.56

save model... 
===== sample =====
eps 0.592 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 32,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.34
round time 0.46  total time 331.93

===== sample =====
eps 0.588 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-13.6  -8. ] 
steps: 87,  total time: 0.32,  step average 0.00
===== train =====
train_time 0.87
round time 1.18  total time 333.11

save model... 
===== sample =====
eps 0.584 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
steps: 22,  total time: 0.26,  step average 0.01
===== train =====
train_time 0.01
round time 0.26  total time 334.22

===== sample =====
eps 0.5800000000000001 number [5, 5]
step   0,  reward: [-1.  0.],  total_reward: [-1.  0.] 
steps: 35,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.35
round time 0.50  total time 334.72

save model... 
===== sample =====
eps 0.5760000000000001 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.8  0. ],  total_reward: [-11. -10.] 
steps: 61,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.68
round time 0.93  total time 336.58

===== sample =====
eps 0.5720000000000001 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -7.6 -10. ] 
steps: 59,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.68
round time 0.90  total time 337.49

save model... 
===== sample =====
eps 0.5680000000000001 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 40,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.37
round time 0.53  total time 339.02

===== sample =====
eps 0.5640000000000001 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 32,  total time: 0.39,  step average 0.01
===== train =====
train_time 0.38
round time 0.78  total time 339.79

save model... 
===== sample =====
eps 0.56 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 37,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.40
round time 0.56  total time 341.77

===== sample =====
eps 0.556 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 39,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.34
round time 0.51  total time 342.28

save model... 
===== sample =====
eps 0.552 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 32,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.23
round time 0.36  total time 343.65

===== sample =====
eps 0.548 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
steps: 38,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.39
round time 0.55  total time 344.20

save model... 
===== sample =====
eps 0.544 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 49,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.50
round time 0.70  total time 345.82

===== sample =====
eps 0.54 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 27,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.26
round time 0.44  total time 346.26

save model... 
===== sample =====
eps 0.536 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-12.4  -9. ] 
steps: 69,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.61
round time 0.86  total time 348.30

===== sample =====
eps 0.532 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 41,  total time: 0.29,  step average 0.01
===== train =====
train_time 0.25
round time 0.55  total time 348.85

save model... 
===== sample =====
eps 0.528 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 31,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.25
round time 0.39  total time 350.22

===== sample =====
eps 0.524 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 37,  total time: 0.46,  step average 0.01
===== train =====
train_time 0.23
round time 0.69  total time 350.91

save model... 
===== sample =====
eps 0.52 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 33,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.27
round time 0.43  total time 352.38

===== sample =====
eps 0.516 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 30,  total time: 0.17,  step average 0.01
===== train =====
train_time 0.31
round time 0.48  total time 352.86

save model... 
===== sample =====
eps 0.512 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
steps: 48,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.36
round time 0.56  total time 354.77

===== sample =====
eps 0.508 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
steps: 36,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.27
round time 0.44  total time 355.21

save model... 
===== sample =====
eps 0.504 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-12.4  -4. ] 
step 100,  reward: [0. 0.],  total_reward: [-24.2 -10. ] 
steps: 128,  total time: 0.55,  step average 0.00
===== train =====
train_time 1.43
round time 1.98  total time 358.29

===== sample =====
eps 0.5 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-12.2  -6. ] 
steps: 86,  total time: 1.14,  step average 0.01
===== train =====
train_time 1.18
round time 2.33  total time 360.61

save model... 
===== sample =====
eps 0.496 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-11.4  -7. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-25.6  -9. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-41.8  -9. ] round 126	 loss: [0.69, 0.24]	 reward: [-76.4, -9.0]	 value: [17.07, 4.23]
round 127	 loss: [0.66, 0]	 reward: [-0.8, -10.0]	 value: [17.0, 0]
round 128	 loss: [0.82, 0.25]	 reward: [-8.8, -10.0]	 value: [17.36, 4.22]
round 129	 loss: [0.71, 0.16]	 reward: [-8.0, -10.0]	 value: [17.22, 4.19]
round 130	 loss: [0.72, 0.25]	 reward: [-11.0, -10.0]	 value: [17.28, 4.08]
round 131	 loss: [0.65, 0]	 reward: [-0.8, -10.0]	 value: [17.27, 0]
round 132	 loss: [0.67, 0.25]	 reward: [-10.0, -10.0]	 value: [17.3, 4.19]
round 133	 loss: [0.74, 0.2]	 reward: [-20.2, -10.0]	 value: [17.69, 4.15]
round 134	 loss: [0.69, 0.26]	 reward: [-16.0, -10.0]	 value: [17.78, 4.09]

batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 1  add: 185  replay_len: 39440/4194304
batch     0,  loss 0.119894, eval 4.650262
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 212  replay_len: 39652/4194304
batch     0,  loss 0.150969, eval 4.674133
batches: 1,  total time: 0.50,  1k average: 0.50
batch number: 5  add: 673  replay_len: 40325/4194304
batch     0,  loss 0.229635, eval 4.729469
batches: 5,  total time: 1.70,  1k average: 1.70
batch number: 4  add: 543  replay_len: 40868/4194304
batch     0,  loss 0.151020, eval 4.727581
batches: 4,  total time: 1.16,  1k average: 1.16
batch number: 2  add: 297  replay_len: 41165/4194304
batch     0,  loss 0.086285, eval 4.730192
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 238  replay_len: 41403/4194304
batch     0,  loss 0.105853, eval 4.755318
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 172  replay_len: 41575/4194304
batch     0,  loss 0.221159, eval 4.815440
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 221  replay_len: 41796/4194304
batch     0,  loss 0.100852, eval 4.894519
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 161  replay_len: 41957/4194304
batch     0,  loss 0.175881, eval 4.910676
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 378  replay_len: 42335/4194304
batch     0,  loss 0.243575, eval 4.995572
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 1  add: 209  replay_len: 42663/4194304
batch     0,  loss 0.257747, eval 5.037050
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 216  replay_len: 42879/4194304
batch     0,  loss 0.114905, eval 5.162587
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 342  replay_len: 43221/4194304
batch     0,  loss 0.322697, eval 5.151583
batches: 2,  total time: 0.75,  1k average: 0.75
batch number: 5  add: 765  replay_len: 43986/4194304
batch     0,  loss 0.230041, eval 5.138414
batches: 5,  total time: 1.43,  1k average: 1.43
batch number: 7  add: 900  replay_len: 44886/4194304
batch     0,  loss 0.207097, eval 5.189781
batches: 7,  total time: 2.17,  1k average: 2.17
batch number: 3  add: 395  replay_len: 45281/4194304
batch     0,  loss 0.128686, eval 5.216418
batches: 3,  total time: 0.88,  1k average: 0.88
batch number: 1  add: 144  replay_len: 45534/4194304
batch     0,  loss 0.143464, eval 5.232767
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 223  replay_len: 45757/4194304
batch     0,  loss 0.236153, eval 5.267946
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 247  replay_len: 46004/4194304
batch     0,  loss 0.266280, eval 5.279912
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 7  add: 1015  replay_len: 47019/4194304
batch     0,  loss 0.269831, eval 5.335875
batches: 7,  total time: 2.04,  1k average: 2.04
batch number: 1  add: 253  replay_len: 47272/4194304
batch     0,  loss 0.203069, eval 5.259610
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 138  replay_len: 47410/4194304
batch     0,  loss 0.305363, eval 5.269679
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 151  replay_len: 47561/4194304
batch     0,  loss 0.305705, eval 5.285180
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 217  replay_len: 47778/4194304
batch     0,  loss 0.232323, eval 5.301745
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 135  replay_len: 48216/4194304
batch     0,  loss 0.189404, eval 5.270605
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 190  replay_len: 48866/4194304
batch     0,  loss 0.240579, eval 5.289058
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 6  add: 775  replay_len: 49641/4194304
batch     0,  loss 0.164817, eval 5.251971
batches: 6,  total time: 1.91,  1k average: 1.91
batch number: 1  add: 140  replay_len: 49781/4194304
batch     0,  loss 0.259969, eval 5.326272
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 177  replay_len: 49958/4194304
batch     0,  loss 0.216296, eval 5.333170
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 2  add: 299  replay_len: 50257/4194304
batch     0,  loss 0.234445, eval 5.271747
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 1  add: 159  replay_len: 50519/4194304
batch     0,  loss 0.181369, eval 5.293151
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 185  replay_len: 50704/4194304
batch     0,  loss 0.268124, eval 5.292334
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 168  replay_len: 50872/4194304
batch     0,  loss 0.260855, eval 5.305236
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 209  replay_len: 51081/4194304
batch     0,  loss 0.335485, eval 5.226185
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 164  replay_len: 51245/4194304
batch     0,  loss 0.194347, eval 5.343073
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 145  replay_len: 51390/4194304
batch     0,  loss 0.291236, eval 5.277157
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 356  replay_len: 51746/4194304
batch     0,  loss 0.313649, eval 5.164280
batches: 2,  total time: 0.53,  1k average: 0.53
batch number: 1  add: 134  replay_len: 51976/4194304
batch     0,  loss 0.184044, eval 5.132576
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 199  replay_len: 52175/4194304
batch     0,  loss 0.265085, eval 5.064659
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 1  add: 177  replay_len: 52352/4194304
batch     0,  loss 0.246839, eval 5.063291
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 146  replay_len: 52498/4194304
batch     0,  loss 0.240011, eval 5.015714
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 128  replay_len: 52626/4194304
batch     0,  loss 0.264392, eval 4.939109
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 143  replay_len: 52769/4194304
batch     0,  loss 0.198530, eval 4.894202
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 135  replay_len: 52904/4194304
batch     0,  loss 0.386635, eval 4.858541
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 133  replay_len: 53150/4194304
batch     0,  loss 0.349866, eval 4.763479
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 241  replay_len: 53622/4194304
batch     0,  loss 0.234851, eval 4.660599
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 186  replay_len: 54308/4194304
batch     0,  loss 0.259379, eval 4.596879
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 306  replay_len: 54691/4194304
batch     0,  loss 0.229115, eval 4.572898
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 2  add: 315  replay_len: 55006/4194304
batch     0,  loss 0.181058, eval 4.490838
batches: 2,  total time: 0.85,  1k average: 0.85
batch number: 3  add: 492  replay_len: 55498/4194304
batch     0,  loss 0.265811, eval 4.321024
batches: 3,  total time: 0.88,  1k average: 0.88
batch number: 1  add: 155  replay_len: 55775/4194304
batch     0,  loss 0.246677, eval 4.217915
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 198  replay_len: 55973/4194304
batch     0,  loss 0.162860, eval 4.194860
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 192  replay_len: 56165/4194304
batch     0,  loss 0.246509, eval 4.076874
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 163  replay_len: 56453/4194304
batch     0,  loss 0.250507, eval 4.185853
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 165  replay_len: 56618/4194304
batch     0,  loss 0.203131, eval 4.153068
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 256  replay_len: 56874/4194304
batch     0,  loss 0.183441, eval 4.153306
batches: 2,  total time: 0.73,  1k average: 0.73
batch number: 1  add: 174  replay_len: 57048/4194304round 135	 loss: [0.6, 0.19]	 reward: [-3.2, -10.0]	 value: [17.54, 4.15]
round 136	 loss: [0.76, 0.19]	 reward: [-5.4, -10.0]	 value: [17.92, 4.06]
round 137	 loss: [0.76, 0.22]	 reward: [-76.4, -8.0]	 value: [18.42, 3.96]
round 138	 loss: [0.87, 0]	 reward: [4.2, -10.0]	 value: [18.36, 0]
round 139	 loss: [0.73, 0.24]	 reward: [-80.0, -10.0]	 value: [18.27, 3.85]
round 140	 loss: [0.64, 0.18]	 reward: [-75.2, -9.0]	 value: [17.93, 3.86]
round 141	 loss: [0.67, 0.19]	 reward: [-14.8, -10.0]	 value: [18.03, 3.85]
round 142	 loss: [0.76, 0.14]	 reward: [-54.2, -10.0]	 value: [18.37, 3.84]
round 143	 loss: [0.75, 0.17]	 reward: [-13.2, -10.0]	 value: [17.96, 3.83]
round 144	 loss: [0.58, 0.18]	 reward: [-63.6, -3.0]	 value: [17.88, 3.83]
round 145	 loss: [0.63, 0.2]	 reward: [-28.6, -10.0]	 value: [17.99, 3.78]
round 146	 loss: [0.55, 0.14]	 reward: [-60.2, -10.0]	 value: [18.06, 3.76]
round 147	 loss: [0.56, 0.18]	 reward: [-75.0, -8.0]	 value: [17.87, 3.73]

step 200,  reward: [-0.2  0. ],  total_reward: [-60.4  -9. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-76.4  -9. ] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 2.66
round time 3.65  total time 365.61

===== sample =====
eps 0.492 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 31,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 365.95

save model... 
===== sample =====
eps 0.488 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -6.6 -10. ] 
steps: 60,  total time: 0.28,  step average 0.00
===== train =====
train_time 0.58
round time 0.86  total time 367.85

===== sample =====
eps 0.484 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -8.2 -10. ] 
steps: 52,  total time: 0.68,  step average 0.01
===== train =====
train_time 0.66
round time 1.34  total time 369.19

save model... 
===== sample =====
eps 0.48 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -6.6 -10. ] 
steps: 66,  total time: 0.49,  step average 0.01
===== train =====
train_time 0.61
round time 1.10  total time 371.64

===== sample =====
eps 0.476 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 38,  total time: 0.19,  step average 0.01
===== train =====
train_time 0.23
round time 0.42  total time 372.06

save model... 
===== sample =====
eps 0.472 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [ -9.6 -10. ] 
steps: 53,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.59
round time 0.81  total time 373.85

===== sample =====
eps 0.46799999999999997 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-12.6 -10. ] 
steps: 78,  total time: 0.53,  step average 0.01
===== train =====
train_time 0.94
round time 1.47  total time 375.32

save model... 
===== sample =====
eps 0.46399999999999997 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-12.4  -9. ] 
steps: 74,  total time: 0.44,  step average 0.01
===== train =====
train_time 0.76
round time 1.21  total time 377.68

===== sample =====
eps 0.45999999999999996 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
steps: 42,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.36
round time 0.52  total time 378.20

save model... 
===== sample =====
eps 0.45599999999999996 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [ -5.2 -10. ] 
steps: 54,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.58
round time 0.80  total time 380.14

===== sample =====
eps 0.45199999999999996 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.6  0. ],  total_reward: [-12.4  -6. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-29.4  -6. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-47.6  -6. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-62.6  -6. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-76.4  -8. ] 
steps: 251,  total time: 0.96,  step average 0.00
===== train =====
train_time 2.59
round time 3.55  total time 383.69

save model... 
===== sample =====
eps 0.44799999999999995 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 28,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.26
round time 0.36  total time 385.24

===== sample =====
eps 0.44399999999999995 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-15.  -6.] 
step 100,  reward: [-0.4  0. ],  total_reward: [-38.  -6.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-56.8 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-71.8 -10. ] 
steps: 223,  total time: 3.81,  step average 0.02
===== train =====
train_time 2.28
round time 6.08  total time 391.33

save model... 
===== sample =====
eps 0.43999999999999995 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-6.2 -9. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-23.2  -9. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-40.  -9.] 
step 200,  reward: [-0.2  0. ],  total_reward: [-57.  -9.] 
step 250,  reward: [-0.6  0. ],  total_reward: [-75.2  -9. ] 
steps: 251,  total time: 1.76,  step average 0.01
===== train =====
train_time 2.74
round time 4.50  total time 397.19

===== sample =====
eps 0.43599999999999994 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -4.8 -10. ] 
steps: 79,  total time: 0.29,  step average 0.00
===== train =====
train_time 0.86
round time 1.15  total time 398.34

save model... 
===== sample =====
eps 0.43199999999999994 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [ 0.4 -1. ],  total_reward: [ -5. -10.] 
step 100,  reward: [-0.8  0. ],  total_reward: [-16.8 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-31. -10.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-46.4 -10. ] 
steps: 232,  total time: 1.04,  step average 0.00
===== train =====
train_time 2.86
round time 3.90  total time 403.36

===== sample =====
eps 0.42799999999999994 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -4.4 -10. ] 
steps: 84,  total time: 0.36,  step average 0.00
===== train =====
train_time 0.88
round time 1.24  total time 404.60

save model... 
===== sample =====
eps 0.42399999999999993 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-12.  -3.] 
step 100,  reward: [-0.2  0. ],  total_reward: [-24.4  -3. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-37.  -3.] 
step 200,  reward: [-0.2  0. ],  total_reward: [-50.  -3.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-63.6  -3. ] 
steps: 251,  total time: 1.48,  step average 0.01
===== train =====
train_time 3.63
round time 5.11  total time 410.88

===== sample =====
eps 0.42000000000000004 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-8.8 -7. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-22.2 -10. ] 
steps: 126,  total time: 0.55,  step average 0.00
===== train =====
train_time 1.45
round time 2.00  total time 412.88

save model... 
===== sample =====
eps 0.41600000000000004 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-6.6 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-18.4 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-33. -10.] 
step 200,  reward: [-0.4  0. ],  total_reward: [-43. -10.] 
step 250,  reward: [-0.2  0. ],  total_reward: [-60.2 -10. ] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.42
round time 3.32  total time 417.24

===== sample =====
eps 0.41200000000000003 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-12.  -4.] 
step 100,  reward: [0. 0.],  total_reward: [-25.2  -6. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-43.4  -6. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-57.8  -8. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-75.  -8.] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 2.82
round time 3.82  total time 421.06

save model... 
===== sample =====
eps 0.40800000000000003 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -5.6 -10. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-30.6 -10. ] round 148	 loss: [0.59, 0.2]	 reward: [-114.8, -10.0]	 value: [17.89, 3.65]
round 149	 loss: [0.57, 0.16]	 reward: [-30.0, -10.0]	 value: [17.86, 3.59]
round 150	 loss: [0.53, 0.18]	 reward: [-67.8, -10.0]	 value: [17.63, 3.57]
round 151	 loss: [0.51, 0.18]	 reward: [-41.2, -10.0]	 value: [17.68, 3.52]
round 152	 loss: [0.5, 0.18]	 reward: [-42.2, -10.0]	 value: [17.62, 3.5]
round 153	 loss: [0.56, 0.16]	 reward: [-62.6, -8.0]	 value: [17.98, 3.39]
round 154	 loss: [0.57, 0.16]	 reward: [-8.6, -10.0]	 value: [17.71, 3.45]
round 155	 loss: [0.69, 0.13]	 reward: [0.6, -10.0]	 value: [18.13, 3.41]
round 156	 loss: [0.53, 0.15]	 reward: [-31.6, -10.0]	 value: [18.38, 3.36]
round 157	 loss: [0.54, 0.15]	 reward: [-79.0, -7.0]	 value: [18.15, 3.21]
round 158	 loss: [0.46, 0.15]	 reward: [-67.0, -8.0]	 value: [18.2, 3.24]
round 159	 loss: [0.49, 0.16]	 reward: [-4.6, -10.0]	 value: [18.04, 3.2]
round 160	 loss: [0.43, 0.14]	 reward: [-40.8, -10.0]	 value: [18.02, 3.24]
round 161	 loss: [0.46, 0.15]	 reward: [-134.2, -9.0]	 value: [18.04, 3.2]
round 162	 loss: [0.46, 0.13]	 reward: [-15.0, -10.0]	 value: [18.08, 3.2]
round 163	 loss: [0.5, 0.15]	 reward: [-31.0, -10.0]	 value: [17.95, 3.19]
round 164	 loss: [0.49, 0]	 reward: [2.6, -10.0]	 value: [17.99, 0]
round 165	 loss: [0.41, 0.15]	 reward: [-44.4, -10.0]	 value: [17.74, 3.17]
round 166	 loss: [0.41, 0.13]	 reward: [-48.4, -10.0]	 value: [17.79, 3.16]
round 167	 loss: [0.41, 0.14]	 reward: [-89.0, -9.0]	 value: [17.68, 3.14]

step 150,  reward: [-0.6  0. ],  total_reward: [-59.8 -10. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-88.8 -10. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-114.8  -10. ] 
steps: 251,  total time: 1.04,  step average 0.00
===== train =====
train_time 2.62
round time 3.65  total time 425.97

===== sample =====
eps 0.404 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -3. -10.] 
step 100,  reward: [-1.  0.],  total_reward: [-19.8 -10. ] 
steps: 130,  total time: 2.43,  step average 0.02
===== train =====
train_time 1.78
round time 4.21  total time 430.18

save model... 
===== sample =====
eps 0.4 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -3. -10.] 
step 100,  reward: [0. 0.],  total_reward: [-19.4 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-35.4 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-52.8 -10. ] 
steps: 249,  total time: 1.02,  step average 0.00
===== train =====
train_time 2.66
round time 3.67  total time 435.34

===== sample =====
eps 0.396 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -3. -10.] 
step 100,  reward: [-0.4  0. ],  total_reward: [-18.4 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-33.2 -10. ] 
steps: 180,  total time: 0.93,  step average 0.01
===== train =====
train_time 2.12
round time 3.05  total time 438.38

save model... 
===== sample =====
eps 0.392 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-5. -7.] 
step 100,  reward: [0. 0.],  total_reward: [-20.  -7.] 
step 150,  reward: [-0.2  0. ],  total_reward: [-32.4  -8. ] 
steps: 192,  total time: 0.73,  step average 0.00
===== train =====
train_time 1.93
round time 2.66  total time 442.29

===== sample =====
eps 0.388 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-5.2 -8. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-19.2  -8. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-32.8  -8. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-46.8  -8. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-62.6  -8. ] 
steps: 251,  total time: 0.93,  step average 0.00
===== train =====
train_time 2.29
round time 3.22  total time 445.51

save model... 
===== sample =====
eps 0.384 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -6.4 -10. ] 
steps: 65,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.57
round time 0.81  total time 447.45

===== sample =====
eps 0.38 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 43,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.32
round time 0.48  total time 447.93

save model... 
===== sample =====
eps 0.376 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [-5. -5.] 
step 100,  reward: [-0.2  0. ],  total_reward: [-12.4  -8. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-26.6  -9. ] 
steps: 173,  total time: 0.63,  step average 0.00
===== train =====
train_time 1.68
round time 2.31  total time 451.34

===== sample =====
eps 0.372 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-8.4 -7. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-26.2  -7. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-45.4  -7. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-62.  -7.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-79.  -7.] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.51
round time 3.41  total time 454.75

save model... 
===== sample =====
eps 0.368 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-5.8 -7. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-19.4  -7. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-36.  -7.] 
step 200,  reward: [-0.4  0. ],  total_reward: [-51.2  -7. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-67.  -8.] 
steps: 251,  total time: 1.39,  step average 0.01
===== train =====
train_time 3.33
round time 4.72  total time 460.81

===== sample =====
eps 0.364 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 43,  total time: 0.64,  step average 0.01
===== train =====
train_time 0.35
round time 0.99  total time 461.80

save model... 
===== sample =====
eps 0.36 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-8.6 -9. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-34.8 -10. ] 
steps: 115,  total time: 0.46,  step average 0.00
===== train =====
train_time 1.02
round time 1.48  total time 464.96

===== sample =====
eps 0.356 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-14.4  -9. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-43.6  -9. ] 
step 150,  reward: [-0.8  0. ],  total_reward: [-76.6  -9. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-105.6   -9. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-134.2   -9. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.55
round time 3.45  total time 468.41

save model... 
===== sample =====
eps 0.352 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-4.8 -7. ] 
steps: 95,  total time: 0.37,  step average 0.00
===== train =====
train_time 1.07
round time 1.45  total time 471.04

===== sample =====
eps 0.348 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-7.8 -7. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-20.2  -9. ] 
steps: 139,  total time: 0.67,  step average 0.00
===== train =====
train_time 1.82
round time 2.49  total time 473.53

save model... 
===== sample =====
eps 0.344 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 34,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.28
round time 0.47  total time 475.38

===== sample =====
eps 0.33999999999999997 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -7.4 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-20.8 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-35. -10.] 
steps: 188,  total time: 0.90,  step average 0.00
===== train =====
train_time 2.19
round time 3.09  total time 478.47

save model... 
===== sample =====
eps 0.33599999999999997 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-2.6 -9. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-13.6 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-24.2 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-37.2 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-48.4 -10. ] 
steps: 251,  total time: 1.01,  step average 0.00
===== train =====
train_time 2.48
round time 3.49  total time 483.34

===== sample =====
eps 0.33199999999999996 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-13.6  -6. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-33.8  -6. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-50.2  -7. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-65.4  -9. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-89.  -9.] 
steps: 251,  total time: 1.02,  step average 0.00
===== train =====
train_time 2.85
round time 3.87  total time 487.21

save model... 
===== sample =====
eps 0.32799999999999996 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 32,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.26round 168	 loss: [0.41, 0]	 reward: [4.0, -10.0]	 value: [17.82, 0]

batch number: 1  add: 245  replay_len: 89980/4194304
batch     0,  loss 0.218845, eval 8.620234
batches: 1,  total time: 0.49,  1k average: 0.49
batch number: 1  add: 135  replay_len: 90115/4194304
batch     0,  loss 0.225144, eval 8.968746
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 2  add: 345  replay_len: 90460/4194304
batch     0,  loss 0.322137, eval 9.456816
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 1  add: 205  replay_len: 90665/4194304
batch     0,  loss 0.588147, eval 9.985519
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 155  replay_len: 90820/4194304
batch     0,  loss 0.337543, eval 10.875296
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 185  replay_len: 91005/4194304
batch     0,  loss 0.420490, eval 11.636600
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 165  replay_len: 91170/4194304
batch     0,  loss 0.414792, eval 11.871511
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 150  replay_len: 91320/4194304
batch     0,  loss 0.450862, eval 12.428288
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 240  replay_len: 91560/4194304
batch     0,  loss 0.548929, eval 12.984694
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 180  replay_len: 91740/4194304
batch     0,  loss 0.477856, eval 13.630505
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 5  add: 640  replay_len: 92380/4194304
batch     0,  loss 0.524047, eval 14.371266
batches: 5,  total time: 1.42,  1k average: 1.42
batch number: 3  add: 430  replay_len: 92810/4194304
batch     0,  loss 3.895202, eval 15.129670
batches: 3,  total time: 1.18,  1k average: 1.18
batch number: 9  add: 1255  replay_len: 94065/4194304
batch     0,  loss 0.826616, eval 17.346865
batches: 9,  total time: 2.64,  1k average: 2.64
batch number: 1  add: 155  replay_len: 94220/4194304
batch     0,  loss 0.659507, eval 16.997120
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 300  replay_len: 94520/4194304
batch     0,  loss 0.846089, eval 17.112200
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 2  add: 260  replay_len: 94780/4194304
batch     0,  loss 0.711637, eval 17.122936
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 2  add: 330  replay_len: 95110/4194304
batch     0,  loss 0.724222, eval 17.577543
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 1  add: 190  replay_len: 95300/4194304
batch     0,  loss 0.649276, eval 17.265137
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 265  replay_len: 95565/4194304
batch     0,  loss 0.686176, eval 17.208965
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 3  add: 390  replay_len: 95955/4194304
batch     0,  loss 0.717447, eval 17.302032
batches: 3,  total time: 0.93,  1k average: 0.93
batch number: 2  add: 370  replay_len: 96325/4194304
batch     0,  loss 0.661156, eval 17.516747
batches: 2,  total time: 0.76,  1k average: 0.76
batch number: 1  add: 210  replay_len: 96535/4194304
batch     0,  loss 0.598294, eval 17.537453
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 270  replay_len: 96805/4194304
batch     0,  loss 0.814533, eval 17.807114
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 9  add: 1255  replay_len: 98060/4194304
batch     0,  loss 0.588734, eval 17.709503
batches: 9,  total time: 2.56,  1k average: 2.56
batch number: 1  add: 140  replay_len: 98200/4194304
batch     0,  loss 0.866239, eval 18.356585
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 8  add: 1115  replay_len: 99315/4194304
batch     0,  loss 0.754337, eval 18.184067
batches: 8,  total time: 2.26,  1k average: 2.26
batch number: 9  add: 1255  replay_len: 100570/4194304
batch     0,  loss 0.767783, eval 18.290379
batches: 9,  total time: 2.72,  1k average: 2.72
batch number: 3  add: 395  replay_len: 100965/4194304
batch     0,  loss 0.719782, eval 17.919445
batches: 3,  total time: 0.85,  1k average: 0.85
batch number: 9  add: 1160  replay_len: 102125/4194304
batch     0,  loss 0.693283, eval 18.173592
batches: 9,  total time: 2.84,  1k average: 2.84
batch number: 3  add: 420  replay_len: 102545/4194304
batch     0,  loss 0.917614, eval 18.250599
batches: 3,  total time: 0.87,  1k average: 0.87
batch number: 9  add: 1255  replay_len: 103800/4194304
batch     0,  loss 0.549383, eval 17.877338
batches: 9,  total time: 3.61,  1k average: 3.61
batch number: 4  add: 630  replay_len: 104430/4194304
batch     0,  loss 0.625502, eval 17.780273
batches: 4,  total time: 1.44,  1k average: 1.44
batch number: 9  add: 1255  replay_len: 105685/4194304
batch     0,  loss 0.519806, eval 18.036232
batches: 9,  total time: 2.40,  1k average: 2.40
batch number: 9  add: 1255  replay_len: 106940/4194304
batch     0,  loss 0.637583, eval 18.083736
batches: 9,  total time: 2.80,  1k average: 2.80
batch number: 9  add: 1255  replay_len: 108195/4194304
batch     0,  loss 0.589331, eval 17.658087
batches: 9,  total time: 2.60,  1k average: 2.60
batch number: 5  add: 650  replay_len: 108845/4194304
batch     0,  loss 0.596738, eval 18.067772
batches: 5,  total time: 1.77,  1k average: 1.77
batch number: 9  add: 1245  replay_len: 110090/4194304
batch     0,  loss 0.486120, eval 17.793095
batches: 9,  total time: 2.64,  1k average: 2.64
batch number: 7  add: 900  replay_len: 110990/4194304
batch     0,  loss 0.536813, eval 17.686995
batches: 7,  total time: 2.11,  1k average: 2.11
batch number: 7  add: 960  replay_len: 111950/4194304
batch     0,  loss 0.522713, eval 17.723343
batches: 7,  total time: 1.92,  1k average: 1.92
batch number: 9  add: 1255  replay_len: 113205/4194304
batch     0,  loss 0.517226, eval 17.646307
batches: 9,  total time: 2.28,  1k average: 2.28
batch number: 2  add: 325  replay_len: 113530/4194304
batch     0,  loss 0.542329, eval 17.814125
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 215  replay_len: 113745/4194304
batch     0,  loss 0.685387, eval 18.131117
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 6  add: 865  replay_len: 114610/4194304
batch     0,  loss 0.426031, eval 18.223614
batches: 6,  total time: 1.67,  1k average: 1.67
batch number: 9  add: 1255  replay_len: 115865/4194304
batch     0,  loss 0.651184, eval 18.525539
batches: 9,  total time: 2.49,  1k average: 2.49
batch number: 9  add: 1255  replay_len: 117120/4194304
batch     0,  loss 0.510623, eval 18.052650
batches: 9,  total time: 3.31,  1k average: 3.31
batch number: 1  add: 215  replay_len: 117335/4194304
batch     0,  loss 0.485406, eval 18.036869
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 4  add: 575  replay_len: 117910/4194304
batch     0,  loss 0.338748, eval 18.026686
batches: 4,  total time: 1.02,  1k average: 1.02
batch number: 9  add: 1255  replay_len: 119165/4194304
batch     0,  loss 0.446346, eval 18.323780
batches: 9,  total time: 2.52,  1k average: 2.52
batch number: 3  add: 475  replay_len: 119640/4194304
batch     0,  loss 0.503423, eval 17.971191
batches: 3,  total time: 1.07,  1k average: 1.07
batch number: 5  add: 695  replay_len: 120335/4194304
batch     0,  loss 0.488393, eval 18.114231
batches: 5,  total time: 1.82,  1k average: 1.82
batch number: 1  add: 170  replay_len: 120505/4194304
batch     0,  loss 0.492475, eval 17.994759
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 7  add: 940  replay_len: 121445/4194304
batch     0,  loss 0.315362, eval 17.862797
batches: 7,  total time: 2.18,  1k average: 2.18
batch number: 9  add: 1255  replay_len: 122700/4194304
batch     0,  loss 0.382639, eval 17.684822
batches: 9,  total time: 2.47,  1k average: 2.47
batch number: 9  add: 1255  replay_len: 123955/4194304
batch     0,  loss 0.478059, eval 17.870319
batches: 9,  total time: 2.84,  1k average: 2.84
batch number: 1  add: 160  replay_len: 124115/4194304
batch     0,  loss 0.408573, eval 17.823692
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 3  add: 440  replay_len: 124555/4194304
batch     0,  loss 0.305083, eval 17.750580
batches: 3,  total time: 0.86,  1k average: 0.86round 169	 loss: [0.36, 0.16]	 reward: [-12.0, -10.0]	 value: [17.74, 3.13]
round 170	 loss: [0.35, 0.17]	 reward: [-5.0, -10.0]	 value: [17.73, 3.01]
round 171	 loss: [0.44, 0.13]	 reward: [-82.6, -8.0]	 value: [17.62, 3.13]
round 172	 loss: [0.39, 0.14]	 reward: [-72.0, -9.0]	 value: [17.64, 3.04]
round 173	 loss: [0.41, 0.12]	 reward: [-12.8, -10.0]	 value: [17.84, 3.07]
round 174	 loss: [0.33, 0.18]	 reward: [-46.8, -10.0]	 value: [17.77, 2.92]
round 175	 loss: [0.36, 0.12]	 reward: [-70.0, -10.0]	 value: [17.74, 2.95]
round 176	 loss: [0.35, 0.12]	 reward: [-3.6, -10.0]	 value: [17.74, 2.97]
round 177	 loss: [0.34, 0.11]	 reward: [-45.6, -8.0]	 value: [17.56, 3.01]
round 178	 loss: [0.32, 0.13]	 reward: [-176.6, -4.0]	 value: [17.27, 2.92]
round 179	 loss: [0.3, 0.14]	 reward: [-44.6, -10.0]	 value: [17.39, 2.85]
round 180	 loss: [0.3, 0.13]	 reward: [-41.6, -8.0]	 value: [17.47, 2.9]
round 181	 loss: [0.28, 0.12]	 reward: [-22.4, -10.0]	 value: [17.48, 2.98]
round 182	 loss: [0.29, 0.13]	 reward: [-44.8, -9.0]	 value: [17.53, 2.98]
round 183	 loss: [0.25, 0.14]	 reward: [-48.0, -10.0]	 value: [17.36, 2.9]
round 184	 loss: [0.29, 0.14]	 reward: [-39.4, -8.0]	 value: [17.39, 2.91]
round 185	 loss: [0.24, 0.13]	 reward: [-143.6, -10.0]	 value: [17.59, 2.83]

round time 0.39  total time 489.00

===== sample =====
eps 0.32399999999999995 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-5.2 -9. ] 
steps: 88,  total time: 1.78,  step average 0.02
===== train =====
train_time 0.87
round time 2.65  total time 491.64

save model... 
===== sample =====
eps 0.31999999999999995 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-1. -9.] 
steps: 77,  total time: 0.28,  step average 0.00
===== train =====
train_time 0.89
round time 1.17  total time 494.07

===== sample =====
eps 0.31599999999999995 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [-7.6 -8. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-26.8  -8. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-45.8  -8. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-65.2  -8. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-82.6  -8. ] 
steps: 251,  total time: 1.28,  step average 0.01
===== train =====
train_time 2.27
round time 3.55  total time 497.62

save model... 
===== sample =====
eps 0.31199999999999994 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-6.4 -7. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-22.6  -8. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-40.2  -9. ] 
step 200,  reward: [0. 0.],  total_reward: [-58.8  -9. ] 
step 250,  reward: [0. 0.],  total_reward: [-72.  -9.] 
steps: 251,  total time: 1.20,  step average 0.00
===== train =====
train_time 2.90
round time 4.11  total time 502.91

===== sample =====
eps 0.30799999999999994 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-3. -8.] 
step 100,  reward: [0. 0.],  total_reward: [-12.4 -10. ] 
steps: 113,  total time: 0.42,  step average 0.00
===== train =====
train_time 1.00
round time 1.42  total time 504.33

save model... 
===== sample =====
eps 0.30399999999999994 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-3.6 -8. ] 
step 100,  reward: [0. 0.],  total_reward: [-15.4  -8. ] 
step 150,  reward: [0. 0.],  total_reward: [-26.6  -8. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-35.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-46.8 -10. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.60
round time 3.49  total time 509.10

===== sample =====
eps 0.29999999999999993 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ -3.4 -10. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-18.4 -10. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-35. -10.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-50.2 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-70. -10.] 
steps: 251,  total time: 0.87,  step average 0.00
===== train =====
train_time 2.34
round time 3.21  total time 512.31

save model... 
===== sample =====
eps 0.29600000000000004 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  0. -10.] 
steps: 75,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.60
round time 0.87  total time 514.61

===== sample =====
eps 0.29200000000000004 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-3.6 -8. ] 
step 100,  reward: [0. 0.],  total_reward: [-11.8  -8. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-21.  -8.] 
step 200,  reward: [-0.4  0. ],  total_reward: [-33.2  -8. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-45.6  -8. ] 
steps: 251,  total time: 1.47,  step average 0.01
===== train =====
train_time 2.59
round time 4.06  total time 518.67

save model... 
===== sample =====
eps 0.28800000000000003 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-30.6  -2. ] 
step 100,  reward: [-0.8  0. ],  total_reward: [-65.2  -4. ] 
step 150,  reward: [-1.  0.],  total_reward: [-102.   -4.] 
step 200,  reward: [-0.8  0. ],  total_reward: [-137.   -4.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-176.6   -4. ] 
steps: 251,  total time: 0.93,  step average 0.00
===== train =====
train_time 3.25
round time 4.18  total time 524.08

===== sample =====
eps 0.28400000000000003 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [ 0.4 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-10. -10.] 
step 150,  reward: [0. 0.],  total_reward: [-21. -10.] 
step 200,  reward: [-0.4  0. ],  total_reward: [-32.8 -10. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-44.6 -10. ] 
steps: 251,  total time: 4.82,  step average 0.02
===== train =====
train_time 2.91
round time 7.73  total time 531.81

save model... 
===== sample =====
eps 0.28 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [-5.4 -7. ] 
step 100,  reward: [0. 0.],  total_reward: [-15.6  -7. ] 
step 150,  reward: [0. 0.],  total_reward: [-23.  -7.] 
step 200,  reward: [0. 0.],  total_reward: [-31.4  -7. ] 
step 250,  reward: [0. 0.],  total_reward: [-41.6  -8. ] 
steps: 251,  total time: 0.96,  step average 0.00
===== train =====
train_time 3.12
round time 4.08  total time 537.14

===== sample =====
eps 0.276 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-0.6 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -7.8 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-21. -10.] 
steps: 165,  total time: 0.78,  step average 0.00
===== train =====
train_time 1.73
round time 2.51  total time 539.66

save model... 
===== sample =====
eps 0.272 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-3.4 -7. ] 
step 100,  reward: [0. 0.],  total_reward: [-14.6  -7. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-22.2  -9. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-35.  -9.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-44.8  -9. ] 
steps: 251,  total time: 0.91,  step average 0.00
===== train =====
train_time 3.08
round time 3.98  total time 544.86

===== sample =====
eps 0.268 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-4. -9.] 
step 100,  reward: [0. 0.],  total_reward: [-17.4  -9. ] 
step 150,  reward: [0. 0.],  total_reward: [-25.2 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-36. -10.] 
step 250,  reward: [0. 0.],  total_reward: [-48. -10.] 
steps: 251,  total time: 1.07,  step average 0.00
===== train =====
train_time 2.64
round time 3.71  total time 548.57

save model... 
===== sample =====
eps 0.264 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 0.2 -8. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-7.4 -8. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-18.8  -8. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-28.8  -8. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-39.4  -8. ] 
steps: 251,  total time: 0.91,  step average 0.00
===== train =====
train_time 2.72
round time 3.64  total time 553.46

===== sample =====
eps 0.26 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [-13.2 -10. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [-47.8 -10. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-77.8 -10. ] 
step 200,  reward: [-0.8  0. ],  total_reward: [-112.6  -10. ] 
step 250,  reward: [-0.6  0. ],  total_reward: [-143.6  -10. ] 
steps: 251,  total time: 0.88,  step average 0.00
===== train =====
train_time 2.31
round time 3.19  total time 556.65

save model... 
===== sample =====
eps 0.256 number [5, 5]round 186	 loss: [0.27, 0.13]	 reward: [-35.0, -10.0]	 value: [17.41, 2.83]
round 187	 loss: [0.25, 0.13]	 reward: [-51.8, -10.0]	 value: [17.43, 2.85]
round 188	 loss: [0.26, 0.12]	 reward: [-78.0, -10.0]	 value: [17.62, 2.91]
round 189	 loss: [0.22, 0.12]	 reward: [-54.6, -10.0]	 value: [17.5, 2.88]
round 190	 loss: [0.23, 0.12]	 reward: [-37.8, -9.0]	 value: [17.34, 2.87]
round 191	 loss: [0.24, 0.1]	 reward: [-28.2, -10.0]	 value: [17.29, 2.96]
round 192	 loss: [0.23, 0.15]	 reward: [-103.8, -10.0]	 value: [17.04, 2.86]
round 193	 loss: [0.21, 0.13]	 reward: [-38.4, -9.0]	 value: [16.99, 2.94]

batch     0,  loss 0.192468, eval 4.152514
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 216  replay_len: 57264/4194304
batch     0,  loss 0.187052, eval 4.060672
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 4  add: 542  replay_len: 57806/4194304
batch     0,  loss 0.219025, eval 4.047198
batches: 4,  total time: 1.25,  1k average: 1.25
batch number: 3  add: 411  replay_len: 58315/4194304
batch     0,  loss 0.207933, eval 3.947241
batches: 3,  total time: 0.97,  1k average: 0.97
batch number: 4  add: 596  replay_len: 58911/4194304
batch     0,  loss 0.198689, eval 3.923218
batches: 4,  total time: 1.32,  1k average: 1.32
batch number: 1  add: 185  replay_len: 59096/4194304
batch     0,  loss 0.185101, eval 3.846020
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 3  add: 429  replay_len: 59525/4194304
batch     0,  loss 0.123804, eval 3.844762
batches: 3,  total time: 1.10,  1k average: 1.10
batch number: 1  add: 190  replay_len: 59715/4194304
batch     0,  loss 0.167265, eval 3.834326
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 8  add: 1029  replay_len: 60744/4194304
batch     0,  loss 0.225491, eval 3.848602
batches: 8,  total time: 3.00,  1k average: 3.00
batch number: 2  add: 295  replay_len: 61039/4194304
batch     0,  loss 0.220552, eval 3.801430
batches: 2,  total time: 0.77,  1k average: 0.77
batch number: 3  add: 488  replay_len: 61527/4194304
batch     0,  loss 0.110354, eval 3.866800
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 4  add: 592  replay_len: 62119/4194304
batch     0,  loss 0.147630, eval 3.731963
batches: 4,  total time: 1.27,  1k average: 1.27
batch number: 5  add: 642  replay_len: 62761/4194304
batch     0,  loss 0.209725, eval 3.692555
batches: 5,  total time: 1.58,  1k average: 1.58
batch number: 2  add: 341  replay_len: 63102/4194304
batch     0,  loss 0.098099, eval 3.733995
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 3  add: 426  replay_len: 63528/4194304
batch     0,  loss 0.166584, eval 3.599861
batches: 3,  total time: 0.98,  1k average: 0.98
batch number: 1  add: 248  replay_len: 63776/4194304
batch     0,  loss 0.184914, eval 3.520601
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 2  add: 369  replay_len: 64145/4194304
batch     0,  loss 0.175005, eval 3.547268
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 2  add: 345  replay_len: 64490/4194304
batch     0,  loss 0.149510, eval 3.545084
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 205  replay_len: 64695/4194304
batch     0,  loss 0.156818, eval 3.445940
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 133  replay_len: 64828/4194304
batch     0,  loss 0.125955, eval 3.410511
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 3  add: 386  replay_len: 65214/4194304
batch     0,  loss 0.117242, eval 3.393602
batches: 3,  total time: 0.91,  1k average: 0.91
batch number: 4  add: 549  replay_len: 65763/4194304
batch     0,  loss 0.137901, eval 3.383591
batches: 4,  total time: 1.21,  1k average: 1.21
batch number: 7  add: 1019  replay_len: 66782/4194304
batch     0,  loss 0.139803, eval 3.245715
batches: 7,  total time: 2.48,  1k average: 2.48
batch number: 1  add: 130  replay_len: 66912/4194304
batch     0,  loss 0.156409, eval 3.199504
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 217  replay_len: 67129/4194304
batch     0,  loss 0.144878, eval 3.243686
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 316  replay_len: 67445/4194304
batch     0,  loss 0.142445, eval 3.247302
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 2  add: 269  replay_len: 67714/4194304
batch     0,  loss 0.154242, eval 3.190590
batches: 2,  total time: 0.71,  1k average: 0.71
batch number: 3  add: 432  replay_len: 68146/4194304
batch     0,  loss 0.165213, eval 3.180511
batches: 3,  total time: 1.12,  1k average: 1.12
batch number: 2  add: 305  replay_len: 68540/4194304
batch     0,  loss 0.133490, eval 3.174808
batches: 2,  total time: 0.76,  1k average: 0.76
batch number: 3  add: 504  replay_len: 69044/4194304
batch     0,  loss 0.106681, eval 3.206539
batches: 3,  total time: 0.92,  1k average: 0.92
batch number: 6  add: 882  replay_len: 69926/4194304
batch     0,  loss 0.147191, eval 3.120902
batches: 6,  total time: 1.79,  1k average: 1.79
batch number: 1  add: 219  replay_len: 70231/4194304
batch     0,  loss 0.163745, eval 3.127573
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 171  replay_len: 70402/4194304
batch     0,  loss 0.169290, eval 3.005397
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 329  replay_len: 70731/4194304
batch     0,  loss 0.130778, eval 3.123968
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 5  add: 684  replay_len: 71415/4194304
batch     0,  loss 0.112917, eval 3.068603
batches: 5,  total time: 1.74,  1k average: 1.74
batch number: 1  add: 232  replay_len: 71647/4194304
batch     0,  loss 0.116852, eval 3.071673
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 3  add: 500  replay_len: 72147/4194304
batch     0,  loss 0.201171, eval 2.970570
batches: 3,  total time: 1.02,  1k average: 1.02
batch number: 2  add: 322  replay_len: 72469/4194304
batch     0,  loss 0.105196, eval 3.060860
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 1  add: 163  replay_len: 72632/4194304
batch     0,  loss 0.115622, eval 2.973855
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 354  replay_len: 72986/4194304
batch     0,  loss 0.089029, eval 3.017200
batches: 2,  total time: 0.70,  1k average: 0.70
batch number: 7  add: 1017  replay_len: 74003/4194304
batch     0,  loss 0.135447, eval 2.917261
batches: 7,  total time: 2.54,  1k average: 2.54
batch number: 4  add: 570  replay_len: 74573/4194304
batch     0,  loss 0.147402, eval 2.974883
batches: 4,  total time: 1.24,  1k average: 1.24
batch number: 8  add: 1025  replay_len: 75598/4194304
batch     0,  loss 0.143319, eval 2.940959
batches: 8,  total time: 2.37,  1k average: 2.37
batch number: 3  add: 398  replay_len: 75996/4194304
batch     0,  loss 0.098884, eval 2.903534
batches: 3,  total time: 0.85,  1k average: 0.85
batch number: 7  add: 929  replay_len: 76925/4194304
batch     0,  loss 0.125174, eval 2.893294
batches: 7,  total time: 2.35,  1k average: 2.35
batch number: 5  add: 705  replay_len: 77630/4194304
batch     0,  loss 0.147519, eval 2.893355
batches: 5,  total time: 1.59,  1k average: 1.59
batch number: 4  add: 639  replay_len: 78269/4194304
batch     0,  loss 0.147684, eval 2.919250
batches: 4,  total time: 1.43,  1k average: 1.43
batch number: 3  add: 395  replay_len: 78664/4194304
batch     0,  loss 0.128513, eval 3.010713
batches: 3,  total time: 0.89,  1k average: 0.89
batch number: 4  add: 621  replay_len: 79285/4194304
batch     0,  loss 0.158791, eval 2.863328
batches: 4,  total time: 1.35,  1k average: 1.35
batch number: 4  add: 612  replay_len: 79897/4194304
batch     0,  loss 0.122498, eval 2.922743
batches: 4,  total time: 1.20,  1k average: 1.20
batch number: 5  add: 667  replay_len: 80564/4194304
batch     0,  loss 0.128267, eval 2.903548
batches: 5,  total time: 1.76,  1k average: 1.76
batch number: 3  add: 508  replay_len: 81072/4194304
batch     0,  loss 0.139506, eval 2.877460
batches: 3,  total time: 0.96,  1k average: 0.96
batch number: 8  add: 1024  replay_len: 82096/4194304
batch     0,  loss 0.120727, eval 2.850409
batches: 8,  total time: 2.45,  1k average: 2.45
batch number: 2  add: 358  replay_len: 82454/4194304
batch     0,  loss 0.107291, eval 2.922123
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 4  add: 617  replay_len: 83071/4194304
batch     0,  loss 0.140438, eval 2.852972
batches: 4,  total time: 1.21,  1k average: 1.21
batch number: 2  add: 376  replay_len: 83447/4194304
batch     0,  loss 0.115341, eval 2.946848
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 7  add: 976  replay_len: 84423/4194304
batch     0,  loss 0.105638, eval 2.902764
batches: 7,  total time: 2.61,  1k average: 2.61round 194	 loss: [0.21, 0.11]	 reward: [-31.4, -8.0]	 value: [16.88, 2.94]
round 195	 loss: [0.2, 0.11]	 reward: [-88.8, -10.0]	 value: [16.81, 2.9]
round 196	 loss: [0.23, 0.11]	 reward: [-38.8, -10.0]	 value: [16.71, 2.83]
round 197	 loss: [0.19, 0.11]	 reward: [-35.8, -8.0]	 value: [16.83, 2.92]
round 198	 loss: [0.18, 0.1]	 reward: [-47.8, -10.0]	 value: [16.87, 2.85]
round 199	 loss: [0.2, 0.11]	 reward: [-71.0, -10.0]	 value: [16.68, 2.85]
round 200	 loss: [0.17, 0.11]	 reward: [-11.6, -10.0]	 value: [16.48, 2.82]
round 201	 loss: [0.17, 0.1]	 reward: [-30.2, -9.0]	 value: [16.47, 2.82]

step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [ -0.8 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -8.6 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-15.4 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-25.8 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-35. -10.] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 3.03
round time 3.92  total time 561.81

===== sample =====
eps 0.252 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-3. -8.] 
step 100,  reward: [-0.2  0. ],  total_reward: [-10.6 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-23.2 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-38.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-51.8 -10. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.46
round time 3.35  total time 565.16

save model... 
===== sample =====
eps 0.248 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-14.6  -7. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-38. -10.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-56. -10.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-74.6 -10. ] 
steps: 214,  total time: 0.77,  step average 0.00
===== train =====
train_time 2.59
round time 3.36  total time 569.79

===== sample =====
eps 0.244 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [-6.2 -7. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-19.2  -7. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-33.2  -7. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-43. -10.] 
step 250,  reward: [0. 0.],  total_reward: [-54.6 -10. ] 
steps: 251,  total time: 4.03,  step average 0.02
===== train =====
train_time 2.74
round time 6.76  total time 576.55

save model... 
===== sample =====
eps 0.24 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-1.4 -8. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-10.2  -8. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-19.2  -9. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-27.6  -9. ] 
step 250,  reward: [0. 0.],  total_reward: [-37.8  -9. ] 
steps: 251,  total time: 0.90,  step average 0.00
===== train =====
train_time 3.09
round time 3.99  total time 581.82

===== sample =====
eps 0.236 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.8  0. ],  total_reward: [-11.2  -9. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-26.4 -10. ] 
steps: 112,  total time: 0.60,  step average 0.01
===== train =====
train_time 1.13
round time 1.73  total time 583.56

save model... 
===== sample =====
eps 0.23199999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-12.6  -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-32.6 -10. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-52.8 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-77.4 -10. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-103.8  -10. ] 
steps: 251,  total time: 0.94,  step average 0.00
===== train =====
train_time 2.79
round time 3.73  total time 588.56

===== sample =====
eps 0.22799999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 1.6 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-8.2 -9. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-17.2  -9. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-28.2  -9. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-38.4  -9. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.31
round time 3.20  total time 591.77

save model... 
===== sample =====
eps 0.22399999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-1. -7.] 
step 100,  reward: [0. 0.],  total_reward: [-8. -7.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-16.8  -7. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-26.  -7.] 
step 250,  reward: [0. 0.],  total_reward: [-31.4  -8. ] 
steps: 251,  total time: 1.02,  step average 0.00
===== train =====
train_time 3.28
round time 4.29  total time 597.42

===== sample =====
eps 0.21999999999999997 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -9.8 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-29. -10.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-49.2 -10. ] 
step 200,  reward: [-0.6  0. ],  total_reward: [-65.6 -10. ] 
step 250,  reward: [-0.8  0. ],  total_reward: [-88.8 -10. ] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 2.54
round time 3.53  total time 600.95

save model... 
===== sample =====
eps 0.21599999999999997 number [5, 5]
step   0,  reward: [-0.6  0. ],  total_reward: [-0.6  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [-1.8 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-11.4 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-19.8 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-29. -10.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-38.8 -10. ] 
steps: 251,  total time: 0.96,  step average 0.00
===== train =====
train_time 3.37
round time 4.32  total time 606.71

===== sample =====
eps 0.21199999999999997 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-5. -5.] 
step 100,  reward: [0. 0.],  total_reward: [-12.8  -5. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-19.4  -7. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-26.8  -8. ] 
step 250,  reward: [-0.4  0. ],  total_reward: [-35.8  -8. ] 
steps: 251,  total time: 1.05,  step average 0.00
===== train =====
train_time 2.73
round time 3.77  total time 610.48

save model... 
===== sample =====
eps 0.20799999999999996 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [  1.4 -10. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-14. -10.] 
step 150,  reward: [-0.2  0. ],  total_reward: [-25.8 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-41.8 -10. ] 
steps: 226,  total time: 0.81,  step average 0.00
===== train =====
train_time 2.71
round time 3.51  total time 615.31

===== sample =====
eps 0.20399999999999996 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-6.8 -9. ] 
step 100,  reward: [-1.  0.],  total_reward: [-28.8  -9. ] 
step 150,  reward: [0. 0.],  total_reward: [-41. -10.] 
step 200,  reward: [-0.6  0. ],  total_reward: [-55.4 -10. ] 
steps: 248,  total time: 4.89,  step average 0.02
===== train =====
train_time 2.80
round time 7.68  total time 623.00

save model... 
===== sample =====
eps 0.2 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 1.4 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-4.6 -9. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-9.4 -9. ] 
steps: 195,  total time: 0.71,  step average 0.00
===== train =====
train_time 1.91
round time 2.62  total time 627.01

===== sample =====
eps 0.19925 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 1. -9.] 
step 100,  reward: [0. 0.],  total_reward: [-4. -9.] 
step 150,  reward: [0. 0.],  total_reward: [-12.8  -9. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-20.6  -9. ] 
step 250,  reward: [0. 0.],  total_reward: [-30.2  -9. ] 
steps: 251,  total time: 1.27,  step average 0.01
===== train =====
train_time 2.72
round time 3.99  total time 631.00

save model... 
===== sample =====
eps 0.1985 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -0.2 -10. ] round 202	 loss: [0.16, 0.1]	 reward: [-1.6, -10.0]	 value: [16.47, 2.86]
round 203	 loss: [0.17, 0.11]	 reward: [-23.2, -10.0]	 value: [16.32, 2.86]
round 204	 loss: [0.16, 0.14]	 reward: [-29.4, -9.0]	 value: [16.25, 2.81]
round 205	 loss: [0, 0]	 reward: [8.2, -10.0]	 value: [0, 0]
round 206	 loss: [0.16, 0]	 reward: [4.4, -10.0]	 value: [16.26, 0]
round 207	 loss: [0.14, 0.1]	 reward: [1.2, -10.0]	 value: [16.17, 2.86]
round 208	 loss: [0.14, 0.09]	 reward: [-5.8, -10.0]	 value: [15.9, 2.87]
round 209	 loss: [0.14, 0.11]	 reward: [-3.0, -10.0]	 value: [15.73, 2.86]
round 210	 loss: [0.16, 0.11]	 reward: [-10.6, -10.0]	 value: [15.88, 2.9]
round 211	 loss: [0.16, 0.09]	 reward: [-7.8, -10.0]	 value: [15.76, 2.88]
round 212	 loss: [0.14, 0.12]	 reward: [-26.2, -10.0]	 value: [15.76, 2.9]
round 213	 loss: [0.15, 0.11]	 reward: [-33.2, -10.0]	 value: [15.59, 2.94]
round 214	 loss: [0.13, 0.1]	 reward: [-2.6, -10.0]	 value: [15.55, 2.88]
round 215	 loss: [0.13, 0.12]	 reward: [-34.2, -10.0]	 value: [15.43, 2.87]
round 216	 loss: [0.12, 0.1]	 reward: [4.2, -10.0]	 value: [15.28, 2.92]
round 217	 loss: [0.13, 0.14]	 reward: [5.4, -10.0]	 value: [15.39, 2.89]
round 218	 loss: [0.11, 0]	 reward: [6.8, -10.0]	 value: [15.18, 0]
round 219	 loss: [0.12, 0.11]	 reward: [1.8, -10.0]	 value: [15.08, 2.94]
round 220	 loss: [0.16, 0.13]	 reward: [6.2, -10.0]	 value: [15.13, 2.95]
round 221	 loss: [0.15, 0.11]	 reward: [-5.0, -10.0]	 value: [14.92, 2.91]
round 222	 loss: [0.12, 0.12]	 reward: [-4.6, -10.0]	 value: [14.77, 2.89]
round 223	 loss: [0.1, 0.08]	 reward: [0.0, -10.0]	 value: [14.73, 2.98]
round 224	 loss: [0.12, 0.1]	 reward: [-33.0, -10.0]	 value: [14.76, 3.0]
round 225	 loss: [0.14, 0.13]	 reward: [3.6, -10.0]	 value: [14.71, 2.96]

steps: 65,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.64
round time 0.89  total time 633.31

===== sample =====
eps 0.19775 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 1.4 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-5.2 -9. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-12.8  -9. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-21.4 -10. ] 
steps: 216,  total time: 1.44,  step average 0.01
===== train =====
train_time 2.65
round time 4.09  total time 637.39

save model... 
===== sample =====
eps 0.197 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 1.4 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [-4.6 -9. ] 
step 150,  reward: [0. 0.],  total_reward: [-12.8  -9. ] 
step 200,  reward: [0. 0.],  total_reward: [-21.  -9.] 
step 250,  reward: [-0.4  0. ],  total_reward: [-29.4  -9. ] 
steps: 251,  total time: 1.00,  step average 0.00
===== train =====
train_time 2.33
round time 3.34  total time 642.26

===== sample =====
eps 0.19625 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 25,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 642.35

save model... 
===== sample =====
eps 0.1955 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 29,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.23
round time 0.33  total time 644.03

===== sample =====
eps 0.19475 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  4.8 -10. ] 
steps: 85,  total time: 0.30,  step average 0.00
===== train =====
train_time 0.77
round time 1.08  total time 645.11

save model... 
===== sample =====
eps 0.194 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  1. -10.] 
step 100,  reward: [-0.4  0. ],  total_reward: [ -5.2 -10. ] 
steps: 115,  total time: 0.41,  step average 0.00
===== train =====
train_time 1.65
round time 2.06  total time 648.51

===== sample =====
eps 0.19325 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  3.2 -10. ] 
step 100,  reward: [0.6 0. ],  total_reward: [ -3. -10.] 
steps: 101,  total time: 1.58,  step average 0.02
===== train =====
train_time 1.59
round time 3.18  total time 651.69

save model... 
===== sample =====
eps 0.1925 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  1.6 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -4.8 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-10.6 -10. ] 
steps: 157,  total time: 0.85,  step average 0.01
===== train =====
train_time 1.77
round time 2.62  total time 656.01

===== sample =====
eps 0.19175 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [-0.2 -8. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [ -6. -10.] 
steps: 116,  total time: 0.47,  step average 0.00
===== train =====
train_time 1.17
round time 1.64  total time 657.65

save model... 
===== sample =====
eps 0.191 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  2.4 -10. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [ -6.4 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-13. -10.] 
step 200,  reward: [0. 0.],  total_reward: [-21. -10.] 
step 250,  reward: [0. 0.],  total_reward: [-26.2 -10. ] 
steps: 251,  total time: 1.13,  step average 0.00
===== train =====
train_time 2.60
round time 3.74  total time 662.95

===== sample =====
eps 0.19025 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-3.6 -6. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-12.  -6.] 
step 150,  reward: [-0.4  0. ],  total_reward: [-21.2  -8. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-24.2 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-33.2 -10. ] 
steps: 251,  total time: 0.91,  step average 0.00
===== train =====
train_time 2.95
round time 3.86  total time 666.82

save model... 
===== sample =====
eps 0.1895 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  3.6 -10. ] 
steps: 91,  total time: 0.35,  step average 0.00
===== train =====
train_time 0.83
round time 1.18  total time 669.64

===== sample =====
eps 0.18875 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  0.8 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [-10.4 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-20.4 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-27.2 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-34.2 -10. ] 
steps: 251,  total time: 0.89,  step average 0.00
===== train =====
train_time 2.28
round time 3.17  total time 672.81

save model... 
===== sample =====
eps 0.188 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 1.6 -8. ] 
steps: 55,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.60
round time 0.81  total time 675.16

===== sample =====
eps 0.18725 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 33,  total time: 0.33,  step average 0.01
===== train =====
train_time 0.52
round time 0.85  total time 676.01

save model... 
===== sample =====
eps 0.1865 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 30,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.25
round time 0.37  total time 678.30

===== sample =====
eps 0.18575 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [-1.2 -6. ] 
steps: 68,  total time: 0.93,  step average 0.01
===== train =====
train_time 0.73
round time 1.66  total time 679.96

save model... 
===== sample =====
eps 0.185 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 39,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.35
round time 0.50  total time 682.16

===== sample =====
eps 0.18425 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  0.4 -10. ] 
steps: 91,  total time: 0.35,  step average 0.00
===== train =====
train_time 0.80
round time 1.16  total time 683.32

save model... 
===== sample =====
eps 0.1835 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  4. -10.] 
step 100,  reward: [0. 0.],  total_reward: [ -2.6 -10. ] 
steps: 110,  total time: 0.51,  step average 0.00
===== train =====
train_time 1.16
round time 1.67  total time 686.51

===== sample =====
eps 0.18275000000000002 number [5, 5]
step   0,  reward: [ 0.2 -1. ],  total_reward: [ 0.2 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  2. -10.] 
steps: 66,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.58
round time 0.83  total time 687.34

save model... 
===== sample =====
eps 0.182 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [ -0.6 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -9.8 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-18.4 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-29.2 -10. ] 
steps: 224,  total time: 0.85,  step average 0.00
===== train =====
train_time 2.19
round time 3.04  total time 691.93

===== sample =====
eps 0.18125000000000002 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  5.4 -10. ] 
steps: 71,  total time: 0.32,  step average 0.00
===== train =====
train_time 0.58
round time 0.90  total time 692.83

save model... 
===== sample =====
eps 0.1805 number [5, 5]
batch number: 3  add: 385  replay_len: 124940/4194304
batch     0,  loss 0.395562, eval 17.776081
batches: 3,  total time: 0.88,  1k average: 0.88
batch number: 9  add: 1255  replay_len: 126195/4194304
batch     0,  loss 0.437632, eval 17.616100
batches: 9,  total time: 2.26,  1k average: 2.26
batch number: 9  add: 1255  replay_len: 127450/4194304
batch     0,  loss 0.414556, eval 17.909245
batches: 9,  total time: 2.88,  1k average: 2.88
batch number: 4  add: 565  replay_len: 128015/4194304
batch     0,  loss 0.483990, eval 17.805384
batches: 4,  total time: 1.00,  1k average: 1.00
batch number: 9  add: 1255  replay_len: 129270/4194304
batch     0,  loss 0.365152, eval 17.710911
batches: 9,  total time: 2.58,  1k average: 2.58
batch number: 9  add: 1255  replay_len: 130525/4194304
batch     0,  loss 0.406764, eval 17.677231
batches: 9,  total time: 2.33,  1k average: 2.33
batch number: 2  add: 375  replay_len: 130900/4194304
batch     0,  loss 0.276556, eval 17.802044
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 9  add: 1255  replay_len: 132155/4194304
batch     0,  loss 0.438255, eval 17.786758
batches: 9,  total time: 2.56,  1k average: 2.56
batch number: 9  add: 1255  replay_len: 133410/4194304
batch     0,  loss 0.351597, eval 17.381624
batches: 9,  total time: 3.23,  1k average: 3.23
batch number: 9  add: 1255  replay_len: 134665/4194304
batch     0,  loss 0.311201, eval 17.308014
batches: 9,  total time: 2.90,  1k average: 2.90
batch number: 9  add: 1255  replay_len: 135920/4194304
batch     0,  loss 0.413597, eval 17.306810
batches: 9,  total time: 3.10,  1k average: 3.10
batch number: 6  add: 825  replay_len: 136745/4194304
batch     0,  loss 0.242037, eval 17.386913
batches: 6,  total time: 1.72,  1k average: 1.72
batch number: 9  add: 1255  replay_len: 138000/4194304
batch     0,  loss 0.292322, eval 17.429306
batches: 9,  total time: 3.04,  1k average: 3.04
batch number: 9  add: 1255  replay_len: 139255/4194304
batch     0,  loss 0.253295, eval 17.428848
batches: 9,  total time: 2.63,  1k average: 2.63
batch number: 9  add: 1255  replay_len: 140510/4194304
batch     0,  loss 0.331011, eval 17.428894
batches: 9,  total time: 2.71,  1k average: 2.71
batch number: 9  add: 1255  replay_len: 141765/4194304
batch     0,  loss 0.307045, eval 17.526051
batches: 9,  total time: 2.30,  1k average: 2.30
batch number: 9  add: 1255  replay_len: 143020/4194304
batch     0,  loss 0.246775, eval 17.592564
batches: 9,  total time: 3.01,  1k average: 3.01
batch number: 9  add: 1255  replay_len: 144275/4194304
batch     0,  loss 0.275324, eval 17.415041
batches: 9,  total time: 2.45,  1k average: 2.45
batch number: 8  add: 1070  replay_len: 145345/4194304
batch     0,  loss 0.235653, eval 17.365225
batches: 8,  total time: 2.58,  1k average: 2.58
batch number: 9  add: 1255  replay_len: 146600/4194304
batch     0,  loss 0.270225, eval 17.551504
batches: 9,  total time: 2.72,  1k average: 2.72
batch number: 9  add: 1255  replay_len: 147855/4194304
batch     0,  loss 0.203918, eval 17.410252
batches: 9,  total time: 3.07,  1k average: 3.07
batch number: 4  add: 560  replay_len: 148415/4194304
batch     0,  loss 0.214027, eval 17.469904
batches: 4,  total time: 1.12,  1k average: 1.12
batch number: 9  add: 1255  replay_len: 149670/4194304
batch     0,  loss 0.300570, eval 17.303440
batches: 9,  total time: 2.77,  1k average: 2.77
batch number: 9  add: 1255  replay_len: 150925/4194304
batch     0,  loss 0.200219, eval 17.061504
batches: 9,  total time: 2.30,  1k average: 2.30
batch number: 9  add: 1255  replay_len: 152180/4194304
batch     0,  loss 0.184401, eval 17.008797
batches: 9,  total time: 3.27,  1k average: 3.27
batch number: 9  add: 1255  replay_len: 153435/4194304
batch     0,  loss 0.175135, eval 16.978214
batches: 9,  total time: 2.53,  1k average: 2.53
batch number: 9  add: 1255  replay_len: 154690/4194304
batch     0,  loss 0.175827, eval 16.858644
batches: 9,  total time: 3.35,  1k average: 3.35
batch number: 9  add: 1255  replay_len: 155945/4194304
batch     0,  loss 0.242429, eval 16.806257
batches: 9,  total time: 2.71,  1k average: 2.71
batch number: 8  add: 1130  replay_len: 157075/4194304
batch     0,  loss 0.170903, eval 16.754234
batches: 8,  total time: 2.68,  1k average: 2.68
batch number: 9  add: 1240  replay_len: 158315/4194304
batch     0,  loss 0.201496, eval 16.693802
batches: 9,  total time: 2.78,  1k average: 2.78
batch number: 7  add: 975  replay_len: 159290/4194304
batch     0,  loss 0.198729, eval 16.682180
batches: 7,  total time: 1.90,  1k average: 1.90
batch number: 9  add: 1255  replay_len: 160545/4194304
batch     0,  loss 0.166291, eval 16.445560
batches: 9,  total time: 2.71,  1k average: 2.71
batch number: 2  add: 325  replay_len: 160870/4194304
batch     0,  loss 0.181295, eval 16.481464
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 8  add: 1080  replay_len: 161950/4194304
batch     0,  loss 0.161247, eval 16.438606
batches: 8,  total time: 2.63,  1k average: 2.63
batch number: 9  add: 1255  replay_len: 163205/4194304
batch     0,  loss 0.154371, eval 16.223160
batches: 9,  total time: 2.32,  1k average: 2.32
batch number: 1  add: 145  replay_len: 163475/4194304
batch     0,  loss 0.161434, eval 16.259842
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 3  add: 425  replay_len: 163900/4194304
batch     0,  loss 0.149265, eval 16.259010
batches: 3,  total time: 0.77,  1k average: 0.77
batch number: 4  add: 575  replay_len: 164475/4194304
batch     0,  loss 0.169385, eval 16.160667
batches: 4,  total time: 1.64,  1k average: 1.64
batch number: 3  add: 505  replay_len: 164980/4194304
batch     0,  loss 0.163329, eval 15.921095
batches: 3,  total time: 1.58,  1k average: 1.58
batch number: 6  add: 785  replay_len: 165765/4194304
batch     0,  loss 0.164756, eval 15.924999
batches: 6,  total time: 1.76,  1k average: 1.76
batch number: 4  add: 580  replay_len: 166345/4194304
batch     0,  loss 0.136568, eval 15.716028
batches: 4,  total time: 1.16,  1k average: 1.16
batch number: 9  add: 1255  replay_len: 167600/4194304
batch     0,  loss 0.172177, eval 15.784823
batches: 9,  total time: 2.58,  1k average: 2.58
batch number: 9  add: 1255  replay_len: 168855/4194304
batch     0,  loss 0.171767, eval 15.687210
batches: 9,  total time: 2.94,  1k average: 2.94
batch number: 3  add: 455  replay_len: 169310/4194304
batch     0,  loss 0.151398, eval 15.691023
batches: 3,  total time: 0.82,  1k average: 0.82
batch number: 9  add: 1255  replay_len: 170565/4194304
batch     0,  loss 0.149176, eval 15.709036
batches: 9,  total time: 2.25,  1k average: 2.25
batch number: 2  add: 275  replay_len: 170840/4194304
batch     0,  loss 0.132029, eval 15.378248
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 165  replay_len: 171005/4194304
batch     0,  loss 0.129340, eval 15.386177
batches: 1,  total time: 0.52,  1k average: 0.52
batch number: 1  add: 150  replay_len: 171155/4194304
batch     0,  loss 0.112336, eval 15.177561
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 340  replay_len: 171495/4194304
batch     0,  loss 0.112064, eval 15.130362
batches: 2,  total time: 0.72,  1k average: 0.72
batch number: 1  add: 195  replay_len: 171690/4194304
batch     0,  loss 0.161957, eval 15.130597
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 3  add: 455  replay_len: 172145/4194304
batch     0,  loss 0.150968, eval 14.885433
batches: 3,  total time: 0.80,  1k average: 0.80
batch number: 4  add: 550  replay_len: 172695/4194304
batch     0,  loss 0.133345, eval 14.871501
batches: 4,  total time: 1.15,  1k average: 1.15
batch number: 2  add: 330  replay_len: 173025/4194304
batch     0,  loss 0.087445, eval 14.711954
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 8  add: 1120  replay_len: 174145/4194304
batch     0,  loss 0.108748, eval 14.675810
batches: 8,  total time: 2.18,  1k average: 2.18
batch number: 2  add: 355  replay_len: 174500/4194304
batch     0,  loss 0.134374, eval 14.766033
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 3  add: 450  replay_len: 174950/4194304
batch     0,  loss 0.123504, eval 14.665244round 226	 loss: [0.11, 0.11]	 reward: [-0.2, -10.0]	 value: [14.58, 2.99]
round 227	 loss: [0.11, 0.16]	 reward: [-3.6, -10.0]	 value: [14.58, 2.94]
round 228	 loss: [0.11, 0.11]	 reward: [-26.2, -9.0]	 value: [14.47, 2.93]
round 229	 loss: [0.11, 0.1]	 reward: [-32.2, -10.0]	 value: [14.41, 2.97]
round 230	 loss: [0.1, 0.17]	 reward: [0.6, -10.0]	 value: [14.29, 2.9]
round 231	 loss: [0.12, 0.14]	 reward: [-13.2, -10.0]	 value: [14.17, 2.98]
round 232	 loss: [0.11, 0.16]	 reward: [-1.4, -10.0]	 value: [14.29, 2.89]
round 233	 loss: [0.09, 0.11]	 reward: [0.8, -10.0]	 value: [14.12, 2.92]
round 234	 loss: [0.1, 0.11]	 reward: [6.4, -10.0]	 value: [14.18, 2.9]
round 235	 loss: [0.11, 0.1]	 reward: [0.6, -10.0]	 value: [13.94, 2.96]
round 236	 loss: [0.1, 0.11]	 reward: [3.4, -10.0]	 value: [13.76, 2.96]
round 237	 loss: [0.1, 0.11]	 reward: [-28.6, -10.0]	 value: [13.63, 3.01]
round 238	 loss: [0.1, 0.14]	 reward: [-2.0, -10.0]	 value: [13.48, 2.97]
round 239	 loss: [0.09, 0.14]	 reward: [-7.0, -10.0]	 value: [13.44, 3.05]
round 240	 loss: [0.09, 0]	 reward: [0.0, -10.0]	 value: [13.44, 0]
round 241	 loss: [0.11, 0.12]	 reward: [7.0, -10.0]	 value: [13.47, 3.06]
round 242	 loss: [0.08, 0.13]	 reward: [2.8, -10.0]	 value: [13.39, 3.1]
round 243	 loss: [0.08, 0]	 reward: [1.2, -10.0]	 value: [13.12, 0]
round 244	 loss: [0.09, 0.11]	 reward: [-11.0, -10.0]	 value: [13.05, 3.03]
round 245	 loss: [0.08, 0.1]	 reward: [-5.2, -10.0]	 value: [13.08, 3.03]
round 246	 loss: [0.09, 0.15]	 reward: [-6.6, -10.0]	 value: [13.02, 3.04]
round 247	 loss: [0.09, 0.11]	 reward: [4.4, -10.0]	 value: [13.01, 3.1]
round 248	 loss: [0.08, 0.1]	 reward: [0.6, -10.0]	 value: [12.9, 3.12]
round 249	 loss: [0.08, 0]	 reward: [7.2, -10.0]	 value: [12.9, 0]
round 250	 loss: [0.07, 0.14]	 reward: [2.2, -10.0]	 value: [12.83, 3.05]

step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  2.4 -10. ] 
steps: 90,  total time: 0.41,  step average 0.00
===== train =====
train_time 0.99
round time 1.40  total time 695.81

===== sample =====
eps 0.17975000000000002 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  0.4 -10. ] 
steps: 95,  total time: 0.47,  step average 0.00
===== train =====
train_time 0.85
round time 1.32  total time 697.13

save model... 
===== sample =====
eps 0.179 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 0. -9.] 
step 100,  reward: [0. 0.],  total_reward: [-7.2 -9. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-13.8  -9. ] 
step 200,  reward: [0. 0.],  total_reward: [-18.8  -9. ] 
step 250,  reward: [0. 0.],  total_reward: [-26.2  -9. ] 
steps: 251,  total time: 0.99,  step average 0.00
===== train =====
train_time 3.38
round time 4.37  total time 703.21

===== sample =====
eps 0.17825000000000002 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  0.2 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -9.4 -10. ] 
step 150,  reward: [-0.6  0. ],  total_reward: [-17.8 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-24.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-32.2 -10. ] 
steps: 251,  total time: 5.00,  step average 0.02
===== train =====
train_time 2.74
round time 7.75  total time 710.95

save model... 
===== sample =====
eps 0.17750000000000002 number [5, 5]
step   0,  reward: [-0.8  0. ],  total_reward: [-0.8  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  2.6 -10. ] 
steps: 70,  total time: 0.26,  step average 0.00
===== train =====
train_time 0.57
round time 0.83  total time 713.48

===== sample =====
eps 0.17675000000000002 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 4. -9.] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -3.2 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [ -9.4 -10. ] 
steps: 176,  total time: 0.62,  step average 0.00
===== train =====
train_time 2.13
round time 2.75  total time 716.23

save model... 
===== sample =====
eps 0.17600000000000002 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 1.2 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -2. -10.] 
steps: 102,  total time: 0.37,  step average 0.00
===== train =====
train_time 0.99
round time 1.36  total time 719.42

===== sample =====
eps 0.17525000000000002 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 0.4 -9. ] 
steps: 66,  total time: 0.35,  step average 0.01
===== train =====
train_time 0.67
round time 1.02  total time 720.45

save model... 
===== sample =====
eps 0.17450000000000002 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 45,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.35
round time 0.51  total time 722.58

===== sample =====
eps 0.17375000000000002 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  0.4 -10. ] 
steps: 70,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.63
round time 0.87  total time 723.45

save model... 
===== sample =====
eps 0.17300000000000001 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  2.8 -10. ] 
steps: 54,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.65
round time 0.85  total time 725.99

===== sample =====
eps 0.17225000000000001 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  1.6 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -3.8 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [-11.4 -10. ] 
step 200,  reward: [0. 0.],  total_reward: [-19.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-28.6 -10. ] 
steps: 251,  total time: 0.97,  step average 0.00
===== train =====
train_time 2.47
round time 3.44  total time 729.42

save model... 
===== sample =====
eps 0.1715 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 2. -9.] 
steps: 85,  total time: 0.35,  step average 0.00
===== train =====
train_time 1.03
round time 1.38  total time 732.82

===== sample =====
eps 0.17075 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [-3.8 -7. ] 
steps: 100,  total time: 1.85,  step average 0.02
===== train =====
train_time 1.12
round time 2.97  total time 735.79

save model... 
===== sample =====
eps 0.17 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 49,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.25
round time 0.44  total time 737.87

===== sample =====
eps 0.16925 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 43,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.36
round time 0.52  total time 738.38

save model... 
===== sample =====
eps 0.1685 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 2. -9.] 
steps: 56,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.60
round time 0.85  total time 740.95

===== sample =====
eps 0.16775 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  1. -10.] 
steps: 56,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.65
round time 0.86  total time 741.81

save model... 
===== sample =====
eps 0.167 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  0.2 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -9.4 -10. ] 
steps: 113,  total time: 0.99,  step average 0.01
===== train =====
train_time 1.95
round time 2.95  total time 747.31

===== sample =====
eps 0.16625 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [ 2.6 -9. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [ -3. -10.] 
steps: 131,  total time: 0.89,  step average 0.01
===== train =====
train_time 2.49
round time 3.37  total time 750.69

save model... 
===== sample =====
eps 0.1655 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  4. -10.] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -2.6 -10. ] 
steps: 129,  total time: 0.69,  step average 0.01
===== train =====
train_time 1.59
round time 2.28  total time 755.25

===== sample =====
eps 0.16475 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  4. -10.] 
steps: 54,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.66
round time 0.89  total time 756.14

save model... 
===== sample =====
eps 0.164 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  2.2 -10. ] 
steps: 74,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.55
round time 0.83  total time 758.63

===== sample =====
eps 0.16325 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 26,  total time: 0.30,  step average 0.01
===== train =====
train_time 0.23
round time 0.53  total time 759.16

save model... 
===== sample =====
eps 0.1625 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 48,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.34
round time 0.52  total time 761.76

===== sample =====
eps 0.16175 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 47,  total time: 0.16,  step average 0.00round 251	 loss: [0.09, 0]	 reward: [3.4, -10.0]	 value: [12.67, 0]
round 252	 loss: [0.11, 0.11]	 reward: [-24.8, -10.0]	 value: [12.48, 3.15]
round 253	 loss: [0.09, 0.11]	 reward: [-9.6, -10.0]	 value: [12.5, 3.11]
round 254	 loss: [0.08, 0.11]	 reward: [-24.0, -10.0]	 value: [12.44, 3.22]
round 255	 loss: [0.08, 0.13]	 reward: [-6.0, -10.0]	 value: [12.46, 3.19]
round 256	 loss: [0.07, 0]	 reward: [8.0, -10.0]	 value: [12.54, 0]
round 257	 loss: [0.06, 0]	 reward: [4.8, -10.0]	 value: [12.38, 0]
round 258	 loss: [0.08, 0.12]	 reward: [-11.6, -10.0]	 value: [12.41, 3.1]
round 259	 loss: [0.08, 0.13]	 reward: [-51.4, -10.0]	 value: [12.36, 3.13]
round 260	 loss: [0.07, 0]	 reward: [0.0, -10.0]	 value: [12.39, 0]

batch number: 4  add: 584  replay_len: 85007/4194304
batch     0,  loss 0.111024, eval 2.852752
batches: 4,  total time: 1.19,  1k average: 1.19
batch number: 8  add: 1028  replay_len: 86035/4194304
batch     0,  loss 0.108339, eval 2.853428
batches: 8,  total time: 2.71,  1k average: 2.71
batch number: 5  add: 655  replay_len: 86690/4194304
batch     0,  loss 0.089334, eval 2.910548
batches: 5,  total time: 1.46,  1k average: 1.46
batch number: 2  add: 359  replay_len: 87049/4194304
batch     0,  loss 0.097233, eval 2.885802
batches: 2,  total time: 0.77,  1k average: 0.77
batch number: 3  add: 430  replay_len: 87479/4194304
batch     0,  loss 0.102512, eval 2.897875
batches: 3,  total time: 0.99,  1k average: 0.99
batch number: 3  add: 485  replay_len: 87964/4194304
batch     0,  loss 0.121122, eval 2.809353
batches: 3,  total time: 0.85,  1k average: 0.85
batch number: 6  add: 799  replay_len: 88763/4194304
batch     0,  loss 0.090121, eval 2.855085
batches: 6,  total time: 1.87,  1k average: 1.87
batch number: 1  add: 212  replay_len: 88975/4194304
batch     0,  loss 0.103184, eval 2.863338
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 3  add: 481  replay_len: 89456/4194304
batch     0,  loss 0.104992, eval 2.851874
batches: 3,  total time: 1.13,  1k average: 1.13
batch number: 2  add: 335  replay_len: 89791/4194304
batch     0,  loss 0.140599, eval 2.825524
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 1  add: 252  replay_len: 90227/4194304
batch     0,  loss 0.101835, eval 2.863029
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 2  add: 378  replay_len: 90605/4194304
batch     0,  loss 0.077531, eval 2.837958
batches: 2,  total time: 0.90,  1k average: 0.90
batch number: 2  add: 313  replay_len: 90918/4194304
batch     0,  loss 0.135888, eval 2.761292
batches: 2,  total time: 0.95,  1k average: 0.95
batch number: 2  add: 332  replay_len: 91250/4194304
batch     0,  loss 0.133842, eval 2.787943
batches: 2,  total time: 0.76,  1k average: 0.76
batch number: 2  add: 298  replay_len: 91548/4194304
batch     0,  loss 0.102735, eval 2.881527
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 2  add: 318  replay_len: 91866/4194304
batch     0,  loss 0.122050, eval 2.840446
batches: 2,  total time: 0.82,  1k average: 0.82
batch number: 6  add: 821  replay_len: 92687/4194304
batch     0,  loss 0.115158, eval 2.901481
batches: 6,  total time: 2.02,  1k average: 2.02
batch number: 1  add: 217  replay_len: 92904/4194304
batch     0,  loss 0.101270, eval 2.879083
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 315  replay_len: 93219/4194304
batch     0,  loss 0.106692, eval 2.942225
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 1  add: 176  replay_len: 93395/4194304
batch     0,  loss 0.098680, eval 2.916194
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 141  replay_len: 93536/4194304
batch     0,  loss 0.140092, eval 2.886463
batches: 1,  total time: 0.50,  1k average: 0.50
batch number: 1  add: 239  replay_len: 93864/4194304
batch     0,  loss 0.108336, eval 2.935123
batches: 1,  total time: 0.42,  1k average: 0.42
batch number: 1  add: 144  replay_len: 94008/4194304
batch     0,  loss 0.133919, eval 2.949450
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 1  add: 233  replay_len: 94241/4194304
batch     0,  loss 0.106999, eval 2.910962
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 271  replay_len: 94512/4194304
batch     0,  loss 0.120531, eval 2.823034
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 219  replay_len: 94731/4194304
batch     0,  loss 0.084311, eval 2.977532
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 3  add: 396  replay_len: 95127/4194304
batch     0,  loss 0.118694, eval 2.961551
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 1  add: 200  replay_len: 95327/4194304
batch     0,  loss 0.125356, eval 2.956613
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 256  replay_len: 95583/4194304
batch     0,  loss 0.110471, eval 3.021391
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 1  add: 249  replay_len: 95832/4194304
batch     0,  loss 0.157347, eval 2.942115
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 6  add: 853  replay_len: 96685/4194304
batch     0,  loss 0.096385, eval 3.046404
batches: 6,  total time: 1.84,  1k average: 1.84
batch number: 2  add: 359  replay_len: 97044/4194304
batch     0,  loss 0.098541, eval 3.043036
batches: 2,  total time: 0.87,  1k average: 0.87
batch number: 1  add: 187  replay_len: 97231/4194304
batch     0,  loss 0.169080, eval 2.898211
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 368  replay_len: 97599/4194304
batch     0,  loss 0.161688, eval 2.986180
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 1  add: 252  replay_len: 97851/4194304
batch     0,  loss 0.157990, eval 2.892630
batches: 1,  total time: 0.46,  1k average: 0.46
batch number: 1  add: 197  replay_len: 98048/4194304
batch     0,  loss 0.111974, eval 2.918864
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 157  replay_len: 98205/4194304
batch     0,  loss 0.106603, eval 2.898062
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 184  replay_len: 98389/4194304
batch     0,  loss 0.103476, eval 2.957401
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 143  replay_len: 98532/4194304
batch     0,  loss 0.114319, eval 2.958634
batches: 1,  total time: 0.37,  1k average: 0.37
batch number: 3  add: 390  replay_len: 98922/4194304
batch     0,  loss 0.112427, eval 3.016078
batches: 3,  total time: 0.94,  1k average: 0.94
batch number: 1  add: 183  replay_len: 99105/4194304
batch     0,  loss 0.140350, eval 2.970787
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 2  add: 355  replay_len: 99460/4194304
batch     0,  loss 0.110901, eval 3.151118
batches: 2,  total time: 0.72,  1k average: 0.72
batch number: 1  add: 133  replay_len: 99715/4194304
batch     0,  loss 0.120999, eval 3.064305
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 147  replay_len: 99862/4194304
batch     0,  loss 0.130689, eval 3.098543
batches: 1,  total time: 0.29,  1k average: 0.29
batch number: 1  add: 247  replay_len: 100219/4194304
batch     0,  loss 0.106556, eval 3.028529
batches: 1,  total time: 0.48,  1k average: 0.48
batch number: 2  add: 347  replay_len: 100566/4194304
batch     0,  loss 0.107687, eval 2.997655
batches: 2,  total time: 1.04,  1k average: 1.04
batch number: 2  add: 288  replay_len: 100854/4194304
batch     0,  loss 0.147593, eval 2.999988
batches: 2,  total time: 0.75,  1k average: 0.75
batch number: 1  add: 137  replay_len: 100991/4194304
batch     0,  loss 0.112454, eval 3.098002
batches: 1,  total time: 0.41,  1k average: 0.41
batch number: 1  add: 208  replay_len: 101199/4194304
batch     0,  loss 0.095521, eval 3.120474
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 172  replay_len: 101431/4194304
batch     0,  loss 0.135927, eval 3.046703
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 279  replay_len: 101832/4194304
batch     0,  loss 0.121799, eval 3.130072
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 1  add: 220  replay_len: 102052/4194304
batch     0,  loss 0.106449, eval 3.107495
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 362  replay_len: 102414/4194304
batch     0,  loss 0.109165, eval 3.107799
batches: 2,  total time: 0.63,  1k average: 0.63
batch number: 2  add: 294  replay_len: 102708/4194304
batch     0,  loss 0.123911, eval 3.112337
batches: 2,  total time: 0.73,  1k average: 0.73
batch number: 2  add: 262  replay_len: 103160/4194304
batch     0,  loss 0.123011, eval 3.117618
batches: 2,  total time: 0.70,  1k average: 0.70
batch number: 3  add: 438  replay_len: 103598/4194304
batch     0,  loss 0.154194, eval 3.146808
batches: 3,  total time: 1.13,  1k average: 1.13
batch number: 1  add: 237  replay_len: 103960/4194304
batch     0,  loss 0.106497, eval 3.152230round 261	 loss: [0.08, 0.11]	 reward: [-11.4, -10.0]	 value: [12.39, 3.15]
round 262	 loss: [0.07, 0.13]	 reward: [-10.8, -10.0]	 value: [12.37, 3.15]
round 263	 loss: [0.08, 0.12]	 reward: [-20.8, -10.0]	 value: [12.3, 3.23]
round 264	 loss: [0.07, 0.15]	 reward: [-16.4, -10.0]	 value: [12.24, 3.17]
round 265	 loss: [0.07, 0.12]	 reward: [-1.8, -10.0]	 value: [12.2, 3.26]
round 266	 loss: [0, 0]	 reward: [9.6, -10.0]	 value: [0, 0]
round 267	 loss: [0.07, 0]	 reward: [8.4, -10.0]	 value: [12.23, 0]
round 268	 loss: [0.07, 0.13]	 reward: [-8.6, -10.0]	 value: [12.26, 3.21]
round 269	 loss: [0.07, 0.2]	 reward: [-1.4, -10.0]	 value: [12.28, 3.16]
round 270	 loss: [0.08, 0.14]	 reward: [6.6, -10.0]	 value: [12.17, 3.23]
round 271	 loss: [0.06, 0]	 reward: [8.6, -10.0]	 value: [12.15, 0]
round 272	 loss: [0.07, 0.14]	 reward: [1.6, -10.0]	 value: [12.09, 3.17]
round 273	 loss: [0.06, 0.12]	 reward: [-15.8, -10.0]	 value: [11.96, 3.16]
round 274	 loss: [0.07, 0]	 reward: [7.2, -10.0]	 value: [11.9, 0]
round 275	 loss: [0.06, 0.12]	 reward: [8.0, -10.0]	 value: [11.88, 3.17]

===== train =====
train_time 0.23
round time 0.40  total time 762.16

save model... 
===== sample =====
eps 0.161 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ -1.8 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-11.8 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-23.6 -10. ] 
steps: 161,  total time: 0.58,  step average 0.00
===== train =====
train_time 1.55
round time 2.13  total time 765.94

===== sample =====
eps 0.16025 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [ 1.4 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -6.8 -10. ] 
steps: 127,  total time: 0.48,  step average 0.00
===== train =====
train_time 1.02
round time 1.50  total time 767.43

save model... 
===== sample =====
eps 0.1595 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  2.6 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -3.4 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [ -9. -10.] 
step 200,  reward: [0. 0.],  total_reward: [-17.2 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-24. -10.] 
steps: 251,  total time: 0.92,  step average 0.00
===== train =====
train_time 2.47
round time 3.40  total time 772.40

===== sample =====
eps 0.15875 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 3.2 -7. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [  0.2 -10. ] 
steps: 141,  total time: 0.49,  step average 0.00
===== train =====
train_time 1.66
round time 2.15  total time 774.56

save model... 
===== sample =====
eps 0.158 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 27,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.23
round time 0.33  total time 776.63

===== sample =====
eps 0.15725 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 34,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 776.97

save model... 
===== sample =====
eps 0.1565 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-1.8 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [ -8.2 -10. ] 
steps: 115,  total time: 0.41,  step average 0.00
===== train =====
train_time 1.40
round time 1.81  total time 780.47

===== sample =====
eps 0.15575 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [-11.2  -4. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [-22.8  -8. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-35.6 -10. ] 
step 200,  reward: [0. 0.],  total_reward: [-50.2 -10. ] 
steps: 209,  total time: 4.06,  step average 0.02
===== train =====
train_time 2.77
round time 6.84  total time 787.31

save model... 
===== sample =====
eps 0.155 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0.4 0. ],  total_reward: [  0. -10.] 
steps: 51,  total time: 0.63,  step average 0.01
===== train =====
train_time 0.30
round time 0.93  total time 790.04

===== sample =====
eps 0.15425 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  0.6 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [-10.6 -10. ] 
steps: 108,  total time: 0.70,  step average 0.01
===== train =====
train_time 1.07
round time 1.77  total time 791.81

save model... 
===== sample =====
eps 0.1535 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 2.4 -9. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [-4.4 -9. ] 
step 150,  reward: [0. 0.],  total_reward: [ -9.8 -10. ] 
steps: 164,  total time: 1.01,  step average 0.01
===== train =====
train_time 1.77
round time 2.78  total time 796.30

===== sample =====
eps 0.15275 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  3. -10.] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -3.8 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-10.8 -10. ] 
step 200,  reward: [0. 0.],  total_reward: [-17. -10.] 
steps: 227,  total time: 0.86,  step average 0.00
===== train =====
train_time 2.34
round time 3.20  total time 799.50

save model... 
===== sample =====
eps 0.152 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  3.4 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -4.6 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [-14.2 -10. ] 
steps: 166,  total time: 0.60,  step average 0.00
===== train =====
train_time 1.60
round time 2.20  total time 803.64

===== sample =====
eps 0.15125 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  4.8 -10. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [ -1.8 -10. ] 
steps: 110,  total time: 0.39,  step average 0.00
===== train =====
train_time 1.01
round time 1.41  total time 805.05

save model... 
===== sample =====
eps 0.1505 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 22,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 806.82

===== sample =====
eps 0.14975 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 38,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.23
round time 0.37  total time 807.19

save model... 
===== sample =====
eps 0.149 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [ 4.4 -9. ] 
step 100,  reward: [-0.4  0. ],  total_reward: [ -2.8 -10. ] 
steps: 128,  total time: 0.46,  step average 0.00
===== train =====
train_time 1.24
round time 1.70  total time 810.60

===== sample =====
eps 0.14825 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  2.4 -10. ] 
steps: 85,  total time: 1.20,  step average 0.01
===== train =====
train_time 1.05
round time 2.25  total time 812.85

save model... 
===== sample =====
eps 0.1475 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  6.2 -10. ] 
steps: 54,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.58
round time 0.77  total time 815.52

===== sample =====
eps 0.14675 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 28,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.22
round time 0.33  total time 815.85

save model... 
===== sample =====
eps 0.14600000000000002 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  3. -10.] 
steps: 85,  total time: 0.63,  step average 0.01
===== train =====
train_time 0.90
round time 1.53  total time 819.47

===== sample =====
eps 0.14525 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 4.4 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [ 0.2 -9. ] 
step 150,  reward: [0. 0.],  total_reward: [ -6.4 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-10.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-15.8 -10. ] 
steps: 251,  total time: 1.24,  step average 0.00
===== train =====
train_time 2.55
round time 3.79  total time 823.27

save model... 
===== sample =====
eps 0.14450000000000002 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 31,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.26
round time 0.40  total time 825.89

===== sample =====
eps 0.14375 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 43,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.40
round time 0.58  total time 826.47

save model... 
===== sample =====round 276	 loss: [0.07, 0]	 reward: [7.6, -10.0]	 value: [11.77, 0]
round 277	 loss: [0.07, 0]	 reward: [8.2, -10.0]	 value: [11.74, 0]
round 278	 loss: [0.05, 0.14]	 reward: [5.8, -10.0]	 value: [11.46, 3.16]
round 279	 loss: [0.05, 0]	 reward: [3.4, -10.0]	 value: [11.33, 0]
round 280	 loss: [0.1, 0.14]	 reward: [-12.0, -10.0]	 value: [10.95, 3.06]
round 281	 loss: [0.09, 0.13]	 reward: [-9.2, -10.0]	 value: [11.05, 3.1]
round 282	 loss: [0.07, 0.15]	 reward: [-23.6, -10.0]	 value: [10.97, 3.12]

batches: 3,  total time: 0.99,  1k average: 0.99
batch number: 3  add: 475  replay_len: 175425/4194304
batch     0,  loss 0.107330, eval 14.590084
batches: 3,  total time: 0.84,  1k average: 0.84
batch number: 9  add: 1255  replay_len: 176680/4194304
batch     0,  loss 0.128015, eval 14.588799
batches: 9,  total time: 3.36,  1k average: 3.36
batch number: 9  add: 1255  replay_len: 177935/4194304
batch     0,  loss 0.112799, eval 14.445471
batches: 9,  total time: 2.72,  1k average: 2.72
batch number: 2  add: 350  replay_len: 178285/4194304
batch     0,  loss 0.101195, eval 14.389360
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 6  add: 880  replay_len: 179165/4194304
batch     0,  loss 0.105366, eval 14.254902
batches: 6,  total time: 2.12,  1k average: 2.12
batch number: 3  add: 510  replay_len: 179675/4194304
batch     0,  loss 0.087921, eval 14.116777
batches: 3,  total time: 0.98,  1k average: 0.98
batch number: 2  add: 330  replay_len: 180005/4194304
batch     0,  loss 0.108589, eval 14.224071
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 1  add: 225  replay_len: 180230/4194304
batch     0,  loss 0.101605, eval 14.176400
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 350  replay_len: 180580/4194304
batch     0,  loss 0.099455, eval 14.054390
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 2  add: 270  replay_len: 180850/4194304
batch     0,  loss 0.103435, eval 13.888773
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 9  add: 1255  replay_len: 182105/4194304
batch     0,  loss 0.103242, eval 13.703794
batches: 9,  total time: 2.45,  1k average: 2.45
batch number: 3  add: 425  replay_len: 182530/4194304
batch     0,  loss 0.108570, eval 13.653713
batches: 3,  total time: 1.02,  1k average: 1.02
batch number: 3  add: 500  replay_len: 183030/4194304
batch     0,  loss 0.099279, eval 13.548163
batches: 3,  total time: 1.12,  1k average: 1.12
batch number: 1  add: 245  replay_len: 183275/4194304
batch     0,  loss 0.086856, eval 13.440476
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 215  replay_len: 183490/4194304
batch     0,  loss 0.107222, eval 13.470505
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 280  replay_len: 183770/4194304
batch     0,  loss 0.084092, eval 13.432067
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 2  add: 280  replay_len: 184050/4194304
batch     0,  loss 0.087333, eval 13.374568
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 4  add: 565  replay_len: 184615/4194304
batch     0,  loss 0.081837, eval 13.149838
batches: 4,  total time: 1.94,  1k average: 1.94
batch number: 5  add: 655  replay_len: 185270/4194304
batch     0,  loss 0.071658, eval 13.031881
batches: 5,  total time: 2.48,  1k average: 2.48
batch number: 5  add: 645  replay_len: 185915/4194304
batch     0,  loss 0.093446, eval 12.991426
batches: 5,  total time: 1.58,  1k average: 1.58
batch number: 2  add: 270  replay_len: 186185/4194304
batch     0,  loss 0.086437, eval 12.974870
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 2  add: 370  replay_len: 186555/4194304
batch     0,  loss 0.087735, eval 12.921363
batches: 2,  total time: 0.55,  1k average: 0.55
batch number: 1  add: 130  replay_len: 186685/4194304
batch     0,  loss 0.075175, eval 12.896292
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 240  replay_len: 186925/4194304
batch     0,  loss 0.066473, eval 12.828211
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 235  replay_len: 187160/4194304
batch     0,  loss 0.090744, eval 12.672222
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 6  add: 805  replay_len: 187965/4194304
batch     0,  loss 0.090787, eval 12.563406
batches: 6,  total time: 1.54,  1k average: 1.54
batch number: 4  add: 635  replay_len: 188600/4194304
batch     0,  loss 0.103020, eval 12.508764
batches: 4,  total time: 1.01,  1k average: 1.01
batch number: 9  add: 1255  replay_len: 189855/4194304
batch     0,  loss 0.088225, eval 12.453432
batches: 9,  total time: 2.46,  1k average: 2.46
batch number: 5  add: 705  replay_len: 190560/4194304
batch     0,  loss 0.094561, eval 12.564377
batches: 5,  total time: 1.66,  1k average: 1.66
batch number: 1  add: 135  replay_len: 190695/4194304
batch     0,  loss 0.074669, eval 12.542706
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 170  replay_len: 190865/4194304
batch     0,  loss 0.058401, eval 12.380376
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 4  add: 575  replay_len: 191440/4194304
batch     0,  loss 0.057430, eval 12.470517
batches: 4,  total time: 1.39,  1k average: 1.39
batch number: 8  add: 1045  replay_len: 192485/4194304
batch     0,  loss 0.102655, eval 12.409724
batches: 8,  total time: 2.76,  1k average: 2.76
batch number: 1  add: 255  replay_len: 192740/4194304
batch     0,  loss 0.070475, eval 12.390194
batches: 1,  total time: 0.29,  1k average: 0.29
batch number: 4  add: 540  replay_len: 193280/4194304
batch     0,  loss 0.087435, eval 12.409319
batches: 4,  total time: 1.06,  1k average: 1.06
batch number: 6  add: 820  replay_len: 194100/4194304
batch     0,  loss 0.060112, eval 12.290895
batches: 6,  total time: 1.76,  1k average: 1.76
batch number: 8  add: 1135  replay_len: 195235/4194304
batch     0,  loss 0.076187, eval 12.341858
batches: 8,  total time: 2.29,  1k average: 2.29
batch number: 6  add: 830  replay_len: 196065/4194304
batch     0,  loss 0.078387, eval 12.253202
batches: 6,  total time: 1.57,  1k average: 1.57
batch number: 4  add: 550  replay_len: 196615/4194304
batch     0,  loss 0.080226, eval 12.283674
batches: 4,  total time: 1.01,  1k average: 1.01
batch number: 1  add: 190  replay_len: 196915/4194304
batch     0,  loss 0.074006, eval 12.233753
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 5  add: 640  replay_len: 197555/4194304
batch     0,  loss 0.070129, eval 12.203176
batches: 5,  total time: 1.22,  1k average: 1.22
batch number: 3  add: 425  replay_len: 197980/4194304
batch     0,  loss 0.070873, eval 12.309960
batches: 3,  total time: 1.05,  1k average: 1.05
batch number: 2  add: 270  replay_len: 198250/4194304
batch     0,  loss 0.075871, eval 12.251171
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 140  replay_len: 198390/4194304
batch     0,  loss 0.055771, eval 12.153373
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 3  add: 425  replay_len: 198815/4194304
batch     0,  loss 0.071179, eval 12.108270
batches: 3,  total time: 0.89,  1k average: 0.89
batch number: 9  add: 1255  replay_len: 200070/4194304
batch     0,  loss 0.065850, eval 12.048227
batches: 9,  total time: 2.54,  1k average: 2.54
batch number: 1  add: 155  replay_len: 200225/4194304
batch     0,  loss 0.068303, eval 11.903543
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 1  add: 215  replay_len: 200440/4194304
batch     0,  loss 0.060774, eval 11.882647
batches: 1,  total time: 0.39,  1k average: 0.39
batch number: 1  add: 220  replay_len: 200660/4194304
batch     0,  loss 0.070535, eval 11.768072
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 180  replay_len: 200840/4194304
batch     0,  loss 0.073356, eval 11.738703
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 235  replay_len: 201075/4194304
batch     0,  loss 0.047471, eval 11.463963
batches: 1,  total time: 0.44,  1k average: 0.44
batch number: 1  add: 230  replay_len: 201305/4194304
batch     0,  loss 0.052144, eval 11.325689
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 7  add: 900  replay_len: 202205/4194304
batch     0,  loss 0.069050, eval 11.206102
batches: 7,  total time: 1.89,  1k average: 1.89
batch number: 6  add: 800  replay_len: 203005/4194304
batch     0,  loss 0.064252, eval 11.002436
batches: 6,  total time: 1.70,  1k average: 1.70
batch number: 9  add: 1255  replay_len: 204260/4194304
batch     0,  loss 0.063154, eval 11.040205
batches: 9,  total time: 2.50,  1k average: 2.50
batch number: 3  add: 500  replay_len: 204760/4194304round 283	 loss: [0.07, 0.17]	 reward: [0.6, -10.0]	 value: [11.0, 3.06]
round 284	 loss: [0.06, 0.13]	 reward: [6.4, -10.0]	 value: [10.98, 3.1]
round 285	 loss: [0.06, 0.16]	 reward: [-0.6, -10.0]	 value: [10.93, 3.08]
round 286	 loss: [0.07, 0]	 reward: [7.4, -10.0]	 value: [10.98, 0]
round 287	 loss: [0.05, 0]	 reward: [9.2, -10.0]	 value: [10.88, 0]
round 288	 loss: [0.06, 0.14]	 reward: [-19.0, -10.0]	 value: [10.85, 3.14]
round 289	 loss: [0.06, 0.16]	 reward: [7.0, -10.0]	 value: [10.89, 3.09]
round 290	 loss: [0.06, 0.15]	 reward: [1.2, -10.0]	 value: [10.8, 3.15]
round 291	 loss: [0.05, 0.16]	 reward: [-29.4, -10.0]	 value: [10.68, 3.06]
round 292	 loss: [0.05, 0.13]	 reward: [-16.0, -10.0]	 value: [10.67, 3.14]
round 293	 loss: [0.06, 0.16]	 reward: [-1.4, -10.0]	 value: [10.6, 3.15]
round 294	 loss: [0.06, 0.13]	 reward: [-9.4, -10.0]	 value: [10.59, 3.22]
round 295	 loss: [0.06, 0.16]	 reward: [6.2, -10.0]	 value: [10.61, 3.11]
round 296	 loss: [0.05, 0.15]	 reward: [4.6, -10.0]	 value: [10.48, 3.19]
round 297	 loss: [0.06, 0]	 reward: [8.6, -10.0]	 value: [10.46, 0]
round 298	 loss: [0.05, 0.17]	 reward: [0.2, -10.0]	 value: [10.18, 3.18]
round 299	 loss: [0.06, 0.15]	 reward: [-15.8, -10.0]	 value: [10.1, 3.16]

eps 0.14300000000000002 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 44,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.39  total time 828.80

===== sample =====
eps 0.14225 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 36,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.25
round time 0.37  total time 829.18

save model... 
===== sample =====
eps 0.14150000000000001 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 47,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.44
round time 0.63  total time 831.65

===== sample =====
eps 0.14075 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 46,  total time: 0.67,  step average 0.01
===== train =====
train_time 0.27
round time 0.94  total time 832.59

save model... 
===== sample =====
eps 0.14 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [  2.8 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -3.6 -10. ] 
step 150,  reward: [-0.2  0. ],  total_reward: [ -9.4 -10. ] 
steps: 180,  total time: 0.70,  step average 0.00
===== train =====
train_time 1.90
round time 2.60  total time 837.15

===== sample =====
eps 0.13924999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  1.6 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -3.2 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [ -9. -10.] 
steps: 160,  total time: 0.68,  step average 0.00
===== train =====
train_time 1.71
round time 2.39  total time 839.54

save model... 
===== sample =====
eps 0.1385 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 1.2 -9. ] 
step 100,  reward: [0. 0.],  total_reward: [ -5.8 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-10.6 -10. ] 
step 200,  reward: [-0.2  0. ],  total_reward: [-15.8 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-23.6 -10. ] 
steps: 251,  total time: 1.02,  step average 0.00
===== train =====
train_time 2.51
round time 3.53  total time 845.04

===== sample =====
eps 0.13774999999999998 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  2.4 -10. ] 
steps: 100,  total time: 0.36,  step average 0.00
===== train =====
train_time 0.78
round time 1.14  total time 846.18

save model... 
===== sample =====
eps 0.137 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  6. -10.] 
steps: 54,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.60
round time 0.79  total time 849.69

===== sample =====
eps 0.13624999999999998 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [ 1.8 -9. ] 
steps: 82,  total time: 0.30,  step average 0.00
===== train =====
train_time 0.94
round time 1.24  total time 850.93

save model... 
===== sample =====
eps 0.1355 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 28,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.32
round time 0.45  total time 853.34

===== sample =====
eps 0.13474999999999998 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 30,  total time: 0.16,  step average 0.01
===== train =====
train_time 0.30
round time 0.46  total time 853.80

save model... 
===== sample =====
eps 0.134 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  6. -10.] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -1. -10.] 
step 150,  reward: [-0.2  0. ],  total_reward: [ -6.8 -10. ] 
step 200,  reward: [-0.4  0. ],  total_reward: [-12.6 -10. ] 
step 250,  reward: [-0.2  0. ],  total_reward: [-19. -10.] 
steps: 251,  total time: 1.06,  step average 0.00
===== train =====
train_time 3.02
round time 4.08  total time 859.92

===== sample =====
eps 0.13324999999999998 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 41,  total time: 0.78,  step average 0.02
===== train =====
train_time 0.38
round time 1.16  total time 861.08

save model... 
===== sample =====
eps 0.1325 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 0.6 -9. ] 
steps: 56,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.61
round time 0.83  total time 864.02

===== sample =====
eps 0.13175 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [-0.6  0. ],  total_reward: [ -0.8 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -8.6 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [-15.2 -10. ] 
step 200,  reward: [0. 0.],  total_reward: [-22.2 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-29.4 -10. ] 
steps: 251,  total time: 1.04,  step average 0.00
===== train =====
train_time 3.33
round time 4.37  total time 868.39

save model... 
===== sample =====
eps 0.131 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  4.2 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -0.4 -10. ] 
step 150,  reward: [-0.4  0. ],  total_reward: [ -6. -10.] 
step 200,  reward: [0. 0.],  total_reward: [-11.8 -10. ] 
step 250,  reward: [0. 0.],  total_reward: [-16. -10.] 
steps: 251,  total time: 1.14,  step average 0.00
===== train =====
train_time 2.46
round time 3.60  total time 874.10

===== sample =====
eps 0.13025 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  3.6 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -1.4 -10. ] 
steps: 120,  total time: 0.48,  step average 0.00
===== train =====
train_time 1.17
round time 1.64  total time 875.74

save model... 
===== sample =====
eps 0.1295 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  3.6 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -2.2 -10. ] 
step 150,  reward: [0. 0.],  total_reward: [ -7.2 -10. ] 
steps: 189,  total time: 0.69,  step average 0.00
===== train =====
train_time 1.80
round time 2.49  total time 880.28

===== sample =====
eps 0.12875 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 44,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.35
round time 0.52  total time 880.81

save model... 
===== sample =====
eps 0.128 number [5, 5]
step   0,  reward: [ 1.2 -2. ],  total_reward: [ 1.2 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  4.6 -10. ] 
steps: 69,  total time: 0.37,  step average 0.01
===== train =====
train_time 0.59
round time 0.97  total time 883.78

===== sample =====
eps 0.12725 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 32,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.25
round time 0.37  total time 884.15

save model... 
===== sample =====
eps 0.1265 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [ 5. -9.] 
step 100,  reward: [-0.2  0. ],  total_reward: [ -0.6 -10. ] 
steps: 102,  total time: 0.45,  step average 0.00
===== train =====
train_time 0.80
round time 1.24  total time 887.25

===== sample =====
eps 0.12575 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  5.4 -10. ] 
step 100,  reward: [-0.6  0. ],  total_reward: [ -7.4 -10. ] 
steps: 144,  total time: 2.30,  step average 0.02
===== train =====
train_time 1.27
round time 3.57  total time 890.83

save model... 
===== sample =====
eps 0.125 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  4.8 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [ -1. -10.] 
step 150,  reward: [0. 0.],  total_reward: [ -7.2 -10. ] round 300	 loss: [0.05, 0.13]	 reward: [-12.4, -10.0]	 value: [10.04, 3.13]
round 301	 loss: [0.04, 0.13]	 reward: [6.4, -10.0]	 value: [10.07, 3.19]
round 302	 loss: [0.05, 0.12]	 reward: [5.2, -10.0]	 value: [9.93, 3.14]
round 303	 loss: [0.05, 0]	 reward: [7.8, -10.0]	 value: [9.95, 0]
round 304	 loss: [0.05, 0.14]	 reward: [6.6, -10.0]	 value: [9.78, 3.09]
round 305	 loss: [0.04, 0]	 reward: [8.2, -10.0]	 value: [9.72, 0]
round 306	 loss: [0.05, 0]	 reward: [9.0, -10.0]	 value: [9.73, 0]
round 307	 loss: [0.05, 0]	 reward: [7.8, -10.0]	 value: [9.73, 0]
round 308	 loss: [0, 0]	 reward: [9.6, -10.0]	 value: [0, 0]
round 309	 loss: [0, 0]	 reward: [9.4, -10.0]	 value: [0, 0]
round 310	 loss: [0.04, 0.17]	 reward: [7.2, -10.0]	 value: [9.62, 3.17]
round 311	 loss: [0.05, 0]	 reward: [7.0, -10.0]	 value: [9.45, 0]
round 312	 loss: [0.06, 0]	 reward: [4.8, -10.0]	 value: [9.49, 0]
round 313	 loss: [0.04, 0.17]	 reward: [3.8, -10.0]	 value: [9.28, 3.17]
round 314	 loss: [0.05, 0.16]	 reward: [7.4, -10.0]	 value: [9.28, 3.14]
round 315	 loss: [0.05, 0.16]	 reward: [8.4, -10.0]	 value: [9.18, 3.19]
round 316	 loss: [0.05, 0]	 reward: [2.8, -10.0]	 value: [9.08, 0]
round 317	 loss: [0.05, 0]	 reward: [5.8, -10.0]	 value: [9.02, 0]
round 318	 loss: [0.04, 0]	 reward: [9.6, -10.0]	 value: [8.91, 0]
round 319	 loss: [0.05, 0]	 reward: [7.6, -10.0]	 value: [8.86, 0]
round 320	 loss: [0.05, 0.16]	 reward: [-9.0, -10.0]	 value: [8.74, 3.17]
round 321	 loss: [0, 0]	 reward: [9.6, -10.0]	 value: [0, 0]
round 322	 loss: [0.05, 0.15]	 reward: [6.4, -10.0]	 value: [8.74, 3.16]
round 323	 loss: [0.05, 0.11]	 reward: [0.4, -10.0]	 value: [8.78, 3.18]
round 324	 loss: [0.05, 0]	 reward: [8.2, -10.0]	 value: [8.88, 0]
round 325	 loss: [0.04, 0.19]	 reward: [3.0, -10.0]	 value: [8.76, 3.08]
round 326	 loss: [0.05, 0]	 reward: [8.2, -10.0]	 value: [8.79, 0]
round 327	 loss: [0.05, 0.15]	 reward: [1.6, -10.0]	 value: [8.75, 3.09]
round 328	 loss: [0.05, 0.17]	 reward: [-4.8, -10.0]	 value: [8.71, 3.01]
round 329	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [8.72, 0]
round 330	 loss: [0, 0]	 reward: [9.6, -10.0]	 value: [0, 0]

step 200,  reward: [0. 0.],  total_reward: [-11.6 -10. ] 
steps: 225,  total time: 0.81,  step average 0.00
===== train =====
train_time 2.03
round time 2.85  total time 895.72

===== sample =====
eps 0.12425 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 49,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.36
round time 0.54  total time 896.26

save model... 
===== sample =====
eps 0.1235 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  4.8 -10. ] 
steps: 56,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.56
round time 0.78  total time 899.02

===== sample =====
eps 0.12275 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 33,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.24
round time 0.37  total time 899.38

save model... 
===== sample =====
eps 0.122 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  7.4 -10. ] 
steps: 71,  total time: 0.30,  step average 0.00
===== train =====
train_time 0.55
round time 0.85  total time 902.11

===== sample =====
eps 0.12125 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 37,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.26
round time 0.40  total time 902.51

save model... 
===== sample =====
eps 0.1205 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 39,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.24
round time 0.42  total time 905.29

===== sample =====
eps 0.11975 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 34,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.23
round time 0.38  total time 905.67

save model... 
===== sample =====
eps 0.119 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 23,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 907.89

===== sample =====
eps 0.11825 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 25,  total time: 0.32,  step average 0.01
===== train =====
train_time 0.00
round time 0.32  total time 908.22

save model... 
===== sample =====
eps 0.1175 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [1.6 0. ],  total_reward: [  7.2 -10. ] 
steps: 51,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.36
round time 0.58  total time 911.09

===== sample =====
eps 0.11674999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  6.4 -10. ] 
steps: 57,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.47
round time 0.74  total time 911.83

save model... 
===== sample =====
eps 0.11599999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 50,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.24
round time 0.42  total time 914.28

===== sample =====
eps 0.11524999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  3.4 -10. ] 
steps: 52,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.66
round time 0.84  total time 915.12

save model... 
===== sample =====
eps 0.11449999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 41,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.35
round time 0.50  total time 917.83

===== sample =====
eps 0.11374999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 37,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.34
round time 0.47  total time 918.29

save model... 
===== sample =====
eps 0.113 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  4.2 -10. ] 
steps: 71,  total time: 0.26,  step average 0.00
===== train =====
train_time 0.50
round time 0.76  total time 921.16

===== sample =====
eps 0.11225 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 38,  total time: 0.20,  step average 0.01
===== train =====
train_time 0.28
round time 0.48  total time 921.64

save model... 
===== sample =====
eps 0.1115 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 27,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.24
round time 0.34  total time 924.18

===== sample =====
eps 0.11075 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 26,  total time: 0.43,  step average 0.02
===== train =====
train_time 0.24
round time 0.68  total time 924.85

save model... 
===== sample =====
eps 0.11 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [-0.4 -8. ] 
step 100,  reward: [0.8 0. ],  total_reward: [ -6.8 -10. ] 
steps: 142,  total time: 0.71,  step average 0.01
===== train =====
train_time 1.48
round time 2.20  total time 929.29

===== sample =====
eps 0.10925 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 21,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.00
round time 0.10  total time 929.39

save model... 
===== sample =====
eps 0.1085 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  6.2 -10. ] 
steps: 67,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.85
round time 1.10  total time 932.78

===== sample =====
eps 0.10775 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  7.4 -10. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [  3.8 -10. ] 
steps: 150,  total time: 0.90,  step average 0.01
===== train =====
train_time 1.29
round time 2.19  total time 934.98

save model... 
===== sample =====
eps 0.107 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 43,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.26
round time 0.42  total time 937.43

===== sample =====
eps 0.10625 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [ 3.8 -9. ] 
step 100,  reward: [-0.2  0. ],  total_reward: [  2.6 -10. ] 
steps: 108,  total time: 0.39,  step average 0.00
===== train =====
train_time 1.23
round time 1.61  total time 939.05

save model... 
===== sample =====
eps 0.1055 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 35,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.24
round time 0.37  total time 941.55

===== sample =====
eps 0.10475 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0.4 0. ],  total_reward: [  1.4 -10. ] 
steps: 66,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.57
round time 0.81  total time 942.36

save model... 
===== sample =====
eps 0.104 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  4. -10.] 
step 100,  reward: [0. 0.],  total_reward: [ -1. -10.] 
step 150,  reward: [-0.4  0. ],  total_reward: [ -4.4 -10. ] 
steps: 166,  total time: 0.77,  step average 0.00
===== train =====
train_time 1.65
round time 2.41  total time 947.13

===== sample =====
eps 0.10325 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 29,  total time: 0.38,  step average 0.01
===== train =====
train_time 0.24
round time 0.62  total time 947.75

save model... 
===== sample =====
eps 0.1025 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 24,  total time: 0.14,  step average 0.01
===== train =====
train_time 0.00
round time 0.14  total time 950.50

===== sample =====
eps 0.10175 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] round 331	 loss: [0.05, 0.14]	 reward: [5.2, -10.0]	 value: [8.71, 3.1]
round 332	 loss: [0.04, 0.14]	 reward: [2.2, -10.0]	 value: [8.64, 3.06]
round 333	 loss: [0.05, 0.13]	 reward: [5.2, -10.0]	 value: [8.61, 3.0]
round 334	 loss: [0.04, 0.14]	 reward: [6.4, -10.0]	 value: [8.52, 3.07]
round 335	 loss: [0.05, 0.13]	 reward: [1.0, -10.0]	 value: [8.52, 3.09]
round 336	 loss: [0.04, 0]	 reward: [7.6, -10.0]	 value: [8.53, 0]
round 337	 loss: [0.04, 0.15]	 reward: [9.0, -10.0]	 value: [8.49, 3.12]
round 338	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]
round 339	 loss: [0.04, 0]	 reward: [8.6, -10.0]	 value: [8.53, 0]
round 340	 loss: [0.04, 0.16]	 reward: [5.8, -10.0]	 value: [8.51, 2.99]
round 341	 loss: [0.04, 0]	 reward: [7.4, -10.0]	 value: [8.54, 0]
round 342	 loss: [0.04, 0.15]	 reward: [8.4, -10.0]	 value: [8.43, 2.95]
round 343	 loss: [0.04, 0]	 reward: [9.0, -10.0]	 value: [8.42, 0]

batch     0,  loss 0.083255, eval 11.013081
batches: 3,  total time: 0.78,  1k average: 0.78
batch number: 2  add: 270  replay_len: 205030/4194304
batch     0,  loss 0.053008, eval 11.062313
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 3  add: 410  replay_len: 205440/4194304
batch     0,  loss 0.054931, eval 10.983069
batches: 3,  total time: 0.94,  1k average: 0.94
batch number: 1  add: 140  replay_len: 205580/4194304
batch     0,  loss 0.065059, eval 10.976021
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 150  replay_len: 205730/4194304
batch     0,  loss 0.052438, eval 10.877268
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 9  add: 1255  replay_len: 206985/4194304
batch     0,  loss 0.049163, eval 10.866652
batches: 9,  total time: 2.99,  1k average: 2.99
batch number: 1  add: 205  replay_len: 207190/4194304
batch     0,  loss 0.055908, eval 10.892467
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 2  add: 280  replay_len: 207470/4194304
batch     0,  loss 0.061580, eval 10.787558
batches: 2,  total time: 0.61,  1k average: 0.61
batch number: 9  add: 1255  replay_len: 208725/4194304
batch     0,  loss 0.058881, eval 10.745102
batches: 9,  total time: 3.32,  1k average: 3.32
batch number: 9  add: 1255  replay_len: 209980/4194304
batch     0,  loss 0.046761, eval 10.745364
batches: 9,  total time: 2.45,  1k average: 2.45
batch number: 4  add: 600  replay_len: 210580/4194304
batch     0,  loss 0.054432, eval 10.714672
batches: 4,  total time: 1.16,  1k average: 1.16
batch number: 7  add: 945  replay_len: 211525/4194304
batch     0,  loss 0.067291, eval 10.688881
batches: 7,  total time: 1.78,  1k average: 1.78
batch number: 1  add: 220  replay_len: 211745/4194304
batch     0,  loss 0.058166, eval 10.607376
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 2  add: 345  replay_len: 212090/4194304
batch     0,  loss 0.051776, eval 10.508788
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 160  replay_len: 212250/4194304
batch     0,  loss 0.063763, eval 10.457962
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 3  add: 510  replay_len: 212760/4194304
batch     0,  loss 0.059283, eval 10.306243
batches: 3,  total time: 0.79,  1k average: 0.79
batch number: 5  add: 720  replay_len: 213480/4194304
batch     0,  loss 0.072119, eval 10.237884
batches: 5,  total time: 1.26,  1k average: 1.26
batch number: 8  add: 1125  replay_len: 214605/4194304
batch     0,  loss 0.048017, eval 10.083178
batches: 8,  total time: 2.02,  1k average: 2.02
batch number: 1  add: 245  replay_len: 214850/4194304
batch     0,  loss 0.043814, eval 10.071453
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 280  replay_len: 215130/4194304
batch     0,  loss 0.050715, eval 10.008710
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 165  replay_len: 215295/4194304
batch     0,  loss 0.048680, eval 9.949637
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 355  replay_len: 215650/4194304
batch     0,  loss 0.055646, eval 9.843532
batches: 2,  total time: 0.55,  1k average: 0.55
batch number: 1  add: 185  replay_len: 215835/4194304
batch     0,  loss 0.038266, eval 9.718427
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 1  add: 195  replay_len: 216030/4194304
batch     0,  loss 0.051006, eval 9.728026
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 170  replay_len: 216200/4194304
batch     0,  loss 0.049462, eval 9.727295
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 255  replay_len: 216695/4194304
batch     0,  loss 0.044597, eval 9.624664
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 285  replay_len: 216980/4194304
batch     0,  loss 0.045764, eval 9.542461
batches: 2,  total time: 0.47,  1k average: 0.47
batch number: 1  add: 250  replay_len: 217230/4194304
batch     0,  loss 0.058033, eval 9.490765
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 260  replay_len: 217490/4194304
batch     0,  loss 0.035151, eval 9.357347
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 1  add: 205  replay_len: 217695/4194304
batch     0,  loss 0.053602, eval 9.277431
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 185  replay_len: 217880/4194304
batch     0,  loss 0.047335, eval 9.177073
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 355  replay_len: 218235/4194304
batch     0,  loss 0.051187, eval 9.151814
batches: 2,  total time: 0.49,  1k average: 0.49
batch number: 1  add: 190  replay_len: 218425/4194304
batch     0,  loss 0.052514, eval 9.019232
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 135  replay_len: 218560/4194304
batch     0,  loss 0.041504, eval 8.907284
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 130  replay_len: 218690/4194304
batch     0,  loss 0.049922, eval 8.855265
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 5  add: 710  replay_len: 219400/4194304
batch     0,  loss 0.043760, eval 8.783444
batches: 5,  total time: 1.48,  1k average: 1.48
batch number: 2  add: 335  replay_len: 219840/4194304
batch     0,  loss 0.044874, eval 8.683064
batches: 2,  total time: 0.85,  1k average: 0.85
batch number: 5  add: 750  replay_len: 220590/4194304
batch     0,  loss 0.057851, eval 8.740147
batches: 5,  total time: 1.28,  1k average: 1.28
batch number: 1  add: 215  replay_len: 220805/4194304
batch     0,  loss 0.047069, eval 8.880938
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 4  add: 540  replay_len: 221345/4194304
batch     0,  loss 0.045177, eval 8.790784
batches: 4,  total time: 1.22,  1k average: 1.22
batch number: 1  add: 175  replay_len: 221520/4194304
batch     0,  loss 0.046445, eval 8.791595
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 330  replay_len: 221850/4194304
batch     0,  loss 0.044973, eval 8.701601
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 6  add: 830  replay_len: 222680/4194304
batch     0,  loss 0.040427, eval 8.695253
batches: 6,  total time: 1.64,  1k average: 1.64
batch number: 1  add: 145  replay_len: 222825/4194304
batch     0,  loss 0.039403, eval 8.724565
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 345  replay_len: 223290/4194304
batch     0,  loss 0.041120, eval 8.722976
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 3  add: 500  replay_len: 223790/4194304
batch     0,  loss 0.039736, eval 8.677760
batches: 3,  total time: 0.91,  1k average: 0.91
batch number: 2  add: 360  replay_len: 224150/4194304
batch     0,  loss 0.050999, eval 8.661633
batches: 2,  total time: 0.72,  1k average: 0.72
batch number: 2  add: 330  replay_len: 224480/4194304
batch     0,  loss 0.041856, eval 8.570099
batches: 2,  total time: 0.78,  1k average: 0.78
batch number: 5  add: 655  replay_len: 225135/4194304
batch     0,  loss 0.045659, eval 8.598214
batches: 5,  total time: 1.69,  1k average: 1.69
batch number: 1  add: 250  replay_len: 225385/4194304
batch     0,  loss 0.041019, eval 8.534727
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 190  replay_len: 225575/4194304
batch     0,  loss 0.040489, eval 8.485724
batches: 1,  total time: 0.41,  1k average: 0.41
batch number: 1  add: 170  replay_len: 225865/4194304
batch     0,  loss 0.044291, eval 8.531040
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 2  add: 360  replay_len: 226225/4194304
batch     0,  loss 0.039412, eval 8.515722
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 250  replay_len: 226475/4194304
batch     0,  loss 0.040755, eval 8.536205
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 2  add: 270  replay_len: 226745/4194304
batch     0,  loss 0.039678, eval 8.477873
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 1  add: 165  replay_len: 226910/4194304
batch     0,  loss 0.037661, eval 8.416052
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 265  replay_len: 227175/4194304round 344	 loss: [0.04, 0.14]	 reward: [7.6, -10.0]	 value: [8.35, 3.03]
round 345	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]
round 346	 loss: [0, 0]	 reward: [10.4, -10.0]	 value: [0, 0]
round 347	 loss: [0.04, 0.14]	 reward: [7.4, -10.0]	 value: [8.33, 3.06]
round 348	 loss: [0.04, 0.14]	 reward: [8.4, -10.0]	 value: [8.28, 3.02]
round 349	 loss: [0.05, 0.17]	 reward: [7.6, -10.0]	 value: [8.22, 3.02]
round 350	 loss: [0.04, 0]	 reward: [7.6, -10.0]	 value: [8.17, 0]
round 351	 loss: [0.03, 0]	 reward: [7.2, -10.0]	 value: [8.07, 0]
round 352	 loss: [0.04, 0]	 reward: [6.0, -10.0]	 value: [7.98, 0]
round 353	 loss: [0.05, 0.15]	 reward: [3.8, -10.0]	 value: [7.87, 3.1]

batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 3  add: 387  replay_len: 104347/4194304
batch     0,  loss 0.097516, eval 3.224123
batches: 3,  total time: 0.89,  1k average: 0.89
batch number: 2  add: 381  replay_len: 104728/4194304
batch     0,  loss 0.079556, eval 3.236497
batches: 2,  total time: 0.70,  1k average: 0.70
batch number: 2  add: 259  replay_len: 104987/4194304
batch     0,  loss 0.162547, eval 3.100743
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 223  replay_len: 105210/4194304
batch     0,  loss 0.120337, eval 3.260227
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 246  replay_len: 105644/4194304
batch     0,  loss 0.128072, eval 3.208744
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 194  replay_len: 105838/4194304
batch     0,  loss 0.203117, eval 3.164723
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 135  replay_len: 105973/4194304
batch     0,  loss 0.140319, eval 3.226125
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 227  replay_len: 106307/4194304
batch     0,  loss 0.144854, eval 3.167205
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 321  replay_len: 106628/4194304
batch     0,  loss 0.110183, eval 3.189307
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 135  replay_len: 106861/4194304
batch     0,  loss 0.122270, eval 3.166430
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 139  replay_len: 107209/4194304
batch     0,  loss 0.136568, eval 3.157806
batches: 1,  total time: 0.41,  1k average: 0.41
batch number: 2  add: 291  replay_len: 107613/4194304
batch     0,  loss 0.125203, eval 3.141109
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 2  add: 367  replay_len: 107980/4194304
batch     0,  loss 0.122842, eval 3.076226
batches: 2,  total time: 0.67,  1k average: 0.67
batch number: 3  add: 511  replay_len: 108491/4194304
batch     0,  loss 0.149790, eval 3.066504
batches: 3,  total time: 0.96,  1k average: 0.96
batch number: 1  add: 250  replay_len: 108741/4194304
batch     0,  loss 0.169207, eval 3.055026
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 128  replay_len: 108869/4194304
batch     0,  loss 0.134497, eval 3.097567
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 267  replay_len: 109136/4194304
batch     0,  loss 0.168466, eval 3.048604
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 2  add: 344  replay_len: 109681/4194304
batch     0,  loss 0.144219, eval 3.143885
batches: 2,  total time: 0.76,  1k average: 0.76
batch number: 1  add: 132  replay_len: 109813/4194304
batch     0,  loss 0.155930, eval 3.085162
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 149  replay_len: 109962/4194304
batch     0,  loss 0.147328, eval 3.145228
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 3  add: 469  replay_len: 110431/4194304
batch     0,  loss 0.149959, eval 3.090220
batches: 3,  total time: 1.33,  1k average: 1.33
batch number: 3  add: 486  replay_len: 110917/4194304
batch     0,  loss 0.130329, eval 3.176517
batches: 3,  total time: 1.00,  1k average: 1.00
batch number: 2  add: 358  replay_len: 111275/4194304
batch     0,  loss 0.119334, eval 3.212137
batches: 2,  total time: 0.52,  1k average: 0.52
batch number: 2  add: 305  replay_len: 111580/4194304
batch     0,  loss 0.129105, eval 3.183110
batches: 2,  total time: 0.54,  1k average: 0.54
batch number: 1  add: 170  replay_len: 111750/4194304
batch     0,  loss 0.160769, eval 3.110257
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 185  replay_len: 111935/4194304
batch     0,  loss 0.151986, eval 3.191998
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 148  replay_len: 112198/4194304
batch     0,  loss 0.172077, eval 3.176027
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 204  replay_len: 112402/4194304
batch     0,  loss 0.145823, eval 3.155756
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 2  add: 348  replay_len: 112750/4194304
batch     0,  loss 0.128154, eval 3.142088
batches: 2,  total time: 0.64,  1k average: 0.64
batch number: 1  add: 152  replay_len: 112902/4194304
batch     0,  loss 0.132564, eval 3.193707
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 132  replay_len: 113034/4194304
batch     0,  loss 0.116245, eval 3.138015
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 160  replay_len: 113288/4194304
batch     0,  loss 0.140737, eval 3.094244
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 165  replay_len: 113932/4194304
batch     0,  loss 0.169127, eval 3.165917
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 155  replay_len: 114317/4194304
batch     0,  loss 0.173955, eval 3.173299
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 133  replay_len: 114450/4194304
batch     0,  loss 0.162417, eval 3.141088
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 128  replay_len: 114578/4194304
batch     0,  loss 0.164286, eval 3.187394
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 2  add: 295  replay_len: 115268/4194304
batch     0,  loss 0.136861, eval 3.165230
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 1  add: 166  replay_len: 115502/4194304
batch     0,  loss 0.146369, eval 3.155468
batches: 1,  total time: 0.40,  1k average: 0.40
batch number: 1  add: 235  replay_len: 115737/4194304
batch     0,  loss 0.109470, eval 3.178401
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 2  add: 285  replay_len: 116130/4194304
batch     0,  loss 0.179005, eval 3.134224
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 1  add: 183  replay_len: 116400/4194304
batch     0,  loss 0.145686, eval 3.092854
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 2  add: 338  replay_len: 116738/4194304
batch     0,  loss 0.123595, eval 3.106156
batches: 2,  total time: 0.66,  1k average: 0.66
batch number: 1  add: 214  replay_len: 117149/4194304
batch     0,  loss 0.137020, eval 3.098645
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 2  add: 285  replay_len: 117434/4194304
batch     0,  loss 0.141694, eval 3.128883
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 188  replay_len: 117622/4194304
batch     0,  loss 0.130718, eval 3.002900
batches: 1,  total time: 0.40,  1k average: 0.40
batch number: 1  add: 151  replay_len: 117773/4194304
batch     0,  loss 0.138942, eval 3.070683
batches: 1,  total time: 0.52,  1k average: 0.52
batch number: 1  add: 200  replay_len: 117973/4194304
batch     0,  loss 0.134413, eval 3.091864
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 142  replay_len: 118223/4194304
batch     0,  loss 0.152991, eval 3.116244
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 1  add: 164  replay_len: 118566/4194304
batch     0,  loss 0.158058, eval 2.990591
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 148  replay_len: 118835/4194304
batch     0,  loss 0.152482, eval 2.947796
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 145  replay_len: 119089/4194304
batch     0,  loss 0.142606, eval 3.030197
batches: 1,  total time: 0.43,  1k average: 0.43
batch number: 1  add: 144  replay_len: 119373/4194304
batch     0,  loss 0.138101, eval 3.063046
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 138  replay_len: 119511/4194304
batch     0,  loss 0.142582, eval 3.020822
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 161  replay_len: 119672/4194304
batch     0,  loss 0.166990, eval 3.016090
batches: 1,  total time: 0.42,  1k average: 0.42
batch number: 1  add: 140  replay_len: 120146/4194304
batch     0,  loss 0.154153, eval 3.100256
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 135  replay_len: 120281/4194304
batch     0,  loss 0.147793, eval 3.058616
batches: 1,  total time: 0.37,  1k average: 0.37round 354	 loss: [0.04, 0.15]	 reward: [6.0, -10.0]	 value: [7.78, 3.06]
round 355	 loss: [0.04, 0.16]	 reward: [4.2, -10.0]	 value: [7.86, 3.1]
round 356	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [7.77, 0]
round 357	 loss: [0.04, 0]	 reward: [9.4, -10.0]	 value: [7.77, 0]
round 358	 loss: [0.04, 0.15]	 reward: [7.0, -10.0]	 value: [7.69, 3.14]
round 359	 loss: [0.04, 0.17]	 reward: [7.8, -10.0]	 value: [7.69, 3.12]
round 360	 loss: [0.04, 0.17]	 reward: [6.0, -10.0]	 value: [7.64, 3.15]

step  50,  reward: [-0.4  0. ],  total_reward: [ 4. -9.] 
steps: 69,  total time: 0.38,  step average 0.01
===== train =====
train_time 0.60
round time 0.98  total time 951.48

save model... 
===== sample =====
eps 0.10099999999999999 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [ 0.6 -1. ],  total_reward: [ 2.2 -8. ] 
steps: 100,  total time: 0.39,  step average 0.00
===== train =====
train_time 0.91
round time 1.31  total time 955.13

===== sample =====
eps 0.10024999999999999 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  6.2 -10. ] 
steps: 72,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.72
round time 1.00  total time 956.12

save model... 
===== sample =====
eps 0.09949999999999999 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
step  50,  reward: [0. 0.],  total_reward: [  6.4 -10. ] 
steps: 66,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.79
round time 1.04  total time 960.08

===== sample =====
eps 0.09874999999999999 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  7. -10.] 
step 100,  reward: [-0.2  0. ],  total_reward: [  2.6 -10. ] 
steps: 131,  total time: 0.60,  step average 0.00
===== train =====
train_time 1.70
round time 2.30  total time 962.39

save model... 
===== sample =====
eps 0.09799999999999999 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 50,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.25
round time 0.47  total time 965.51

===== sample =====
eps 0.09724999999999999 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 38,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.42
round time 0.56  total time 966.07

save model... 
===== sample =====
eps 0.09649999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 24,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.00
round time 0.18  total time 968.55

===== sample =====
eps 0.09574999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 34,  total time: 0.47,  step average 0.01
===== train =====
train_time 0.27
round time 0.75  total time 969.30

save model... 
===== sample =====
eps 0.09499999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  5.8 -10. ] 
steps: 72,  total time: 0.26,  step average 0.00
===== train =====
train_time 0.57
round time 0.83  total time 972.65

===== sample =====
eps 0.09425 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 50,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.22
round time 0.45  total time 973.10

save model... 
===== sample =====
eps 0.0935 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  8.2 -10. ] 
steps: 54,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.60
round time 0.85  total time 976.38

===== sample =====
eps 0.09275 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 33,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.23
round time 0.36  total time 976.74

save model... 
===== sample =====
eps 0.092 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  6.8 -10. ] 
steps: 53,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.67
round time 0.88  total time 979.91

===== sample =====
eps 0.09125 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
steps: 24,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.10  total time 980.01

save model... 
===== sample =====
eps 0.0905 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 18,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.08  total time 982.57

===== sample =====
eps 0.08975 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  7. -10.] 
steps: 53,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.56
round time 0.77  total time 983.34

save model... 
===== sample =====
eps 0.089 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 43,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.37
round time 0.54  total time 986.07

===== sample =====
eps 0.08825 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  7. -10.] 
steps: 53,  total time: 0.83,  step average 0.02
===== train =====
train_time 0.69
round time 1.52  total time 987.59

save model... 
===== sample =====
eps 0.0875 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 44,  total time: 0.23,  step average 0.01
===== train =====
train_time 0.25
round time 0.48  total time 990.68

===== sample =====
eps 0.08675 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 49,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.25
round time 0.43  total time 991.11

save model... 
===== sample =====
eps 0.086 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  5.8 -10. ] 
steps: 62,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.67
round time 0.91  total time 994.32

===== sample =====
eps 0.08524999999999999 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  5.8 -10. ] 
steps: 79,  total time: 0.65,  step average 0.01
===== train =====
train_time 0.86
round time 1.51  total time 995.84

save model... 
===== sample =====
eps 0.08449999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  5.2 -10. ] 
steps: 53,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.68
round time 0.90  total time 999.12

===== sample =====
eps 0.08374999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  4.6 -10. ] 
steps: 75,  total time: 0.52,  step average 0.01
===== train =====
train_time 0.61
round time 1.13  total time 1000.24

save model... 
===== sample =====
eps 0.08299999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 32,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 1002.92

===== sample =====
eps 0.08224999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 29,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 1003.26

save model... 
===== sample =====
eps 0.08149999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  8.2 -10. ] 
steps: 70,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.56
round time 0.81  total time 1006.38

===== sample =====
eps 0.08074999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.2 -10. ] 
steps: 52,  total time: 0.66,  step average 0.01
===== train =====
train_time 0.70
round time 1.36  total time 1007.74

save model... 
===== sample =====
eps 0.07999999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  8.2 -10. ] 
steps: 90,  total time: 0.33,  step average 0.00
===== train =====
train_time 0.77
round time 1.09  total time 1011.18

===== sample =====
eps 0.07924999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 31,  total time: 0.11,  step average 0.00round 361	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [7.64, 0]
round 362	 loss: [0.04, 0.13]	 reward: [5.8, -10.0]	 value: [7.6, 3.09]
round 363	 loss: [0.04, 0.15]	 reward: [7.4, -10.0]	 value: [7.57, 3.13]
round 364	 loss: [0.04, 0.14]	 reward: [7.8, -10.0]	 value: [7.52, 3.17]
round 365	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [7.52, 0]
round 366	 loss: [0.03, 0.18]	 reward: [9.8, -10.0]	 value: [7.52, 3.13]
round 367	 loss: [0.03, 0]	 reward: [6.8, -10.0]	 value: [7.45, 0]
round 368	 loss: [0.04, 0]	 reward: [9.8, -10.0]	 value: [7.46, 0]
round 369	 loss: [0.03, 0]	 reward: [10.2, -10.0]	 value: [7.4, 0]
round 370	 loss: [0.04, 0]	 reward: [9.4, -10.0]	 value: [7.36, 0]
round 371	 loss: [0.03, 0.2]	 reward: [6.6, -10.0]	 value: [7.34, 3.2]
round 372	 loss: [0.03, 0.16]	 reward: [-0.8, -10.0]	 value: [7.17, 3.15]
round 373	 loss: [0.04, 0.13]	 reward: [5.6, -10.0]	 value: [7.15, 3.19]
round 374	 loss: [0.04, 0.18]	 reward: [3.8, -10.0]	 value: [7.14, 3.21]
round 375	 loss: [0.06, 0]	 reward: [9.4, -10.0]	 value: [7.16, 0]
round 376	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [7.1, 0]
round 377	 loss: [0.04, 0.16]	 reward: [4.6, -10.0]	 value: [7.06, 3.17]
round 378	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]
round 379	 loss: [0.03, 0]	 reward: [8.0, -10.0]	 value: [7.07, 0]
round 380	 loss: [0.03, 0]	 reward: [9.6, -10.0]	 value: [6.99, 0]
round 381	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [6.99, 0]
round 382	 loss: [0.04, 0]	 reward: [9.0, -10.0]	 value: [7.0, 0]
round 383	 loss: [0, 0]	 reward: [10.6, -10.0]	 value: [0, 0]
round 384	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [7.02, 0]
round 385	 loss: [0.03, 0]	 reward: [11.0, -10.0]	 value: [6.99, 0]
round 386	 loss: [0.04, 0]	 reward: [11.0, -10.0]	 value: [6.91, 0]
round 387	 loss: [0.03, 0]	 reward: [10.8, -10.0]	 value: [6.89, 0]
round 388	 loss: [0.04, 0]	 reward: [9.8, -10.0]	 value: [6.88, 0]
round 389	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [6.76, 0]
round 390	 loss: [0.03, 0]	 reward: [10.6, -10.0]	 value: [6.76, 0]
round 391	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]
round 392	 loss: [0.04, 0.21]	 reward: [6.8, -10.0]	 value: [6.74, 3.23]
round 393	 loss: [0, 0]	 reward: [9.8, -10.0]	 value: [0, 0]

===== train =====
train_time 0.23
round time 0.34  total time 1011.52

save model... 
===== sample =====
eps 0.07849999999999999 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  5.6 -10. ] 
steps: 64,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.58
round time 0.82  total time 1014.75

===== sample =====
eps 0.07774999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.4 -10. ] 
steps: 58,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.65
round time 0.85  total time 1015.60

save model... 
===== sample =====
eps 0.07699999999999999 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  8. -10.] 
steps: 64,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.59
round time 0.83  total time 1018.80

===== sample =====
eps 0.07624999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 30,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 1019.14

save model... 
===== sample =====
eps 0.07549999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 33,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.35
round time 0.48  total time 1022.16

===== sample =====
eps 0.07474999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 46,  total time: 0.17,  step average 0.00
===== train =====
train_time 0.27
round time 0.44  total time 1022.61

save model... 
===== sample =====
eps 0.07399999999999998 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 38,  total time: 0.25,  step average 0.01
===== train =====
train_time 0.24
round time 0.49  total time 1025.84

===== sample =====
eps 0.07324999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 35,  total time: 0.45,  step average 0.01
===== train =====
train_time 0.24
round time 0.69  total time 1026.53

save model... 
===== sample =====
eps 0.07249999999999998 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 36,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.24
round time 0.38  total time 1029.41

===== sample =====
eps 0.07174999999999998 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 39,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.38
round time 0.53  total time 1029.94

save model... 
===== sample =====
eps 0.07099999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  5.6 -10. ] 
steps: 84,  total time: 0.31,  step average 0.00
===== train =====
train_time 0.84
round time 1.15  total time 1033.55

===== sample =====
eps 0.07024999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  6. -10.] 
steps: 67,  total time: 0.27,  step average 0.00
===== train =====
train_time 0.57
round time 0.84  total time 1034.39

save model... 
===== sample =====
eps 0.06949999999999998 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 48,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.36
round time 0.57  total time 1037.40

===== sample =====
eps 0.06874999999999998 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 39,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.28
round time 0.44  total time 1037.84

save model... 
===== sample =====
eps 0.06799999999999998 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 41,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.39  total time 1040.83

===== sample =====
eps 0.06724999999999998 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  7. -10.] 
steps: 90,  total time: 0.34,  step average 0.00
===== train =====
train_time 0.96
round time 1.29  total time 1042.12

save model... 
===== sample =====
eps 0.06649999999999998 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 24,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.10  total time 1044.77

===== sample =====
eps 0.06574999999999998 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 40,  total time: 0.53,  step average 0.01
===== train =====
train_time 0.26
round time 0.79  total time 1045.56

save model... 
===== sample =====
eps 0.065 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 31,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.24
round time 0.42  total time 1048.80

===== sample =====
eps 0.06425 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 38,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.24
round time 0.40  total time 1049.21

save model... 
===== sample =====
eps 0.0635 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 38,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.23
round time 0.37  total time 1052.00

===== sample =====
eps 0.06275 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 22,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.08  total time 1052.08

save model... 
===== sample =====
eps 0.062 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 33,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 1055.02

===== sample =====
eps 0.06125 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 31,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 1055.36

save model... 
===== sample =====
eps 0.0605 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 27,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.24
round time 0.35  total time 1058.07

===== sample =====
eps 0.05975 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 36,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.23
round time 0.36  total time 1058.43

save model... 
===== sample =====
eps 0.059 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 38,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.23
round time 0.39  total time 1061.41

===== sample =====
eps 0.058249999999999996 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  9. -10.] 
steps: 53,  total time: 0.76,  step average 0.01
===== train =====
train_time 0.58
round time 1.34  total time 1062.75

save model... 
===== sample =====
eps 0.057499999999999996 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 26,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.24
round time 0.36  total time 1066.15

===== sample =====
eps 0.056749999999999995 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 24,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 1066.24

save model... 
===== sample =====
eps 0.055999999999999994 number [5, 5]
step   0,  reward: [ 2.2 -3. ],  total_reward: [ 2.2 -3. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.6 -10. ] 
step 100,  reward: [0. 0.],  total_reward: [  6. -10.] 
steps: 103,  total time: 0.37,  step average 0.00
===== train =====
train_time 1.13
round time 1.51  total time 1070.12

===== sample =====
eps 0.055249999999999994 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 23,  total time: 0.13,  step average 0.01
===== train =====
train_time 0.00
round time 0.13  total time 1070.24
round 394	 loss: [0, 0]	 reward: [11.0, -10.0]	 value: [0, 0]
round 395	 loss: [0, 0]	 reward: [11.8, -10.0]	 value: [0, 0]
round 396	 loss: [0.03, 0]	 reward: [8.6, -10.0]	 value: [6.67, 0]
round 397	 loss: [0.04, 0]	 reward: [8.6, -10.0]	 value: [6.72, 0]
round 398	 loss: [0.03, 0.21]	 reward: [9.4, -10.0]	 value: [6.7, 3.23]
round 399	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [6.69, 0]
round 400	 loss: [0, 0]	 reward: [10.4, -10.0]	 value: [0, 0]
round 401	 loss: [0.04, 0]	 reward: [8.2, -10.0]	 value: [6.77, 0]
round 402	 loss: [0.04, 0]	 reward: [9.4, -10.0]	 value: [6.7, 0]
round 403	 loss: [0.04, 0]	 reward: [11.0, -10.0]	 value: [6.7, 0]
round 404	 loss: [0.04, 0.21]	 reward: [10.2, -10.0]	 value: [6.68, 3.24]
round 405	 loss: [0, 0]	 reward: [10.6, -10.0]	 value: [0, 0]
round 406	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [6.67, 0]
round 407	 loss: [0.04, 0]	 reward: [10.0, -10.0]	 value: [6.56, 0]
round 408	 loss: [0.04, 0]	 reward: [10.8, -10.0]	 value: [6.58, 0]
round 409	 loss: [0.04, 0]	 reward: [10.4, -10.0]	 value: [6.54, 0]
round 410	 loss: [0, 0]	 reward: [11.2, -10.0]	 value: [0, 0]
round 411	 loss: [0, 0]	 reward: [10.8, -10.0]	 value: [0, 0]
round 412	 loss: [0, 0]	 reward: [10.8, -10.0]	 value: [0, 0]

batch     0,  loss 0.043766, eval 8.378153
batches: 2,  total time: 0.67,  1k average: 0.67
batch number: 2  add: 265  replay_len: 227650/4194304
batch     0,  loss 0.043281, eval 8.360027
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 215  replay_len: 227865/4194304
batch     0,  loss 0.037762, eval 8.282663
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 265  replay_len: 228130/4194304
batch     0,  loss 0.045341, eval 8.202059
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 1  add: 220  replay_len: 228350/4194304
batch     0,  loss 0.040643, eval 8.172863
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 245  replay_len: 228595/4194304
batch     0,  loss 0.034320, eval 8.069925
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 2  add: 310  replay_len: 228905/4194304
batch     0,  loss 0.041020, eval 7.994762
batches: 2,  total time: 0.67,  1k average: 0.67
batch number: 3  add: 395  replay_len: 229300/4194304
batch     0,  loss 0.042379, eval 7.940173
batches: 3,  total time: 0.86,  1k average: 0.86
batch number: 2  add: 265  replay_len: 229565/4194304
batch     0,  loss 0.046010, eval 7.887833
batches: 2,  total time: 0.68,  1k average: 0.68
batch number: 2  add: 375  replay_len: 229940/4194304
batch     0,  loss 0.036917, eval 7.807532
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 1  add: 160  replay_len: 230100/4194304
batch     0,  loss 0.042803, eval 7.772416
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 145  replay_len: 230245/4194304
batch     0,  loss 0.035779, eval 7.766589
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 350  replay_len: 230595/4194304
batch     0,  loss 0.037722, eval 7.725871
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 2  add: 260  replay_len: 230855/4194304
batch     0,  loss 0.044800, eval 7.717685
batches: 2,  total time: 0.70,  1k average: 0.70
batch number: 3  add: 450  replay_len: 231305/4194304
batch     0,  loss 0.038069, eval 7.693949
batches: 3,  total time: 0.76,  1k average: 0.76
batch number: 1  add: 155  replay_len: 231460/4194304
batch     0,  loss 0.042155, eval 7.636632
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 2  add: 320  replay_len: 231780/4194304
batch     0,  loss 0.038431, eval 7.557351
batches: 2,  total time: 0.57,  1k average: 0.57
batch number: 2  add: 290  replay_len: 232070/4194304
batch     0,  loss 0.039333, eval 7.650004
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 2  add: 320  replay_len: 232390/4194304
batch     0,  loss 0.039142, eval 7.583249
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 150  replay_len: 232540/4194304
batch     0,  loss 0.040406, eval 7.524457
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 165  replay_len: 232705/4194304
batch     0,  loss 0.031580, eval 7.517022
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 230  replay_len: 232935/4194304
batch     0,  loss 0.033013, eval 7.454913
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 190  replay_len: 233125/4194304
batch     0,  loss 0.037315, eval 7.463116
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 175  replay_len: 233300/4194304
batch     0,  loss 0.030378, eval 7.395986
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 180  replay_len: 233480/4194304
batch     0,  loss 0.036790, eval 7.361798
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 195  replay_len: 233675/4194304
batch     0,  loss 0.034793, eval 7.336349
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 3  add: 420  replay_len: 234095/4194304
batch     0,  loss 0.035697, eval 7.295344
batches: 3,  total time: 0.83,  1k average: 0.83
batch number: 2  add: 335  replay_len: 234430/4194304
batch     0,  loss 0.044061, eval 7.252013
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 240  replay_len: 234670/4194304
batch     0,  loss 0.036034, eval 7.141159
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 195  replay_len: 234865/4194304
batch     0,  loss 0.055963, eval 7.156995
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 1  add: 205  replay_len: 235070/4194304
batch     0,  loss 0.035772, eval 7.103131
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 3  add: 450  replay_len: 235520/4194304
batch     0,  loss 0.036988, eval 7.114353
batches: 3,  total time: 0.95,  1k average: 0.95
batch number: 1  add: 200  replay_len: 235840/4194304
batch     0,  loss 0.034412, eval 7.073496
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 1  add: 155  replay_len: 235995/4194304
batch     0,  loss 0.028334, eval 6.987847
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 190  replay_len: 236185/4194304
batch     0,  loss 0.043732, eval 6.993869
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 190  replay_len: 236375/4194304
batch     0,  loss 0.037178, eval 6.997955
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 165  replay_len: 236650/4194304
batch     0,  loss 0.037146, eval 7.022266
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 155  replay_len: 236805/4194304
batch     0,  loss 0.033152, eval 6.987110
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 135  replay_len: 236940/4194304
batch     0,  loss 0.037822, eval 6.906082
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 180  replay_len: 237120/4194304
batch     0,  loss 0.033901, eval 6.891463
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 190  replay_len: 237310/4194304
batch     0,  loss 0.035616, eval 6.879955
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 265  replay_len: 237575/4194304
batch     0,  loss 0.034708, eval 6.845265
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 1  add: 130  replay_len: 237705/4194304
batch     0,  loss 0.034059, eval 6.757255
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 4  add: 515  replay_len: 238340/4194304
batch     0,  loss 0.032135, eval 6.704362
batches: 4,  total time: 1.13,  1k average: 1.13
batch number: 1  add: 175  replay_len: 238810/4194304
batch     0,  loss 0.034173, eval 6.669608
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 2  add: 285  replay_len: 239095/4194304
batch     0,  loss 0.034978, eval 6.689706
batches: 2,  total time: 0.46,  1k average: 0.46
batch number: 1  add: 250  replay_len: 239345/4194304
batch     0,  loss 0.032883, eval 6.700552
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 180  replay_len: 239525/4194304
batch     0,  loss 0.034202, eval 6.690549
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 235  replay_len: 239880/4194304
batch     0,  loss 0.038224, eval 6.774371
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 165  replay_len: 240045/4194304
batch     0,  loss 0.037651, eval 6.701177
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 180  replay_len: 240225/4194304
batch     0,  loss 0.035816, eval 6.695087
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 285  replay_len: 240510/4194304
batch     0,  loss 0.032160, eval 6.685025
batches: 2,  total time: 0.65,  1k average: 0.65
batch number: 1  add: 160  replay_len: 240775/4194304
batch     0,  loss 0.036023, eval 6.670891
batches: 1,  total time: 0.25,  1k average: 0.25
batch number: 1  add: 200  replay_len: 240975/4194304
batch     0,  loss 0.037809, eval 6.561670
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 155  replay_len: 241130/4194304
batch     0,  loss 0.036950, eval 6.578213
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 165  replay_len: 241295/4194304
batch     0,  loss 0.041335, eval 6.542225
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 160  replay_len: 241770/4194304
batch     0,  loss 0.038419, eval 6.521723round 413	 loss: [0.04, 0]	 reward: [10.4, -10.0]	 value: [6.52, 0]
round 414	 loss: [0.04, 0]	 reward: [9.6, -10.0]	 value: [6.56, 0]
round 415	 loss: [0.03, 0]	 reward: [10.2, -10.0]	 value: [6.5, 0]
round 416	 loss: [0.04, 0]	 reward: [10.4, -10.0]	 value: [6.52, 0]
round 417	 loss: [0.03, 0]	 reward: [10.6, -10.0]	 value: [6.42, 0]
round 418	 loss: [0.03, 0]	 reward: [10.0, -10.0]	 value: [6.43, 0]
round 419	 loss: [0, 0]	 reward: [10.4, -10.0]	 value: [0, 0]
round 420	 loss: [0.03, 0]	 reward: [9.4, -10.0]	 value: [6.42, 0]
round 421	 loss: [0.03, 0]	 reward: [10.6, -10.0]	 value: [6.33, 0]
round 422	 loss: [0.03, 0]	 reward: [9.0, -10.0]	 value: [6.31, 0]
round 423	 loss: [0.04, 0]	 reward: [10.8, -10.0]	 value: [6.31, 0]
round 424	 loss: [0.04, 0.21]	 reward: [4.4, -10.0]	 value: [6.22, 3.32]
round 425	 loss: [0.03, 0.18]	 reward: [8.2, -10.0]	 value: [6.08, 3.35]
round 426	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [6.12, 0]
round 427	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]

save model... 
===== sample =====
eps 0.05449999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 23,  total time: 0.12,  step average 0.01
===== train =====
train_time 0.00
round time 0.12  total time 1073.29

===== sample =====
eps 0.05374999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 13,  total time: 0.08,  step average 0.01
===== train =====
train_time 0.00
round time 0.08  total time 1073.37

save model... 
===== sample =====
eps 0.05299999999999999 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 35,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.25
round time 0.39  total time 1076.53

===== sample =====
eps 0.05224999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.8 -10. ] 
steps: 57,  total time: 0.21,  step average 0.00
===== train =====
train_time 0.46
round time 0.67  total time 1077.20

save model... 
===== sample =====
eps 0.05149999999999999 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 50,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.35
round time 0.58  total time 1080.51

===== sample =====
eps 0.05074999999999999 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 36,  total time: 0.47,  step average 0.01
===== train =====
train_time 0.24
round time 0.71  total time 1081.22

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 24,  total time: 0.13,  step average 0.01
===== train =====
train_time 0.00
round time 0.14  total time 1084.53

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 47,  total time: 0.18,  step average 0.00
===== train =====
train_time 0.23
round time 0.40  total time 1084.93

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 33,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.24
round time 0.36  total time 1087.65

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 36,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.23
round time 0.36  total time 1088.01

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  9.4 -10. ] 
steps: 57,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.65
round time 0.90  total time 1091.86

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 21,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.00
round time 0.10  total time 1091.96

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 32,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.25
round time 0.39  total time 1095.41

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 40,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.36
round time 0.52  total time 1095.92

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 31,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.24
round time 0.39  total time 1099.40

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 33,  total time: 0.44,  step average 0.01
===== train =====
train_time 0.35
round time 0.79  total time 1100.19

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 21,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.08  total time 1103.15

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 18,  total time: 0.06,  step average 0.00
===== train =====
train_time 0.00
round time 0.07  total time 1103.22

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 24,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.00
round time 0.11  total time 1105.85

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 32,  total time: 0.24,  step average 0.01
===== train =====
train_time 0.34
round time 0.58  total time 1106.43

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 32,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.27
round time 0.41  total time 1109.96

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 29,  total time: 0.20,  step average 0.01
===== train =====
train_time 0.24
round time 0.44  total time 1110.40

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 50,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.28
round time 0.51  total time 1113.98

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 30,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.25
round time 0.37  total time 1114.35

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 30,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.34
round time 0.48  total time 1117.80

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 18,  total time: 0.28,  step average 0.02
===== train =====
train_time 0.00
round time 0.28  total time 1118.08

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 38,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.24
round time 0.39  total time 1121.64

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 45,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.38  total time 1122.02

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 34,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.24
round time 0.41  total time 1125.64

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 28,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.28
round time 0.40  total time 1126.04

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.4  0. ],  total_reward: [-0.4  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  5. -10.] 
steps: 61,  total time: 0.44,  step average 0.01
===== train =====
train_time 0.82
round time 1.26  total time 1130.19

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  8.2 -10. ] 
steps: 59,  total time: 0.30,  step average 0.01
===== train =====
train_time 0.60
round time 0.90  total time 1131.09

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 48,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.24
round time 0.43  total time 1134.49

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 21,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.08  total time 1134.56

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] round 428	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [6.06, 0]
round 429	 loss: [0, 0]	 reward: [11.2, -10.0]	 value: [0, 0]
round 430	 loss: [0.03, 0.21]	 reward: [6.8, -10.0]	 value: [6.02, 3.28]
round 431	 loss: [0.03, 0]	 reward: [10.6, -10.0]	 value: [6.03, 0]
round 432	 loss: [0.04, 0]	 reward: [9.0, -10.0]	 value: [6.01, 0]
round 433	 loss: [0.03, 0.19]	 reward: [9.0, -10.0]	 value: [5.95, 3.32]
round 434	 loss: [0.04, 0]	 reward: [10.4, -10.0]	 value: [5.98, 0]
round 435	 loss: [0.03, 0.2]	 reward: [7.2, -10.0]	 value: [5.91, 3.32]
round 436	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [5.92, 0]
round 437	 loss: [0.03, 0]	 reward: [10.4, -10.0]	 value: [5.93, 0]
round 438	 loss: [0.03, 0.21]	 reward: [7.8, -10.0]	 value: [5.91, 3.29]
round 439	 loss: [0.03, 0.2]	 reward: [6.0, -10.0]	 value: [5.91, 3.17]
round 440	 loss: [0.04, 0]	 reward: [9.6, -10.0]	 value: [5.89, 0]
round 441	 loss: [0.03, 0.22]	 reward: [6.2, -10.0]	 value: [5.87, 3.07]
round 442	 loss: [0.03, 0]	 reward: [8.2, -10.0]	 value: [5.87, 0]
round 443	 loss: [0.04, 0]	 reward: [10.4, -10.0]	 value: [5.85, 0]
round 444	 loss: [0.03, 0]	 reward: [11.2, -10.0]	 value: [5.82, 0]
round 445	 loss: [0.03, 0]	 reward: [9.0, -10.0]	 value: [5.8, 0]
round 446	 loss: [0, 0]	 reward: [10.6, -10.0]	 value: [0, 0]
round 447	 loss: [0.04, 0]	 reward: [10.2, -10.0]	 value: [5.82, 0]
round 448	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [5.79, 0]
round 449	 loss: [0.03, 0]	 reward: [9.8, -10.0]	 value: [5.83, 0]
round 450	 loss: [0.03, 0.19]	 reward: [8.2, -10.0]	 value: [5.74, 2.99]
round 451	 loss: [0.03, 0]	 reward: [10.0, -10.0]	 value: [5.78, 0]
round 452	 loss: [0, 0]	 reward: [11.2, -10.0]	 value: [0, 0]
round 453	 loss: [0, 0]	 reward: [9.8, -10.0]	 value: [0, 0]
round 454	 loss: [0, 0]	 reward: [11.4, -10.0]	 value: [0, 0]
round 455	 loss: [0, 0]	 reward: [9.8, -10.0]	 value: [0, 0]
round 456	 loss: [0.03, 0]	 reward: [9.2, -10.0]	 value: [5.78, 0]
round 457	 loss: [0.04, 0]	 reward: [9.6, -10.0]	 value: [5.75, 0]
round 458	 loss: [0.03, 0]	 reward: [9.0, -10.0]	 value: [5.7, 0]
round 459	 loss: [0.03, 0.2]	 reward: [0.6, -10.0]	 value: [5.64, 2.93]
round 460	 loss: [0.04, 0]	 reward: [9.0, -10.0]	 value: [5.67, 0]

steps: 36,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.25
round time 0.38  total time 1137.91

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 17,  total time: 0.22,  step average 0.01
===== train =====
train_time 0.00
round time 0.22  total time 1138.14

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  6. -10.] 
steps: 56,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.63
round time 0.86  total time 1141.98

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 28,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.29
round time 0.48  total time 1142.46

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  8.2 -10. ] 
steps: 53,  total time: 0.35,  step average 0.01
===== train =====
train_time 0.48
round time 0.83  total time 1145.97

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0. 0.],  total_reward: [  8.2 -10. ] 
steps: 55,  total time: 0.22,  step average 0.00
===== train =====
train_time 0.55
round time 0.77  total time 1146.74

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 33,  total time: 0.19,  step average 0.01
===== train =====
train_time 0.27
round time 0.46  total time 1150.69

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  7.4 -10. ] 
steps: 60,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.59
round time 0.84  total time 1151.53

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 31,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 1155.76

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 32,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 1156.10

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 42,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.36
round time 0.52  total time 1159.26

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  5.8 -10. ] 
steps: 62,  total time: 0.82,  step average 0.01
===== train =====
train_time 0.84
round time 1.66  total time 1160.91

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 42,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.40  total time 1164.14

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [0. 0.],  total_reward: [  8.4 -10. ] 
steps: 80,  total time: 0.30,  step average 0.00
===== train =====
train_time 0.80
round time 1.09  total time 1165.23

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  8.2 -10. ] 
steps: 64,  total time: 0.23,  step average 0.00
===== train =====
train_time 0.56
round time 0.80  total time 1168.80

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 37,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.23
round time 0.39  total time 1169.19

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [ 10.6 -10. ] 
steps: 59,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.49
round time 0.74  total time 1173.16

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 35,  total time: 0.16,  step average 0.00
===== train =====
train_time 0.26
round time 0.42  total time 1173.57

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 20,  total time: 0.12,  step average 0.01
===== train =====
train_time 0.00
round time 0.12  total time 1178.56

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 27,  total time: 0.14,  step average 0.01
===== train =====
train_time 0.31
round time 0.45  total time 1179.01

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 28,  total time: 0.19,  step average 0.01
===== train =====
train_time 0.31
round time 0.50  total time 1183.34

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 34,  total time: 1.01,  step average 0.03
===== train =====
train_time 0.32
round time 1.32  total time 1184.67

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 45,  total time: 0.23,  step average 0.01
===== train =====
train_time 0.46
round time 0.69  total time 1189.20

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 33,  total time: 0.25,  step average 0.01
===== train =====
train_time 0.27
round time 0.52  total time 1189.73

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.4 -1. ],  total_reward: [ 0.4 -1. ] 
steps: 15,  total time: 0.07,  step average 0.00
===== train =====
train_time 0.00
round time 0.07  total time 1193.42

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 20,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.00
round time 0.10  total time 1193.52

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 18,  total time: 0.11,  step average 0.01
===== train =====
train_time 0.00
round time 0.12  total time 1197.27

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 22,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.00
round time 0.11  total time 1197.38

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 34,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.24
round time 0.36  total time 1201.58

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 30,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.34
round time 0.45  total time 1202.03

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 36,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.26
round time 0.41  total time 1205.98

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  7. -10.] 
step 100,  reward: [0. 0.],  total_reward: [  3.6 -10. ] 
steps: 146,  total time: 2.79,  step average 0.02
===== train =====
train_time 1.54
round time 4.33  total time 1210.31

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 42,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.25
round time 0.43  total time 1214.32

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 16,  total time: 0.07,  step average 0.00
===== train =====
train_time 0.00round 461	 loss: [0, 0]	 reward: [11.6, -10.0]	 value: [0, 0]
round 462	 loss: [0, 0]	 reward: [11.4, -10.0]	 value: [0, 0]
round 463	 loss: [0, 0]	 reward: [10.6, -10.0]	 value: [0, 0]
round 464	 loss: [0.04, 0]	 reward: [8.0, -10.0]	 value: [5.68, 0]
round 465	 loss: [0.03, 0]	 reward: [10.4, -10.0]	 value: [5.64, 0]
round 466	 loss: [0.03, 0]	 reward: [9.6, -10.0]	 value: [5.63, 0]
round 467	 loss: [0, 0]	 reward: [11.4, -10.0]	 value: [0, 0]
round 468	 loss: [0.03, 0.15]	 reward: [7.8, -10.0]	 value: [5.63, 2.86]
round 469	 loss: [0.03, 0]	 reward: [7.8, -10.0]	 value: [5.66, 0]
round 470	 loss: [0.03, 0.21]	 reward: [3.0, -10.0]	 value: [5.57, 2.73]
round 471	 loss: [0.03, 0]	 reward: [8.4, -10.0]	 value: [5.61, 0]
round 472	 loss: [0, 0]	 reward: [10.4, -10.0]	 value: [0, 0]
round 473	 loss: [0, 0]	 reward: [10.4, -10.0]	 value: [0, 0]
round 474	 loss: [0, 0]	 reward: [10.6, -10.0]	 value: [0, 0]
round 475	 loss: [0.03, 0.22]	 reward: [6.6, -10.0]	 value: [5.59, 2.65]
round 476	 loss: [0.03, 0.18]	 reward: [8.2, -10.0]	 value: [5.56, 2.59]
round 477	 loss: [0.03, 0.19]	 reward: [5.6, -10.0]	 value: [5.52, 2.52]
round 478	 loss: [0.03, 0.14]	 reward: [-0.6, -10.0]	 value: [5.55, 2.52]
round 479	 loss: [0, 0]	 reward: [10.2, -10.0]	 value: [0, 0]
round 480	 loss: [0.04, 0.19]	 reward: [8.2, -10.0]	 value: [5.55, 2.44]
round 481	 loss: [0.03, 0.16]	 reward: [10.2, -10.0]	 value: [5.52, 2.33]
round 482	 loss: [0, 0]	 reward: [11.2, -10.0]	 value: [0, 0]
round 483	 loss: [0.03, 0]	 reward: [10.0, -10.0]	 value: [5.52, 0]
round 484	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [5.56, 0]
round 485	 loss: [0.04, 0]	 reward: [9.6, -10.0]	 value: [5.58, 0]
round 486	 loss: [0, 0]	 reward: [10.0, -10.0]	 value: [0, 0]

batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 160  replay_len: 241930/4194304
batch     0,  loss 0.037471, eval 6.562956
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 145  replay_len: 242075/4194304
batch     0,  loss 0.033739, eval 6.501729
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 250  replay_len: 242325/4194304
batch     0,  loss 0.035241, eval 6.522492
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 150  replay_len: 242475/4194304
batch     0,  loss 0.029472, eval 6.423989
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 150  replay_len: 242625/4194304
batch     0,  loss 0.031992, eval 6.431552
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 190  replay_len: 242905/4194304
batch     0,  loss 0.026627, eval 6.423877
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 225  replay_len: 243130/4194304
batch     0,  loss 0.030874, eval 6.329998
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 170  replay_len: 243300/4194304
batch     0,  loss 0.033168, eval 6.306314
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 140  replay_len: 243440/4194304
batch     0,  loss 0.038033, eval 6.312175
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 2  add: 305  replay_len: 243745/4194304
batch     0,  loss 0.034783, eval 6.198721
batches: 2,  total time: 0.81,  1k average: 0.81
batch number: 2  add: 295  replay_len: 244040/4194304
batch     0,  loss 0.030995, eval 6.188175
batches: 2,  total time: 0.60,  1k average: 0.60
batch number: 1  add: 240  replay_len: 244280/4194304
batch     0,  loss 0.030205, eval 6.124569
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 180  replay_len: 244565/4194304
batch     0,  loss 0.039316, eval 6.061316
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 2  add: 280  replay_len: 244930/4194304
batch     0,  loss 0.040503, eval 6.056960
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 1  add: 140  replay_len: 245070/4194304
batch     0,  loss 0.029231, eval 6.025440
batches: 1,  total time: 0.29,  1k average: 0.29
batch number: 2  add: 265  replay_len: 245335/4194304
batch     0,  loss 0.034609, eval 6.018180
batches: 2,  total time: 0.48,  1k average: 0.48
batch number: 2  add: 275  replay_len: 245610/4194304
batch     0,  loss 0.033922, eval 5.966043
batches: 2,  total time: 0.55,  1k average: 0.55
batch number: 1  add: 165  replay_len: 245775/4194304
batch     0,  loss 0.038115, eval 5.975708
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 2  add: 300  replay_len: 246075/4194304
batch     0,  loss 0.032876, eval 5.973778
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 1  add: 155  replay_len: 246230/4194304
batch     0,  loss 0.026310, eval 5.919670
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 160  replay_len: 246390/4194304
batch     0,  loss 0.032216, eval 5.934743
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 1  add: 210  replay_len: 246600/4194304
batch     0,  loss 0.030132, eval 5.906895
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 2  add: 310  replay_len: 246910/4194304
batch     0,  loss 0.029948, eval 5.902235
batches: 2,  total time: 0.83,  1k average: 0.83
batch number: 1  add: 210  replay_len: 247120/4194304
batch     0,  loss 0.035070, eval 5.885993
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 3  add: 400  replay_len: 247520/4194304
batch     0,  loss 0.033013, eval 5.867786
batches: 3,  total time: 0.79,  1k average: 0.79
batch number: 2  add: 320  replay_len: 247840/4194304
batch     0,  loss 0.032681, eval 5.904143
batches: 2,  total time: 0.56,  1k average: 0.56
batch number: 1  add: 185  replay_len: 248025/4194304
batch     0,  loss 0.035955, eval 5.851166
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 2  add: 295  replay_len: 248320/4194304
batch     0,  loss 0.031299, eval 5.865781
batches: 2,  total time: 0.49,  1k average: 0.49
batch number: 1  add: 175  replay_len: 248495/4194304
batch     0,  loss 0.026915, eval 5.803253
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 1  add: 135  replay_len: 248730/4194304
batch     0,  loss 0.037220, eval 5.821535
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 140  replay_len: 248870/4194304
batch     0,  loss 0.025859, eval 5.790791
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 170  replay_len: 249040/4194304
batch     0,  loss 0.034619, eval 5.826197
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 225  replay_len: 249265/4194304
batch     0,  loss 0.029052, eval 5.741700
batches: 1,  total time: 0.46,  1k average: 0.46
batch number: 1  add: 165  replay_len: 249430/4194304
batch     0,  loss 0.029689, eval 5.779213
batches: 1,  total time: 0.27,  1k average: 0.27
batch number: 1  add: 170  replay_len: 249975/4194304
batch     0,  loss 0.029289, eval 5.779585
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 150  replay_len: 250125/4194304
batch     0,  loss 0.042031, eval 5.746144
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 180  replay_len: 250305/4194304
batch     0,  loss 0.030719, eval 5.696600
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 5  add: 730  replay_len: 251035/4194304
batch     0,  loss 0.035206, eval 5.699741
batches: 5,  total time: 1.53,  1k average: 1.53
batch number: 1  add: 210  replay_len: 251245/4194304
batch     0,  loss 0.036081, eval 5.666386
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 175  replay_len: 251680/4194304
batch     0,  loss 0.038999, eval 5.683723
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 140  replay_len: 251820/4194304
batch     0,  loss 0.032680, eval 5.637712
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 155  replay_len: 251975/4194304
batch     0,  loss 0.030993, eval 5.627110
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 215  replay_len: 252255/4194304
batch     0,  loss 0.033040, eval 5.633090
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 2  add: 265  replay_len: 252520/4194304
batch     0,  loss 0.030154, eval 5.680614
batches: 2,  total time: 0.62,  1k average: 0.62
batch number: 3  add: 445  replay_len: 252965/4194304
batch     0,  loss 0.029847, eval 5.586465
batches: 3,  total time: 1.21,  1k average: 1.21
batch number: 1  add: 145  replay_len: 253110/4194304
batch     0,  loss 0.033796, eval 5.614284
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 2  add: 320  replay_len: 253725/4194304
batch     0,  loss 0.030240, eval 5.521782
batches: 2,  total time: 1.01,  1k average: 1.01
batch number: 1  add: 180  replay_len: 253905/4194304
batch     0,  loss 0.031600, eval 5.560720
batches: 1,  total time: 0.37,  1k average: 0.37
batch number: 1  add: 255  replay_len: 254160/4194304
batch     0,  loss 0.032730, eval 5.519775
batches: 1,  total time: 0.37,  1k average: 0.37
batch number: 4  add: 615  replay_len: 254775/4194304
batch     0,  loss 0.026914, eval 5.498761
batches: 4,  total time: 1.00,  1k average: 1.00
batch number: 2  add: 330  replay_len: 255230/4194304
batch     0,  loss 0.035142, eval 5.541508
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 1  add: 155  replay_len: 255385/4194304
batch     0,  loss 0.031159, eval 5.516557
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 130  replay_len: 255630/4194304
batch     0,  loss 0.032204, eval 5.518443
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 205  replay_len: 255835/4194304
batch     0,  loss 0.038108, eval 5.561232
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 155  replay_len: 255990/4194304
batch     0,  loss 0.035196, eval 5.580804
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 150  replay_len: 256230/4194304
batch     0,  loss 0.035694, eval 5.536057
batches: 1,  total time: 0.34,  1k average: 0.34round 487	 loss: [0.04, 0]	 reward: [9.2, -10.0]	 value: [5.54, 0]
round 488	 loss: [0.03, 0]	 reward: [9.0, -10.0]	 value: [5.56, 0]
round 489	 loss: [0.03, 0]	 reward: [8.0, -10.0]	 value: [5.55, 0]
round 490	 loss: [0.04, 0]	 reward: [9.8, -10.0]	 value: [5.56, 0]
round 491	 loss: [0, 0]	 reward: [9.4, -10.0]	 value: [0, 0]
round 492	 loss: [0.04, 0]	 reward: [7.8, -10.0]	 value: [5.64, 0]
round 493	 loss: [0.03, 0]	 reward: [8.6, -10.0]	 value: [5.57, 0]
round 494	 loss: [0.03, 0]	 reward: [8.4, -10.0]	 value: [5.57, 0]
round 495	 loss: [0, 0]	 reward: [9.6, -10.0]	 value: [0, 0]

round time 0.07  total time 1214.39

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 16,  total time: 0.07,  step average 0.00
===== train =====
train_time 0.00
round time 0.07  total time 1217.56

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 20,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 1217.65

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 35,  total time: 0.14,  step average 0.00
===== train =====
train_time 0.23
round time 0.37  total time 1221.30

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [-0.2  0. ],  total_reward: [-0.2  0. ] 
steps: 28,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.24
round time 0.34  total time 1221.64

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 31,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.24
round time 0.37  total time 1225.05

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
steps: 13,  total time: 0.05,  step average 0.00
===== train =====
train_time 0.00
round time 0.05  total time 1225.10

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 43,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.35
round time 0.54  total time 1228.75

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.2 -10. ] 
steps: 53,  total time: 0.81,  step average 0.02
===== train =====
train_time 0.62
round time 1.43  total time 1230.18

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  5.6 -10. ] 
steps: 89,  total time: 0.36,  step average 0.00
===== train =====
train_time 1.22
round time 1.57  total time 1234.95

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 29,  total time: 0.20,  step average 0.01
===== train =====
train_time 0.24
round time 0.43  total time 1235.38

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 15,  total time: 0.06,  step average 0.00
===== train =====
train_time 0.00
round time 0.06  total time 1238.52

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 21,  total time: 0.11,  step average 0.01
===== train =====
train_time 0.00
round time 0.11  total time 1238.63

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 23,  total time: 0.08,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 1241.73

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.4 -2. ],  total_reward: [ 1.4 -2. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.2 -10. ] 
steps: 64,  total time: 0.23,  step average 0.00
===== train =====
train_time 1.01
round time 1.25  total time 1242.97

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 36,  total time: 0.18,  step average 0.01
===== train =====
train_time 0.37
round time 0.56  total time 1246.60

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [0.6 0. ],  total_reward: [  5.6 -10. ] 
steps: 51,  total time: 0.24,  step average 0.00
===== train =====
train_time 0.38
round time 0.61  total time 1247.21

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
step  50,  reward: [-0.2  0. ],  total_reward: [  6. -10.] 
step 100,  reward: [-0.4  0. ],  total_reward: [  0.6 -10. ] 
steps: 123,  total time: 0.50,  step average 0.00
===== train =====
train_time 1.01
round time 1.51  total time 1251.83

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 25,  total time: 0.30,  step average 0.01
===== train =====
train_time 0.00
round time 0.30  total time 1252.12

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
step  50,  reward: [-0.2  0. ],  total_reward: [  8. -10.] 
steps: 66,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.58
round time 0.83  total time 1256.44

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 31,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.35
round time 0.47  total time 1256.91

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 23,  total time: 0.09,  step average 0.00
===== train =====
train_time 0.00
round time 0.09  total time 1260.18

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 26,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.25
round time 0.34  total time 1260.52

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 41,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.24
round time 0.39  total time 1264.15

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 31,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.23
round time 0.34  total time 1264.49

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 18,  total time: 0.07,  step average 0.00
===== train =====
train_time 0.00
round time 0.07  total time 1267.59

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 30,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.34
round time 0.48  total time 1268.07

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 39,  total time: 0.19,  step average 0.00
===== train =====
train_time 0.28
round time 0.47  total time 1271.81

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 37,  total time: 0.56,  step average 0.02
===== train =====
train_time 0.23
round time 0.79  total time 1272.60

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [0. 0.],  total_reward: [0. 0.] 
steps: 34,  total time: 0.12,  step average 0.00
===== train =====
train_time 0.23
round time 0.35  total time 1276.21

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 21,  total time: 0.07,  step average 0.00
===== train =====
train_time 0.00
round time 0.08  total time 1276.29

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 42,  total time: 0.15,  step average 0.00
===== train =====
train_time 0.24
round time 0.40  total time 1279.71

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 26,  total time: 0.10,  step average 0.00
===== train =====
train_time 0.24
round time 0.34  total time 1280.05

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
steps: 31,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.26
round time 0.39  total time 1283.57

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 2.4 -3. ],  total_reward: [ 2.4 -3. ] 
steps: 23,  total time: 0.11,  step average 0.00
===== train =====
train_time 0.00
round time 0.11  total time 1283.68
round 496	 loss: [0.03, 0.17]	 reward: [7.0, -10.0]	 value: [5.54, 2.33]
round 497	 loss: [0.03, 0]	 reward: [10.0, -10.0]	 value: [5.53, 0]
round 498	 loss: [0.03, 0.19]	 reward: [7.6, -10.0]	 value: [5.49, 2.26]
round 499	 loss: [0.03, 0]	 reward: [8.4, -10.0]	 value: [5.51, 0]

batch number: 1  add: 195  replay_len: 256425/4194304
batch     0,  loss 0.030372, eval 5.559722
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 1  add: 185  replay_len: 256610/4194304
batch     0,  loss 0.033238, eval 5.550755
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 170  replay_len: 256780/4194304
batch     0,  loss 0.035639, eval 5.560755
batches: 1,  total time: 0.23,  1k average: 0.23
batch number: 1  add: 210  replay_len: 257095/4194304
batch     0,  loss 0.038425, eval 5.639521
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 130  replay_len: 257225/4194304
batch     0,  loss 0.034064, eval 5.567860
batches: 1,  total time: 0.24,  1k average: 0.24
batch number: 1  add: 155  replay_len: 257380/4194304
batch     0,  loss 0.030044, eval 5.568523
batches: 1,  total time: 0.26,  1k average: 0.26
batch number: 2  add: 340  replay_len: 257835/4194304
batch     0,  loss 0.033148, eval 5.533015
batches: 2,  total time: 0.59,  1k average: 0.59
batch number: 1  add: 160  replay_len: 257995/4194304
batch     0,  loss 0.034054, eval 5.526312
batches: 1,  total time: 0.22,  1k average: 0.22
batch number: 2  add: 280  replay_len: 258275/4194304
batch     0,  loss 0.033392, eval 5.495535
batches: 2,  total time: 0.58,  1k average: 0.58
batch number: 1  add: 220  replay_len: 258495/4194304
batch     0,  loss 0.028995, eval 5.513034
batches: 1,  total time: 0.24,  1k average: 0.24

batch number: 1  add: 218  replay_len: 120499/4194304
batch     0,  loss 0.160067, eval 3.099671
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 171  replay_len: 120885/4194304
batch     0,  loss 0.150439, eval 3.140627
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 155  replay_len: 121040/4194304
batch     0,  loss 0.173968, eval 3.120438
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 161  replay_len: 121201/4194304
batch     0,  loss 0.165500, eval 3.152066
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 160  replay_len: 121466/4194304
batch     0,  loss 0.131857, eval 3.090216
batches: 1,  total time: 0.28,  1k average: 0.28
batch number: 1  add: 161  replay_len: 121627/4194304
batch     0,  loss 0.152539, eval 3.129441
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 169  replay_len: 121796/4194304
batch     0,  loss 0.141656, eval 3.174779
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 136  replay_len: 122039/4194304
batch     0,  loss 0.178166, eval 3.125188
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 139  replay_len: 122581/4194304
batch     0,  loss 0.202805, eval 3.202329
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 148  replay_len: 122729/4194304
batch     0,  loss 0.161588, eval 3.148708
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 171  replay_len: 122900/4194304
batch     0,  loss 0.132976, eval 3.190703
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 167  replay_len: 123067/4194304
batch     0,  loss 0.177472, eval 3.209534
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 146  replay_len: 123439/4194304
batch     0,  loss 0.164766, eval 3.172856
batches: 1,  total time: 0.38,  1k average: 0.38
batch number: 1  add: 182  replay_len: 124969/4194304
batch     0,  loss 0.210018, eval 3.231749
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 132  replay_len: 125527/4194304
batch     0,  loss 0.207297, eval 3.225808
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 128  replay_len: 126175/4194304
batch     0,  loss 0.210565, eval 3.235550
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 157  replay_len: 127989/4194304
batch     0,  loss 0.209397, eval 3.316799
batches: 1,  total time: 0.41,  1k average: 0.41
batch number: 1  add: 146  replay_len: 128135/4194304
batch     0,  loss 0.176173, eval 3.348528
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 145  replay_len: 128622/4194304
batch     0,  loss 0.207129, eval 3.281132
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 160  replay_len: 128986/4194304
batch     0,  loss 0.189991, eval 3.315402
batches: 1,  total time: 0.30,  1k average: 0.30
batch number: 1  add: 154  replay_len: 129227/4194304
batch     0,  loss 0.202368, eval 3.322330
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 150  replay_len: 129568/4194304
batch     0,  loss 0.209574, eval 3.292010
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 166  replay_len: 129734/4194304
batch     0,  loss 0.203078, eval 3.174809
batches: 1,  total time: 0.48,  1k average: 0.48
batch number: 1  add: 147  replay_len: 129979/4194304
batch     0,  loss 0.216571, eval 3.074011
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 158  replay_len: 130922/4194304
batch     0,  loss 0.186813, eval 2.994853
batches: 1,  total time: 0.44,  1k average: 0.44
batch number: 1  add: 220  replay_len: 131845/4194304
batch     0,  loss 0.198851, eval 2.927923
batches: 1,  total time: 0.43,  1k average: 0.43
batch number: 1  add: 131  replay_len: 132589/4194304
batch     0,  loss 0.154466, eval 2.861739
batches: 1,  total time: 0.31,  1k average: 0.31
batch number: 1  add: 185  replay_len: 132900/4194304
batch     0,  loss 0.211370, eval 2.732980
batches: 1,  total time: 0.34,  1k average: 0.34
batch number: 1  add: 132  replay_len: 133370/4194304
batch     0,  loss 0.223740, eval 2.649554
batches: 1,  total time: 0.67,  1k average: 0.67
batch number: 1  add: 130  replay_len: 133500/4194304
batch     0,  loss 0.180025, eval 2.592586
batches: 1,  total time: 0.35,  1k average: 0.35
batch number: 1  add: 165  replay_len: 133665/4194304
batch     0,  loss 0.185965, eval 2.524464
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 193  replay_len: 133858/4194304
batch     0,  loss 0.137407, eval 2.521920
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 150  replay_len: 134078/4194304
batch     0,  loss 0.190121, eval 2.435146
batches: 1,  total time: 0.33,  1k average: 0.33
batch number: 1  add: 131  replay_len: 134209/4194304
batch     0,  loss 0.163714, eval 2.327806
batches: 1,  total time: 0.32,  1k average: 0.32
batch number: 1  add: 168  replay_len: 135743/4194304
batch     0,  loss 0.169746, eval 2.333061
batches: 1,  total time: 0.36,  1k average: 0.36
batch number: 1  add: 134  replay_len: 135978/4194304
batch     0,  loss 0.190826, eval 2.256460
batches: 1,  total time: 0.32,  1k average: 0.32

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.8 -1. ],  total_reward: [ 0.8 -1. ] 
step  50,  reward: [-0.4  0. ],  total_reward: [  7. -10.] 
steps: 68,  total time: 0.25,  step average 0.00
===== train =====
train_time 0.59
round time 0.84  total time 1287.59

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 1.6 -2. ],  total_reward: [ 1.6 -2. ] 
steps: 32,  total time: 0.13,  step average 0.00
===== train =====
train_time 0.22
round time 0.36  total time 1287.94

save model... 
===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
step  50,  reward: [0. 0.],  total_reward: [  7.2 -10. ] 
steps: 56,  total time: 0.20,  step average 0.00
===== train =====
train_time 0.59
round time 0.79  total time 1291.64

===== sample =====
eps 0.05 number [5, 5]
step   0,  reward: [ 0.6 -1. ],  total_reward: [ 0.6 -1. ] 
steps: 44,  total time: 0.58,  step average 0.01
===== train =====
train_time 0.24
round time 0.82  total time 1292.46

save model... 
